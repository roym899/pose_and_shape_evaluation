{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started CPAS Toolbox is a package for evaluation of categorical pose and shape estimation methods. It contains metrics, and wrappers for datasets and methods. Installation Run pip install cpas_toolbox to install the latest release of the toolbox. There is no need to download any additional weights or datasets. Upon first usage the evaluation script will ask to download the weights if they are not available at the expected path. Evaluation of baseline methods To reproduce the REAL275 benchmark run: python -m cpas_toolbox.evaluate --config real275.yaml --out_dir ./results/ To reproduce the REDWOOD75 benchmark run: python -m cpas_toolbox.evaluate --config redwood75.yaml --out_dir ./results/ We can overwrite settings of the configuration via the command-line. For example, python -m cpas_toolbox.evaluate --config redwood75.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True enables interactive visualization of ground truth and predictions. Alternatively, you could specify --store_visualization True to save the visualization of every prediction in the results directory. Citation If you find this library useful in your research, consider citing our publication : @article{bruns2022evaluation, title={On the Evaluation of {RGB-D}-based Categorical Pose and Shape Estimation}, author={Bruns, Leonard and Jensfelt, Patric}, journal={arXiv preprint arXiv:2202.10346}, year={2022} }","title":"Getting started"},{"location":"#getting-started","text":"CPAS Toolbox is a package for evaluation of categorical pose and shape estimation methods. It contains metrics, and wrappers for datasets and methods.","title":"Getting started"},{"location":"#installation","text":"Run pip install cpas_toolbox to install the latest release of the toolbox. There is no need to download any additional weights or datasets. Upon first usage the evaluation script will ask to download the weights if they are not available at the expected path.","title":"Installation"},{"location":"#evaluation-of-baseline-methods","text":"To reproduce the REAL275 benchmark run: python -m cpas_toolbox.evaluate --config real275.yaml --out_dir ./results/ To reproduce the REDWOOD75 benchmark run: python -m cpas_toolbox.evaluate --config redwood75.yaml --out_dir ./results/ We can overwrite settings of the configuration via the command-line. For example, python -m cpas_toolbox.evaluate --config redwood75.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True enables interactive visualization of ground truth and predictions. Alternatively, you could specify --store_visualization True to save the visualization of every prediction in the results directory.","title":"Evaluation of baseline methods"},{"location":"#citation","text":"If you find this library useful in your research, consider citing our publication : @article{bruns2022evaluation, title={On the Evaluation of {RGB-D}-based Categorical Pose and Shape Estimation}, author={Bruns, Leonard and Jensfelt, Patric}, journal={arXiv preprint arXiv:2202.10346}, year={2022} }","title":"Citation"},{"location":"data_preparation/","text":"Data preparation On first usage of a dataset the script will download and preprocess the datasets automatically. This is the recommended way to use the package as it ensures an unmodified dataset. If you already downloaded a dataset and want to use symlinks instead of storing them again to save storage space, you can follow the manual instructions below. The default directories can be found in the configuration file for the respective dataset ( REAL275 , REDWOOD75 ). REAL275 For download links check the NOCS repository . The expected directory structure for REAL275 evaluation is as follows: {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... An additional directory {root_dir}/cpas_toolbox/ will be created to store preprocessed files. By default {root_dir} will be data/nocs/ (i.e., relative to the current working directory, when executing the evaluation script), but it can be modified. REDWOOD75 To download the raw data check the redwood-data website . You can download the REDWOOD75 annotations here . Only the Redwood sequences ids contained in this file are required for evaluation. The expected directory structure for REDWOOD75 evaluation is as follows: {root_dir}/bottle/rgbd/00049/depth/*.png {root_dir}/bottle/rgbd/... {root_dir}/bowl/... {root_dir}/mug/... {ann_dir}/obj_models/... By default {root_dir} will be data/redwood/ (i.e., relative to the current working directory, when executing the evaluation script) {ann_dir} will be data/redwood75/ , but those can be modified.","title":"Data preparation"},{"location":"data_preparation/#data-preparation","text":"On first usage of a dataset the script will download and preprocess the datasets automatically. This is the recommended way to use the package as it ensures an unmodified dataset. If you already downloaded a dataset and want to use symlinks instead of storing them again to save storage space, you can follow the manual instructions below. The default directories can be found in the configuration file for the respective dataset ( REAL275 , REDWOOD75 ).","title":"Data preparation"},{"location":"data_preparation/#real275","text":"For download links check the NOCS repository . The expected directory structure for REAL275 evaluation is as follows: {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... An additional directory {root_dir}/cpas_toolbox/ will be created to store preprocessed files. By default {root_dir} will be data/nocs/ (i.e., relative to the current working directory, when executing the evaluation script), but it can be modified.","title":"REAL275"},{"location":"data_preparation/#redwood75","text":"To download the raw data check the redwood-data website . You can download the REDWOOD75 annotations here . Only the Redwood sequences ids contained in this file are required for evaluation. The expected directory structure for REDWOOD75 evaluation is as follows: {root_dir}/bottle/rgbd/00049/depth/*.png {root_dir}/bottle/rgbd/... {root_dir}/bowl/... {root_dir}/mug/... {ann_dir}/obj_models/... By default {root_dir} will be data/redwood/ (i.e., relative to the current working directory, when executing the evaluation script) {ann_dir} will be data/redwood75/ , but those can be modified.","title":"REDWOOD75"},{"location":"api_reference/camera_utils/","text":"cpas_toolbox.camera_utils This module provides a pinhole camera class. Camera Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. Source code in cpas_toolbox/camera_utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Camera : \"\"\"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. \"\"\" def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None : \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s __init__ __init__ ( width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. PARAMETER DESCRIPTION width Number of pixels in horizontal direction. TYPE: int height Number of pixels in vertical direction. TYPE: int fx Horizontal focal length. TYPE: float fy Vertical focal length. TYPE: float cx Principal point x-coordinate. TYPE: float cy Principal point y-coordinate. TYPE: float s Skew. TYPE: float DEFAULT: 0.0 pixel_center The center offset for the provided principal point. TYPE: float DEFAULT: 0.0 Source code in cpas_toolbox/camera_utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None : \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height get_o3d_pinhole_camera_parameters get_o3d_pinhole_camera_parameters () -> o3d . camera . PinholeCameraParameters () Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. RETURNS DESCRIPTION o3d . camera . PinholeCameraParameters () The pinhole camera parameters. Source code in cpas_toolbox/camera_utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params get_pinhole_camera_parameters get_pinhole_camera_parameters ( pixel_center : float ) -> Tuple Convert camera to general camera parameters. PARAMETER DESCRIPTION pixel_center At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. TYPE: float RETURNS DESCRIPTION Tuple fx, fy: The horizontal and vertical focal length Tuple cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. Tuple s: The skew. Source code in cpas_toolbox/camera_utils.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s","title":"camera_utils.py"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils","text":"This module provides a pinhole camera class.","title":"camera_utils"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera","text":"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. Source code in cpas_toolbox/camera_utils.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Camera : \"\"\"Pinhole camera parameters. This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision. \"\"\" def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None : \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s","title":"Camera"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.__init__","text":"__init__ ( width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. PARAMETER DESCRIPTION width Number of pixels in horizontal direction. TYPE: int height Number of pixels in vertical direction. TYPE: int fx Horizontal focal length. TYPE: float fy Vertical focal length. TYPE: float cx Principal point x-coordinate. TYPE: float cy Principal point y-coordinate. TYPE: float s Skew. TYPE: float DEFAULT: 0.0 pixel_center The center offset for the provided principal point. TYPE: float DEFAULT: 0.0 Source code in cpas_toolbox/camera_utils.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , width : int , height : int , fx : float , fy : float , cx : float , cy : float , s : float = 0.0 , pixel_center : float = 0.0 , ) -> None : \"\"\"Initialize camera parameters. Note that the principal point is only fully defined in combination with pixel_center. The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates. A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix. Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate. For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate. Args: width: Number of pixels in horizontal direction. height: Number of pixels in vertical direction. fx: Horizontal focal length. fy: Vertical focal length. cx: Principal point x-coordinate. cy: Principal point y-coordinate. s: Skew. pixel_center: The center offset for the provided principal point. \"\"\" # focal length self . fx = fx self . fy = fy # principal point self . cx = cx self . cy = cy self . pixel_center = pixel_center # skew self . s = s # image dimensions self . width = width self . height = height","title":"__init__()"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_o3d_pinhole_camera_parameters","text":"get_o3d_pinhole_camera_parameters () -> o3d . camera . PinholeCameraParameters () Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. RETURNS DESCRIPTION o3d . camera . PinholeCameraParameters () The pinhole camera parameters. Source code in cpas_toolbox/camera_utils.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def get_o3d_pinhole_camera_parameters ( self ) -> o3d . camera . PinholeCameraParameters (): \"\"\"Convert camera to Open3D pinhole camera parameters. Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew. Returns: The pinhole camera parameters. \"\"\" fx , fy , cx , cy , _ = self . get_pinhole_camera_parameters ( 0 ) params = o3d . camera . PinholeCameraParameters () params . intrinsic . set_intrinsics ( self . width , self . height , fx , fy , cx , cy ) params . extrinsic = np . array ( [[ 1 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 ], [ 0 , 0 , 1 , 0 ], [ 0 , 0 , 0 , 1 ]] ) return params","title":"get_o3d_pinhole_camera_parameters()"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_pinhole_camera_parameters","text":"get_pinhole_camera_parameters ( pixel_center : float ) -> Tuple Convert camera to general camera parameters. PARAMETER DESCRIPTION pixel_center At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. TYPE: float RETURNS DESCRIPTION Tuple fx, fy: The horizontal and vertical focal length Tuple cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. Tuple s: The skew. Source code in cpas_toolbox/camera_utils.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_pinhole_camera_parameters ( self , pixel_center : float ) -> Tuple : \"\"\"Convert camera to general camera parameters. Args: pixel_center: At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info. Returns: - fx, fy: The horizontal and vertical focal length - cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction. - s: The skew. \"\"\" cx_corrected = self . cx - self . pixel_center + pixel_center cy_corrected = self . cy - self . pixel_center + pixel_center return self . fx , self . fy , cx_corrected , cy_corrected , self . s","title":"get_pinhole_camera_parameters()"},{"location":"api_reference/cpas_method/","text":"cpas_toolbox.cpas_method This module defines the interface for categorical pose and shape estimation methods. This module defines two classes: PredictionDict and CPASMethod. PredictionDict defines the prediction produced by a CPASMethod. CPASMethod defines the interface used to evaluate categorical pose and shape estimation methods. PredictionDict Bases: TypedDict Pose and shape prediction. ATTRIBUTE DESCRIPTION position Position of object center in camera frame. OpenCV convention. Shape (3,). TYPE: torch . Tensor orientation Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,). TYPE: torch . Tensor extents Bounding box side lengths, shape (3,). TYPE: torch . Tensor reconstructed_pointcloud Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction. TYPE: Optional [ torch . Tensor ] reconstructed_mesh Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction. TYPE: Optional [ o3d . geometry . TriangleMesh ] Source code in cpas_toolbox/cpas_method.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PredictionDict ( TypedDict ): \"\"\"Pose and shape prediction. Attributes: position: Position of object center in camera frame. OpenCV convention. Shape (3,). orientation: Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,). extents: Bounding box side lengths, shape (3,). reconstructed_pointcloud: Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction. reconstructed_mesh: Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction. \"\"\" position : torch . Tensor orientation : torch . Tensor extents : torch . Tensor reconstructed_pointcloud : Optional [ torch . Tensor ] reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ] CPASMethod Bases: ABC Interface class for categorical pose and shape estimation methods. Source code in cpas_toolbox/cpas_method.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class CPASMethod ( ABC ): \"\"\"Interface class for categorical pose and shape estimation methods.\"\"\" def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"Run a method to predict pose and shape of an object. Args: color_image: The color image, shape (H, W, 3), RGB, 0-1, float. depth_image: The depth image, shape (H, W), meters, float. instance_mask: Mask of object of interest. (H, W), bool. category_str: The category of the object. \"\"\" pass inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict Run a method to predict pose and shape of an object. PARAMETER DESCRIPTION color_image The color image, shape (H, W, 3), RGB, 0-1, float. TYPE: torch . Tensor depth_image The depth image, shape (H, W), meters, float. TYPE: torch . Tensor instance_mask Mask of object of interest. (H, W), bool. TYPE: torch . Tensor category_str The category of the object. TYPE: str Source code in cpas_toolbox/cpas_method.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"Run a method to predict pose and shape of an object. Args: color_image: The color image, shape (H, W, 3), RGB, 0-1, float. depth_image: The depth image, shape (H, W), meters, float. instance_mask: Mask of object of interest. (H, W), bool. category_str: The category of the object. \"\"\" pass","title":"cpas_method.py"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method","text":"This module defines the interface for categorical pose and shape estimation methods. This module defines two classes: PredictionDict and CPASMethod. PredictionDict defines the prediction produced by a CPASMethod. CPASMethod defines the interface used to evaluate categorical pose and shape estimation methods.","title":"cpas_method"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.PredictionDict","text":"Bases: TypedDict Pose and shape prediction. ATTRIBUTE DESCRIPTION position Position of object center in camera frame. OpenCV convention. Shape (3,). TYPE: torch . Tensor orientation Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,). TYPE: torch . Tensor extents Bounding box side lengths, shape (3,). TYPE: torch . Tensor reconstructed_pointcloud Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction. TYPE: Optional [ torch . Tensor ] reconstructed_mesh Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction. TYPE: Optional [ o3d . geometry . TriangleMesh ] Source code in cpas_toolbox/cpas_method.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PredictionDict ( TypedDict ): \"\"\"Pose and shape prediction. Attributes: position: Position of object center in camera frame. OpenCV convention. Shape (3,). orientation: Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,). extents: Bounding box side lengths, shape (3,). reconstructed_pointcloud: Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction. reconstructed_mesh: Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction. \"\"\" position : torch . Tensor orientation : torch . Tensor extents : torch . Tensor reconstructed_pointcloud : Optional [ torch . Tensor ] reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ]","title":"PredictionDict"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod","text":"Bases: ABC Interface class for categorical pose and shape estimation methods. Source code in cpas_toolbox/cpas_method.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class CPASMethod ( ABC ): \"\"\"Interface class for categorical pose and shape estimation methods.\"\"\" def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"Run a method to predict pose and shape of an object. Args: color_image: The color image, shape (H, W, 3), RGB, 0-1, float. depth_image: The depth image, shape (H, W), meters, float. instance_mask: Mask of object of interest. (H, W), bool. category_str: The category of the object. \"\"\" pass","title":"CPASMethod"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict Run a method to predict pose and shape of an object. PARAMETER DESCRIPTION color_image The color image, shape (H, W, 3), RGB, 0-1, float. TYPE: torch . Tensor depth_image The depth image, shape (H, W), meters, float. TYPE: torch . Tensor instance_mask Mask of object of interest. (H, W), bool. TYPE: torch . Tensor category_str The category of the object. TYPE: str Source code in cpas_toolbox/cpas_method.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"Run a method to predict pose and shape of an object. Args: color_image: The color image, shape (H, W, 3), RGB, 0-1, float. depth_image: The depth image, shape (H, W), meters, float. instance_mask: Mask of object of interest. (H, W), bool. category_str: The category of the object. \"\"\" pass","title":"inference()"},{"location":"api_reference/evaluate/","text":"cpas_toolbox.evaluate Script to run pose and shape evaluation for different datasets and methods. Evaluator Class to evaluate various pose and shape estimation algorithms. Source code in cpas_toolbox/evaluate.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 class Evaluator : \"\"\"Class to evaluate various pose and shape estimation algorithms.\"\"\" # ShapeNetV2 convention for all objects and datasets assumed # for simplicity assume all cans, bowls and bottles to be rotation symmetric SYMMETRY_AXIS_DICT = { \"mug\" : None , \"laptop\" : None , \"camera\" : None , \"can\" : 1 , \"bowl\" : 1 , \"bottle\" : 1 , } def __init__ ( self , config : dict ) -> None : \"\"\"Initialize model wrappers and evaluator.\"\"\" self . _parse_config ( config ) def _parse_config ( self , config : dict ) -> None : \"\"\"Read config and initialize method wrappers.\"\"\" self . _init_dataset ( config [ \"dataset_config\" ]) self . _visualize_input = config [ \"visualize_input\" ] self . _visualize_prediction = config [ \"visualize_prediction\" ] self . _visualize_gt = config [ \"visualize_gt\" ] self . _fast_eval = config [ \"fast_eval\" ] self . _store_visualization = config [ \"store_visualization\" ] self . _run_name = ( f \" { self . _dataset_name } _eval_ { config [ 'run_name' ] } _\" f \" { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" ) self . _out_dir_path = config [ \"out_dir\" ] self . _metrics = config [ \"metrics\" ] self . _num_gt_points = config [ \"num_gt_points\" ] self . _vis_camera_json = config [ \"vis_camera_json\" ] self . _render_options_json = config [ \"render_options_json\" ] self . _cam = camera_utils . Camera ( ** config [ \"camera\" ]) self . _init_wrappers ( config [ \"methods\" ]) self . _config = config def _init_dataset ( self , dataset_config : dict ) -> None : \"\"\"Initialize reading of dataset. This includes sanity checks whether the provided path is correct. \"\"\" self . _dataset_name = dataset_config [ \"name\" ] print ( f \"Initializing { self . _dataset_name } dataset...\" ) dataset_type = utils . str_to_object ( dataset_config [ \"type\" ]) self . _dataset = dataset_type ( config = dataset_config [ \"config_dict\" ]) # Faster but probably only worth it if whole evaluation supports batches # self._dataloader = DataLoader(self._dataset, 1, num_workers=8) if len ( self . _dataset ) == 0 : print ( f \"No images found for dataset { self . _dataset_name } \" ) exit () print ( f \" { len ( self . _dataset ) } samples found for dataset { self . _dataset_name } .\" ) def _init_wrappers ( self , method_configs : dict ) -> None : \"\"\"Initialize method wrappers.\"\"\" self . _wrappers = {} for method_dict in method_configs . values (): method_name = method_dict [ \"name\" ] print ( f \"Initializing { method_name } ...\" ) method_type = utils . str_to_object ( method_dict [ \"method_type\" ]) if method_type is None : print ( f \"Could not find class { method_dict [ 'method_type' ] } \" ) continue self . _wrappers [ method_name ] = method_type ( config = method_dict [ \"config_dict\" ], camera = self . _cam ) def _eval_method ( self , method_name : str , method_wrapper : CPASMethod ) -> None : \"\"\"Run and evaluate method on all samples.\"\"\" print ( f \"Run { method_name } ...\" ) self . _init_metrics () indices = list ( range ( len ( self . _dataset ))) random . seed ( 0 ) random . shuffle ( indices ) for i in tqdm ( indices ): if self . _fast_eval and i % 10 != 0 : continue sample = self . _dataset [ i ] if self . _visualize_input : _ , (( ax1 , ax2 ), ( ax3 , _ )) = plt . subplots ( 2 , 2 ) ax1 . imshow ( sample [ \"color\" ] . numpy ()) ax2 . imshow ( sample [ \"depth\" ] . numpy ()) ax3 . imshow ( sample [ \"mask\" ] . numpy ()) plt . show () t_start = time . time () prediction = method_wrapper . inference ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], instance_mask = sample [ \"mask\" ], category_str = sample [ \"category_str\" ], ) inference_time = time . time () - t_start self . _runtime_data [ \"total\" ] += inference_time self . _runtime_data [ \"count\" ] += 1 if self . _visualize_gt : visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = sample [ \"position\" ], local_cv_orientation_q = sample [ \"quaternion\" ], reconstructed_mesh = self . _dataset . load_mesh ( sample [ \"obj_path\" ]), extents = sample [ \"scale\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , ) if self . _visualize_prediction : visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = prediction [ \"position\" ], local_cv_orientation_q = prediction [ \"orientation\" ], extents = prediction [ \"extents\" ], reconstructed_points = prediction [ \"reconstructed_pointcloud\" ], reconstructed_mesh = prediction [ \"reconstructed_mesh\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , ) if self . _store_visualization : vis_dir_path = os . path . join ( self . _out_dir_path , self . _run_name , \"visualization\" ) os . makedirs ( vis_dir_path , exist_ok = True ) vis_file_path = os . path . join ( vis_dir_path , f \" { i : 06 } _ { method_name } .jpg\" ) visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = prediction [ \"position\" ], local_cv_orientation_q = prediction [ \"orientation\" ], extents = prediction [ \"extents\" ], reconstructed_points = prediction [ \"reconstructed_pointcloud\" ], reconstructed_mesh = prediction [ \"reconstructed_mesh\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , vis_file_path = vis_file_path , ) self . _eval_prediction ( prediction , sample ) self . _finalize_metrics ( method_name ) def _eval_prediction ( self , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate all metrics for a prediction.\"\"\" # correctness metric for metric_name in self . _metrics . keys (): self . _eval_metric ( metric_name , prediction , sample ) def _init_metrics ( self ) -> None : \"\"\"Initialize metrics.\"\"\" self . _metric_data = {} self . _runtime_data = { \"total\" : 0.0 , \"count\" : 0.0 , } for metric_name , metric_config_dict in self . _metrics . items (): self . _metric_data [ metric_name ] = self . _init_metric_data ( metric_config_dict ) def _init_metric_data ( self , metric_config_dict : dict ) -> dict : \"\"\"Create data structure necessary to compute a metric.\"\"\" metric_data = {} if \"position_thresholds\" in metric_config_dict : pts = metric_config_dict [ \"position_thresholds\" ] dts = metric_config_dict [ \"deg_thresholds\" ] its = metric_config_dict [ \"iou_thresholds\" ] fts = metric_config_dict [ \"f_thresholds\" ] metric_data [ \"correct_counters\" ] = np . zeros ( ( len ( pts ), len ( dts ), len ( its ), len ( fts ), self . _dataset . num_categories + 1 , ) ) metric_data [ \"total_counters\" ] = np . zeros ( self . _dataset . num_categories + 1 ) elif \"pointwise_f\" in metric_config_dict : metric_data [ \"means\" ] = np . zeros ( self . _dataset . num_categories + 1 ) metric_data [ \"m2s\" ] = np . zeros ( self . _dataset . num_categories + 1 ) metric_data [ \"counts\" ] = np . zeros ( self . _dataset . num_categories + 1 ) else : raise NotImplementedError ( \"Unsupported metric configuration.\" ) return metric_data def _eval_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_config_dict = self . _metrics [ metric_name ] if \"position_thresholds\" in metric_config_dict : # correctness metrics self . _eval_correctness_metric ( metric_name , prediction , sample ) elif \"pointwise_f\" in metric_config_dict : # pointwise reconstruction metrics self . _eval_pointwise_metric ( metric_name , prediction , sample ) else : raise NotImplementedError ( f \"Unsupported metric configuration with name { metric_name } .\" ) def _eval_correctness_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single correctness metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_dict = self . _metrics [ metric_name ] correct_counters = self . _metric_data [ metric_name ][ \"correct_counters\" ] total_counters = self . _metric_data [ metric_name ][ \"total_counters\" ] category_id = sample [ \"category_id\" ] total_counters [ category_id ] += 1 total_counters [ - 1 ] += 1 gt_points , pred_points = self . _get_points ( sample , prediction , True ) for pi , p in enumerate ( metric_dict [ \"position_thresholds\" ]): for di , d in enumerate ( metric_dict [ \"deg_thresholds\" ]): for ii , i in enumerate ( metric_dict [ \"iou_thresholds\" ]): for fi , f in enumerate ( metric_dict [ \"f_thresholds\" ]): correct = metrics . correct_thresh ( position_gt = sample [ \"position\" ] . cpu () . numpy (), position_prediction = prediction [ \"position\" ] . cpu () . numpy (), orientation_gt = Rotation . from_quat ( sample [ \"quaternion\" ]), orientation_prediction = Rotation . from_quat ( prediction [ \"orientation\" ] ), extent_gt = sample [ \"scale\" ] . cpu () . numpy (), extent_prediction = prediction [ \"extents\" ] . cpu () . numpy (), points_gt = gt_points , points_prediction = pred_points , position_threshold = p , degree_threshold = d , iou_3d_threshold = i , fscore_threshold = f , rotational_symmetry_axis = self . SYMMETRY_AXIS_DICT [ sample [ \"category_str\" ] ], ) correct_counters [ pi , di , ii , fi , category_id ] += correct correct_counters [ pi , di , ii , fi , - 1 ] += correct # all def _eval_pointwise_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single pointwise metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_config_dict = self . _metrics [ metric_name ] means = self . _metric_data [ metric_name ][ \"means\" ] m2s = self . _metric_data [ metric_name ][ \"m2s\" ] counts = self . _metric_data [ metric_name ][ \"counts\" ] category_id = sample [ \"category_id\" ] point_metric = utils . str_to_object ( metric_config_dict [ \"pointwise_f\" ]) gt_points , pred_points = self . _get_points ( sample , prediction , metric_config_dict [ \"posed\" ] ) result = point_metric ( gt_points . numpy (), pred_points . numpy (), ** metric_config_dict [ \"kwargs\" ] ) # Use Welfords algorithm to update mean and variance # for category counts [ category_id ] += 1 delta = result - means [ category_id ] means [ category_id ] += delta / counts [ category_id ] delta2 = result - means [ category_id ] m2s [ category_id ] += delta * delta2 # for all counts [ - 1 ] += 1 delta = result - means [ - 1 ] means [ - 1 ] += delta / counts [ - 1 ] delta2 = result - means [ - 1 ] m2s [ - 1 ] += delta * delta2 def _get_points ( self , sample : dict , prediction : PredictionDict , posed : bool ) -> Tuple [ np . ndarray ]: # load ground truth mesh gt_mesh = self . _dataset . load_mesh ( sample [ \"obj_path\" ]) gt_points = torch . from_numpy ( np . asarray ( gt_mesh . sample_points_uniformly ( self . _num_gt_points ) . points ) ) pred_points = prediction [ \"reconstructed_pointcloud\" ] # transform points if posed if posed : gt_points = quaternion_utils . quaternion_apply ( sample [ \"quaternion\" ], gt_points ) gt_points += sample [ \"position\" ] pred_points = quaternion_utils . quaternion_apply ( prediction [ \"orientation\" ], pred_points ) pred_points += prediction [ \"position\" ] return gt_points , pred_points def _finalize_metrics ( self , method_name : str ) -> None : \"\"\"Finalize metrics after all samples have been evaluated. Also writes results to disk and create plot if applicable. \"\"\" results_dir_path = os . path . join ( self . _out_dir_path , self . _run_name ) os . makedirs ( results_dir_path , exist_ok = True ) yaml_file_path = os . path . join ( results_dir_path , \"results.yaml\" ) self . _results_dict [ method_name ] = {} self . _runtime_results_dict [ method_name ] = ( self . _runtime_data [ \"total\" ] / self . _runtime_data [ \"count\" ] ) for metric_name , metric_dict in self . _metrics . items (): if \"position_thresholds\" in metric_dict : # correctness metrics correct_counter = self . _metric_data [ metric_name ][ \"correct_counters\" ] total_counter = self . _metric_data [ metric_name ][ \"total_counters\" ] correct_percentage = correct_counter / total_counter self . _results_dict [ method_name ][ metric_name ] = correct_percentage . tolist () self . _create_metric_plot ( method_name , metric_name , metric_dict , correct_percentage , results_dir_path , ) elif \"pointwise_f\" in metric_dict : # pointwise reconstruction metrics counts = self . _metric_data [ metric_name ][ \"counts\" ] m2s = self . _metric_data [ metric_name ][ \"m2s\" ] means = self . _metric_data [ metric_name ][ \"means\" ] variances = m2s / counts stds = np . sqrt ( variances ) self . _results_dict [ method_name ][ metric_name ] = { \"means\" : means . tolist (), \"variances\" : variances . tolist (), \"std\" : stds . tolist (), } else : raise NotImplementedError ( f \"Unsupported metric configuration with name { metric_name } .\" ) results_dict = { ** self . _config , \"results\" : self . _results_dict , \"runtime_results\" : self . _runtime_results_dict , } yoco . save_config_to_file ( yaml_file_path , results_dict ) print ( f \"Results saved to: { yaml_file_path } \" ) def _create_metric_plot ( self , method_name : str , metric_name : str , metric_dict : dict , correct_percentage : np . ndarray , out_dir : str , ) -> None : \"\"\"Create metric plot if applicable. Applicable means only one of the thresholds has multiple values. Args: correct_percentage: Array holding the percentage of correct predictions. Shape (NUM_POS_THRESH,NUM_DEG_THRESH,NUM_IOU_THRESH,NUM_CATEGORIES + 1). \"\"\" axis = None for i , s in enumerate ( correct_percentage . shape [: 4 ]): if s != 1 and axis is None : axis = i elif s != 1 : # multiple axis with != 1 size return if axis is None : return axis_to_threshold_key = { 0 : \"position_thresholds\" , 1 : \"deg_thresholds\" , 2 : \"iou_thresholds\" , 3 : \"f_thresholds\" , } threshold_key = axis_to_threshold_key [ axis ] x_values = metric_dict [ threshold_key ] for category_id in range ( self . _dataset . num_categories + 1 ): y_values = correct_percentage [ ... , category_id ] . flatten () if category_id in self . _dataset . category_id_to_str : label = self . _dataset . category_id_to_str [ category_id ] else : label = \"all\" plt . plot ( x_values , y_values , label = label ) figure_file_path = os . path . join ( out_dir , f \" { method_name } _ { metric_name } \" ) plt . xlabel ( threshold_key ) plt . ylabel ( \"Correct\" ) plt . legend () plt . grid () tikzplotlib . save ( figure_file_path + \".tex\" ) plt . savefig ( figure_file_path + \".png\" ) plt . close () def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" self . _results_dict = {} self . _runtime_results_dict = {} for method_name , method_wrapper in self . _wrappers . items (): self . _eval_method ( method_name , method_wrapper ) __init__ __init__ ( config : dict ) -> None Initialize model wrappers and evaluator. Source code in cpas_toolbox/evaluate.py 176 177 178 def __init__ ( self , config : dict ) -> None : \"\"\"Initialize model wrappers and evaluator.\"\"\" self . _parse_config ( config ) run run () -> None Run the evaluation. Source code in cpas_toolbox/evaluate.py 582 583 584 585 586 587 def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" self . _results_dict = {} self . _runtime_results_dict = {} for method_name , method_wrapper in self . _wrappers . items (): self . _eval_method ( method_name , method_wrapper ) visualize_estimation visualize_estimation ( color_image : torch . Tensor , depth_image : torch . Tensor , local_cv_position : torch . Tensor , local_cv_orientation_q : torch . Tensor , camera : camera_utils . Camera , instance_mask : Optional [ torch . Tensor ] = None , extents : Optional [ torch . Tensor ] = None , reconstructed_points : Optional [ torch . Tensor ] = None , reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ] = None , vis_camera_json : Optional [ str ] = None , render_options_json : Optional [ str ] = None , vis_file_path : Optional [ str ] = None , ) -> None Visualize prediction and ask for confirmation. PARAMETER DESCRIPTION color_image The unmasked color image. Shape (H,W,3), RGB, 0-1, float. TYPE: torch . Tensor depth_image The unmasked depth image. Shape (H,W), float (meters along z). TYPE: torch . Tensor local_cv_position The position in the OpenCV camera frame. Shape (3,). TYPE: torch . Tensor local_cv_orientation_q The orientation in the OpenCV camera frame. Scalar last, shape (4,). TYPE: torch . Tensor extents Extents of the bounding box. Not visualized if None. Shape (3,). TYPE: Optional [ torch . Tensor ] DEFAULT: None instance_mask The instance mask. No masking if None. Shape (H,W). TYPE: Optional [ torch . Tensor ] DEFAULT: None reconstructed_points Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3). TYPE: Optional [ torch . Tensor ] DEFAULT: None reconstructed_mesh Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled. TYPE: Optional [ o3d . geometry . TriangleMesh ] DEFAULT: None vis_camera_json Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None. TYPE: Optional [ str ] DEFAULT: None vis_file_path If not None, the image will be rendered off screen and saved at the specified path. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION None True if confirmation was positive. False if negative. Source code in cpas_toolbox/evaluate.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def visualize_estimation ( color_image : torch . Tensor , depth_image : torch . Tensor , local_cv_position : torch . Tensor , local_cv_orientation_q : torch . Tensor , camera : camera_utils . Camera , instance_mask : Optional [ torch . Tensor ] = None , extents : Optional [ torch . Tensor ] = None , reconstructed_points : Optional [ torch . Tensor ] = None , reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ] = None , vis_camera_json : Optional [ str ] = None , render_options_json : Optional [ str ] = None , vis_file_path : Optional [ str ] = None , ) -> None : \"\"\"Visualize prediction and ask for confirmation. Args: color_image: The unmasked color image. Shape (H,W,3), RGB, 0-1, float. depth_image: The unmasked depth image. Shape (H,W), float (meters along z). local_cv_position: The position in the OpenCV camera frame. Shape (3,). local_cv_orientation_q: The orientation in the OpenCV camera frame. Scalar last, shape (4,). extents: Extents of the bounding box. Not visualized if None. Shape (3,). instance_mask: The instance mask. No masking if None. Shape (H,W). reconstructed_points: Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3). reconstructed_mesh: Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled. vis_camera_json: Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None. vis_file_path: If not None, the image will be rendered off screen and saved at the specified path. Returns: True if confirmation was positive. False if negative. \"\"\" o3d_geometries = [] local_cv_position = local_cv_position . cpu () . double () . numpy () # shape (3,) local_cv_orientation_q = local_cv_orientation_q . cpu () . double () . numpy () # shape (4,) if instance_mask is not None : valid_depth_mask = ( depth_image != 0 ) * instance_mask else : valid_depth_mask = depth_image != 0 pointset_colors = color_image [ valid_depth_mask ] masked_pointset = pointset_utils . depth_to_pointcloud ( depth_image , camera , normalize = False , mask = instance_mask , convention = \"opencv\" , ) o3d_points = o3d . geometry . PointCloud ( points = o3d . utility . Vector3dVector ( masked_pointset . cpu () . numpy ()) ) o3d_points . colors = o3d . utility . Vector3dVector ( pointset_colors . cpu () . numpy ()) o3d_geometries . append ( o3d_points ) # coordinate frame local_cv_orientation_m = Rotation . from_quat ( local_cv_orientation_q ) . as_matrix () o3d_frame = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 ) o3d_frame . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) o3d_frame . translate ( local_cv_position [:, None ]) o3d_geometries . append ( o3d_frame ) o3d_cam_frame = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.3 ) o3d_geometries . append ( o3d_cam_frame ) if extents is not None : extents = extents . cpu () . double () . numpy () o3d_obb = o3d . geometry . OrientedBoundingBox ( center = local_cv_position [:, None ], R = local_cv_orientation_m , extent = extents [:, None ], ) o3d_geometries . append ( o3d_obb ) if reconstructed_points is not None : o3d_rec_points = o3d . geometry . PointCloud ( points = o3d . utility . Vector3dVector ( reconstructed_points . cpu () . numpy ()) ) o3d_rec_points . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) o3d_rec_points . translate ( local_cv_position [:, None ]) o3d_geometries . append ( o3d_rec_points ) if reconstructed_mesh is not None : # copy the mesh to keep original unmoved posed_mesh = o3d . geometry . TriangleMesh ( reconstructed_mesh ) posed_mesh . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) posed_mesh . translate ( local_cv_position [:, None ]) posed_mesh . compute_vertex_normals () o3d_geometries . append ( posed_mesh ) vis = o3d . visualization . Visualizer () if vis_camera_json is not None : vis_camera = o3d . io . read_pinhole_camera_parameters ( vis_camera_json ) width = vis_camera . intrinsic . width height = vis_camera . intrinsic . height else : width = 800 height = 600 vis_camera = None vis . create_window ( width = width , height = height , visible = ( vis_file_path is None )) for g in o3d_geometries : vis . add_geometry ( g ) if vis_camera is not None : view_control = vis . get_view_control () view_control . convert_from_pinhole_camera_parameters ( vis_camera ) if render_options_json is not None : render_option = vis . get_render_option () render_option . load_from_json ( render_options_json ) if vis_file_path is not None : vis . poll_events () vis . update_renderer () vis . capture_screen_image ( vis_file_path , do_render = True ) else : vis . run () main main () -> None Entry point of the evaluation program. Source code in cpas_toolbox/evaluate.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" parser = argparse . ArgumentParser ( description = \"Pose and shape estimation evaluation on REAL275 data\" ) parser . add_argument ( \"--config\" , required = True ) parser . add_argument ( \"--out_dir\" , required = True ) resolved_args = _resolve_config_args ( sys . argv [ 1 :]) config = yoco . load_config_from_args ( parser , resolved_args ) evaluator = Evaluator ( config ) evaluator . run ()","title":"evaluate.py"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate","text":"Script to run pose and shape evaluation for different datasets and methods.","title":"evaluate"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator","text":"Class to evaluate various pose and shape estimation algorithms. Source code in cpas_toolbox/evaluate.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 class Evaluator : \"\"\"Class to evaluate various pose and shape estimation algorithms.\"\"\" # ShapeNetV2 convention for all objects and datasets assumed # for simplicity assume all cans, bowls and bottles to be rotation symmetric SYMMETRY_AXIS_DICT = { \"mug\" : None , \"laptop\" : None , \"camera\" : None , \"can\" : 1 , \"bowl\" : 1 , \"bottle\" : 1 , } def __init__ ( self , config : dict ) -> None : \"\"\"Initialize model wrappers and evaluator.\"\"\" self . _parse_config ( config ) def _parse_config ( self , config : dict ) -> None : \"\"\"Read config and initialize method wrappers.\"\"\" self . _init_dataset ( config [ \"dataset_config\" ]) self . _visualize_input = config [ \"visualize_input\" ] self . _visualize_prediction = config [ \"visualize_prediction\" ] self . _visualize_gt = config [ \"visualize_gt\" ] self . _fast_eval = config [ \"fast_eval\" ] self . _store_visualization = config [ \"store_visualization\" ] self . _run_name = ( f \" { self . _dataset_name } _eval_ { config [ 'run_name' ] } _\" f \" { datetime . now () . strftime ( '%Y-%m- %d _%H-%M-%S' ) } \" ) self . _out_dir_path = config [ \"out_dir\" ] self . _metrics = config [ \"metrics\" ] self . _num_gt_points = config [ \"num_gt_points\" ] self . _vis_camera_json = config [ \"vis_camera_json\" ] self . _render_options_json = config [ \"render_options_json\" ] self . _cam = camera_utils . Camera ( ** config [ \"camera\" ]) self . _init_wrappers ( config [ \"methods\" ]) self . _config = config def _init_dataset ( self , dataset_config : dict ) -> None : \"\"\"Initialize reading of dataset. This includes sanity checks whether the provided path is correct. \"\"\" self . _dataset_name = dataset_config [ \"name\" ] print ( f \"Initializing { self . _dataset_name } dataset...\" ) dataset_type = utils . str_to_object ( dataset_config [ \"type\" ]) self . _dataset = dataset_type ( config = dataset_config [ \"config_dict\" ]) # Faster but probably only worth it if whole evaluation supports batches # self._dataloader = DataLoader(self._dataset, 1, num_workers=8) if len ( self . _dataset ) == 0 : print ( f \"No images found for dataset { self . _dataset_name } \" ) exit () print ( f \" { len ( self . _dataset ) } samples found for dataset { self . _dataset_name } .\" ) def _init_wrappers ( self , method_configs : dict ) -> None : \"\"\"Initialize method wrappers.\"\"\" self . _wrappers = {} for method_dict in method_configs . values (): method_name = method_dict [ \"name\" ] print ( f \"Initializing { method_name } ...\" ) method_type = utils . str_to_object ( method_dict [ \"method_type\" ]) if method_type is None : print ( f \"Could not find class { method_dict [ 'method_type' ] } \" ) continue self . _wrappers [ method_name ] = method_type ( config = method_dict [ \"config_dict\" ], camera = self . _cam ) def _eval_method ( self , method_name : str , method_wrapper : CPASMethod ) -> None : \"\"\"Run and evaluate method on all samples.\"\"\" print ( f \"Run { method_name } ...\" ) self . _init_metrics () indices = list ( range ( len ( self . _dataset ))) random . seed ( 0 ) random . shuffle ( indices ) for i in tqdm ( indices ): if self . _fast_eval and i % 10 != 0 : continue sample = self . _dataset [ i ] if self . _visualize_input : _ , (( ax1 , ax2 ), ( ax3 , _ )) = plt . subplots ( 2 , 2 ) ax1 . imshow ( sample [ \"color\" ] . numpy ()) ax2 . imshow ( sample [ \"depth\" ] . numpy ()) ax3 . imshow ( sample [ \"mask\" ] . numpy ()) plt . show () t_start = time . time () prediction = method_wrapper . inference ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], instance_mask = sample [ \"mask\" ], category_str = sample [ \"category_str\" ], ) inference_time = time . time () - t_start self . _runtime_data [ \"total\" ] += inference_time self . _runtime_data [ \"count\" ] += 1 if self . _visualize_gt : visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = sample [ \"position\" ], local_cv_orientation_q = sample [ \"quaternion\" ], reconstructed_mesh = self . _dataset . load_mesh ( sample [ \"obj_path\" ]), extents = sample [ \"scale\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , ) if self . _visualize_prediction : visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = prediction [ \"position\" ], local_cv_orientation_q = prediction [ \"orientation\" ], extents = prediction [ \"extents\" ], reconstructed_points = prediction [ \"reconstructed_pointcloud\" ], reconstructed_mesh = prediction [ \"reconstructed_mesh\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , ) if self . _store_visualization : vis_dir_path = os . path . join ( self . _out_dir_path , self . _run_name , \"visualization\" ) os . makedirs ( vis_dir_path , exist_ok = True ) vis_file_path = os . path . join ( vis_dir_path , f \" { i : 06 } _ { method_name } .jpg\" ) visualize_estimation ( color_image = sample [ \"color\" ], depth_image = sample [ \"depth\" ], local_cv_position = prediction [ \"position\" ], local_cv_orientation_q = prediction [ \"orientation\" ], extents = prediction [ \"extents\" ], reconstructed_points = prediction [ \"reconstructed_pointcloud\" ], reconstructed_mesh = prediction [ \"reconstructed_mesh\" ], camera = self . _cam , vis_camera_json = self . _vis_camera_json , render_options_json = self . _render_options_json , vis_file_path = vis_file_path , ) self . _eval_prediction ( prediction , sample ) self . _finalize_metrics ( method_name ) def _eval_prediction ( self , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate all metrics for a prediction.\"\"\" # correctness metric for metric_name in self . _metrics . keys (): self . _eval_metric ( metric_name , prediction , sample ) def _init_metrics ( self ) -> None : \"\"\"Initialize metrics.\"\"\" self . _metric_data = {} self . _runtime_data = { \"total\" : 0.0 , \"count\" : 0.0 , } for metric_name , metric_config_dict in self . _metrics . items (): self . _metric_data [ metric_name ] = self . _init_metric_data ( metric_config_dict ) def _init_metric_data ( self , metric_config_dict : dict ) -> dict : \"\"\"Create data structure necessary to compute a metric.\"\"\" metric_data = {} if \"position_thresholds\" in metric_config_dict : pts = metric_config_dict [ \"position_thresholds\" ] dts = metric_config_dict [ \"deg_thresholds\" ] its = metric_config_dict [ \"iou_thresholds\" ] fts = metric_config_dict [ \"f_thresholds\" ] metric_data [ \"correct_counters\" ] = np . zeros ( ( len ( pts ), len ( dts ), len ( its ), len ( fts ), self . _dataset . num_categories + 1 , ) ) metric_data [ \"total_counters\" ] = np . zeros ( self . _dataset . num_categories + 1 ) elif \"pointwise_f\" in metric_config_dict : metric_data [ \"means\" ] = np . zeros ( self . _dataset . num_categories + 1 ) metric_data [ \"m2s\" ] = np . zeros ( self . _dataset . num_categories + 1 ) metric_data [ \"counts\" ] = np . zeros ( self . _dataset . num_categories + 1 ) else : raise NotImplementedError ( \"Unsupported metric configuration.\" ) return metric_data def _eval_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_config_dict = self . _metrics [ metric_name ] if \"position_thresholds\" in metric_config_dict : # correctness metrics self . _eval_correctness_metric ( metric_name , prediction , sample ) elif \"pointwise_f\" in metric_config_dict : # pointwise reconstruction metrics self . _eval_pointwise_metric ( metric_name , prediction , sample ) else : raise NotImplementedError ( f \"Unsupported metric configuration with name { metric_name } .\" ) def _eval_correctness_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single correctness metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_dict = self . _metrics [ metric_name ] correct_counters = self . _metric_data [ metric_name ][ \"correct_counters\" ] total_counters = self . _metric_data [ metric_name ][ \"total_counters\" ] category_id = sample [ \"category_id\" ] total_counters [ category_id ] += 1 total_counters [ - 1 ] += 1 gt_points , pred_points = self . _get_points ( sample , prediction , True ) for pi , p in enumerate ( metric_dict [ \"position_thresholds\" ]): for di , d in enumerate ( metric_dict [ \"deg_thresholds\" ]): for ii , i in enumerate ( metric_dict [ \"iou_thresholds\" ]): for fi , f in enumerate ( metric_dict [ \"f_thresholds\" ]): correct = metrics . correct_thresh ( position_gt = sample [ \"position\" ] . cpu () . numpy (), position_prediction = prediction [ \"position\" ] . cpu () . numpy (), orientation_gt = Rotation . from_quat ( sample [ \"quaternion\" ]), orientation_prediction = Rotation . from_quat ( prediction [ \"orientation\" ] ), extent_gt = sample [ \"scale\" ] . cpu () . numpy (), extent_prediction = prediction [ \"extents\" ] . cpu () . numpy (), points_gt = gt_points , points_prediction = pred_points , position_threshold = p , degree_threshold = d , iou_3d_threshold = i , fscore_threshold = f , rotational_symmetry_axis = self . SYMMETRY_AXIS_DICT [ sample [ \"category_str\" ] ], ) correct_counters [ pi , di , ii , fi , category_id ] += correct correct_counters [ pi , di , ii , fi , - 1 ] += correct # all def _eval_pointwise_metric ( self , metric_name : str , prediction : PredictionDict , sample : dict ) -> None : \"\"\"Evaluate and update single pointwise metric for a single prediction. Args: metric_name: Name of metric to evaluate. prediction: Dictionary containing prediction data. sample: Sample containing ground truth information. \"\"\" metric_config_dict = self . _metrics [ metric_name ] means = self . _metric_data [ metric_name ][ \"means\" ] m2s = self . _metric_data [ metric_name ][ \"m2s\" ] counts = self . _metric_data [ metric_name ][ \"counts\" ] category_id = sample [ \"category_id\" ] point_metric = utils . str_to_object ( metric_config_dict [ \"pointwise_f\" ]) gt_points , pred_points = self . _get_points ( sample , prediction , metric_config_dict [ \"posed\" ] ) result = point_metric ( gt_points . numpy (), pred_points . numpy (), ** metric_config_dict [ \"kwargs\" ] ) # Use Welfords algorithm to update mean and variance # for category counts [ category_id ] += 1 delta = result - means [ category_id ] means [ category_id ] += delta / counts [ category_id ] delta2 = result - means [ category_id ] m2s [ category_id ] += delta * delta2 # for all counts [ - 1 ] += 1 delta = result - means [ - 1 ] means [ - 1 ] += delta / counts [ - 1 ] delta2 = result - means [ - 1 ] m2s [ - 1 ] += delta * delta2 def _get_points ( self , sample : dict , prediction : PredictionDict , posed : bool ) -> Tuple [ np . ndarray ]: # load ground truth mesh gt_mesh = self . _dataset . load_mesh ( sample [ \"obj_path\" ]) gt_points = torch . from_numpy ( np . asarray ( gt_mesh . sample_points_uniformly ( self . _num_gt_points ) . points ) ) pred_points = prediction [ \"reconstructed_pointcloud\" ] # transform points if posed if posed : gt_points = quaternion_utils . quaternion_apply ( sample [ \"quaternion\" ], gt_points ) gt_points += sample [ \"position\" ] pred_points = quaternion_utils . quaternion_apply ( prediction [ \"orientation\" ], pred_points ) pred_points += prediction [ \"position\" ] return gt_points , pred_points def _finalize_metrics ( self , method_name : str ) -> None : \"\"\"Finalize metrics after all samples have been evaluated. Also writes results to disk and create plot if applicable. \"\"\" results_dir_path = os . path . join ( self . _out_dir_path , self . _run_name ) os . makedirs ( results_dir_path , exist_ok = True ) yaml_file_path = os . path . join ( results_dir_path , \"results.yaml\" ) self . _results_dict [ method_name ] = {} self . _runtime_results_dict [ method_name ] = ( self . _runtime_data [ \"total\" ] / self . _runtime_data [ \"count\" ] ) for metric_name , metric_dict in self . _metrics . items (): if \"position_thresholds\" in metric_dict : # correctness metrics correct_counter = self . _metric_data [ metric_name ][ \"correct_counters\" ] total_counter = self . _metric_data [ metric_name ][ \"total_counters\" ] correct_percentage = correct_counter / total_counter self . _results_dict [ method_name ][ metric_name ] = correct_percentage . tolist () self . _create_metric_plot ( method_name , metric_name , metric_dict , correct_percentage , results_dir_path , ) elif \"pointwise_f\" in metric_dict : # pointwise reconstruction metrics counts = self . _metric_data [ metric_name ][ \"counts\" ] m2s = self . _metric_data [ metric_name ][ \"m2s\" ] means = self . _metric_data [ metric_name ][ \"means\" ] variances = m2s / counts stds = np . sqrt ( variances ) self . _results_dict [ method_name ][ metric_name ] = { \"means\" : means . tolist (), \"variances\" : variances . tolist (), \"std\" : stds . tolist (), } else : raise NotImplementedError ( f \"Unsupported metric configuration with name { metric_name } .\" ) results_dict = { ** self . _config , \"results\" : self . _results_dict , \"runtime_results\" : self . _runtime_results_dict , } yoco . save_config_to_file ( yaml_file_path , results_dict ) print ( f \"Results saved to: { yaml_file_path } \" ) def _create_metric_plot ( self , method_name : str , metric_name : str , metric_dict : dict , correct_percentage : np . ndarray , out_dir : str , ) -> None : \"\"\"Create metric plot if applicable. Applicable means only one of the thresholds has multiple values. Args: correct_percentage: Array holding the percentage of correct predictions. Shape (NUM_POS_THRESH,NUM_DEG_THRESH,NUM_IOU_THRESH,NUM_CATEGORIES + 1). \"\"\" axis = None for i , s in enumerate ( correct_percentage . shape [: 4 ]): if s != 1 and axis is None : axis = i elif s != 1 : # multiple axis with != 1 size return if axis is None : return axis_to_threshold_key = { 0 : \"position_thresholds\" , 1 : \"deg_thresholds\" , 2 : \"iou_thresholds\" , 3 : \"f_thresholds\" , } threshold_key = axis_to_threshold_key [ axis ] x_values = metric_dict [ threshold_key ] for category_id in range ( self . _dataset . num_categories + 1 ): y_values = correct_percentage [ ... , category_id ] . flatten () if category_id in self . _dataset . category_id_to_str : label = self . _dataset . category_id_to_str [ category_id ] else : label = \"all\" plt . plot ( x_values , y_values , label = label ) figure_file_path = os . path . join ( out_dir , f \" { method_name } _ { metric_name } \" ) plt . xlabel ( threshold_key ) plt . ylabel ( \"Correct\" ) plt . legend () plt . grid () tikzplotlib . save ( figure_file_path + \".tex\" ) plt . savefig ( figure_file_path + \".png\" ) plt . close () def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" self . _results_dict = {} self . _runtime_results_dict = {} for method_name , method_wrapper in self . _wrappers . items (): self . _eval_method ( method_name , method_wrapper )","title":"Evaluator"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.__init__","text":"__init__ ( config : dict ) -> None Initialize model wrappers and evaluator. Source code in cpas_toolbox/evaluate.py 176 177 178 def __init__ ( self , config : dict ) -> None : \"\"\"Initialize model wrappers and evaluator.\"\"\" self . _parse_config ( config )","title":"__init__()"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.run","text":"run () -> None Run the evaluation. Source code in cpas_toolbox/evaluate.py 582 583 584 585 586 587 def run ( self ) -> None : \"\"\"Run the evaluation.\"\"\" self . _results_dict = {} self . _runtime_results_dict = {} for method_name , method_wrapper in self . _wrappers . items (): self . _eval_method ( method_name , method_wrapper )","title":"run()"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.visualize_estimation","text":"visualize_estimation ( color_image : torch . Tensor , depth_image : torch . Tensor , local_cv_position : torch . Tensor , local_cv_orientation_q : torch . Tensor , camera : camera_utils . Camera , instance_mask : Optional [ torch . Tensor ] = None , extents : Optional [ torch . Tensor ] = None , reconstructed_points : Optional [ torch . Tensor ] = None , reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ] = None , vis_camera_json : Optional [ str ] = None , render_options_json : Optional [ str ] = None , vis_file_path : Optional [ str ] = None , ) -> None Visualize prediction and ask for confirmation. PARAMETER DESCRIPTION color_image The unmasked color image. Shape (H,W,3), RGB, 0-1, float. TYPE: torch . Tensor depth_image The unmasked depth image. Shape (H,W), float (meters along z). TYPE: torch . Tensor local_cv_position The position in the OpenCV camera frame. Shape (3,). TYPE: torch . Tensor local_cv_orientation_q The orientation in the OpenCV camera frame. Scalar last, shape (4,). TYPE: torch . Tensor extents Extents of the bounding box. Not visualized if None. Shape (3,). TYPE: Optional [ torch . Tensor ] DEFAULT: None instance_mask The instance mask. No masking if None. Shape (H,W). TYPE: Optional [ torch . Tensor ] DEFAULT: None reconstructed_points Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3). TYPE: Optional [ torch . Tensor ] DEFAULT: None reconstructed_mesh Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled. TYPE: Optional [ o3d . geometry . TriangleMesh ] DEFAULT: None vis_camera_json Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None. TYPE: Optional [ str ] DEFAULT: None vis_file_path If not None, the image will be rendered off screen and saved at the specified path. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION None True if confirmation was positive. False if negative. Source code in cpas_toolbox/evaluate.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def visualize_estimation ( color_image : torch . Tensor , depth_image : torch . Tensor , local_cv_position : torch . Tensor , local_cv_orientation_q : torch . Tensor , camera : camera_utils . Camera , instance_mask : Optional [ torch . Tensor ] = None , extents : Optional [ torch . Tensor ] = None , reconstructed_points : Optional [ torch . Tensor ] = None , reconstructed_mesh : Optional [ o3d . geometry . TriangleMesh ] = None , vis_camera_json : Optional [ str ] = None , render_options_json : Optional [ str ] = None , vis_file_path : Optional [ str ] = None , ) -> None : \"\"\"Visualize prediction and ask for confirmation. Args: color_image: The unmasked color image. Shape (H,W,3), RGB, 0-1, float. depth_image: The unmasked depth image. Shape (H,W), float (meters along z). local_cv_position: The position in the OpenCV camera frame. Shape (3,). local_cv_orientation_q: The orientation in the OpenCV camera frame. Scalar last, shape (4,). extents: Extents of the bounding box. Not visualized if None. Shape (3,). instance_mask: The instance mask. No masking if None. Shape (H,W). reconstructed_points: Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3). reconstructed_mesh: Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled. vis_camera_json: Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None. vis_file_path: If not None, the image will be rendered off screen and saved at the specified path. Returns: True if confirmation was positive. False if negative. \"\"\" o3d_geometries = [] local_cv_position = local_cv_position . cpu () . double () . numpy () # shape (3,) local_cv_orientation_q = local_cv_orientation_q . cpu () . double () . numpy () # shape (4,) if instance_mask is not None : valid_depth_mask = ( depth_image != 0 ) * instance_mask else : valid_depth_mask = depth_image != 0 pointset_colors = color_image [ valid_depth_mask ] masked_pointset = pointset_utils . depth_to_pointcloud ( depth_image , camera , normalize = False , mask = instance_mask , convention = \"opencv\" , ) o3d_points = o3d . geometry . PointCloud ( points = o3d . utility . Vector3dVector ( masked_pointset . cpu () . numpy ()) ) o3d_points . colors = o3d . utility . Vector3dVector ( pointset_colors . cpu () . numpy ()) o3d_geometries . append ( o3d_points ) # coordinate frame local_cv_orientation_m = Rotation . from_quat ( local_cv_orientation_q ) . as_matrix () o3d_frame = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.1 ) o3d_frame . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) o3d_frame . translate ( local_cv_position [:, None ]) o3d_geometries . append ( o3d_frame ) o3d_cam_frame = o3d . geometry . TriangleMesh . create_coordinate_frame ( size = 0.3 ) o3d_geometries . append ( o3d_cam_frame ) if extents is not None : extents = extents . cpu () . double () . numpy () o3d_obb = o3d . geometry . OrientedBoundingBox ( center = local_cv_position [:, None ], R = local_cv_orientation_m , extent = extents [:, None ], ) o3d_geometries . append ( o3d_obb ) if reconstructed_points is not None : o3d_rec_points = o3d . geometry . PointCloud ( points = o3d . utility . Vector3dVector ( reconstructed_points . cpu () . numpy ()) ) o3d_rec_points . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) o3d_rec_points . translate ( local_cv_position [:, None ]) o3d_geometries . append ( o3d_rec_points ) if reconstructed_mesh is not None : # copy the mesh to keep original unmoved posed_mesh = o3d . geometry . TriangleMesh ( reconstructed_mesh ) posed_mesh . rotate ( local_cv_orientation_m , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) posed_mesh . translate ( local_cv_position [:, None ]) posed_mesh . compute_vertex_normals () o3d_geometries . append ( posed_mesh ) vis = o3d . visualization . Visualizer () if vis_camera_json is not None : vis_camera = o3d . io . read_pinhole_camera_parameters ( vis_camera_json ) width = vis_camera . intrinsic . width height = vis_camera . intrinsic . height else : width = 800 height = 600 vis_camera = None vis . create_window ( width = width , height = height , visible = ( vis_file_path is None )) for g in o3d_geometries : vis . add_geometry ( g ) if vis_camera is not None : view_control = vis . get_view_control () view_control . convert_from_pinhole_camera_parameters ( vis_camera ) if render_options_json is not None : render_option = vis . get_render_option () render_option . load_from_json ( render_options_json ) if vis_file_path is not None : vis . poll_events () vis . update_renderer () vis . capture_screen_image ( vis_file_path , do_render = True ) else : vis . run ()","title":"visualize_estimation()"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.main","text":"main () -> None Entry point of the evaluation program. Source code in cpas_toolbox/evaluate.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def main () -> None : \"\"\"Entry point of the evaluation program.\"\"\" parser = argparse . ArgumentParser ( description = \"Pose and shape estimation evaluation on REAL275 data\" ) parser . add_argument ( \"--config\" , required = True ) parser . add_argument ( \"--out_dir\" , required = True ) resolved_args = _resolve_config_args ( sys . argv [ 1 :]) config = yoco . load_config_from_args ( parser , resolved_args ) evaluator = Evaluator ( config ) evaluator . run ()","title":"main()"},{"location":"api_reference/metrics/","text":"cpas_toolbox.metrics Metrics for shape evaluation. correct_thresh correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int Classify a pose prediction as correct or incorrect. PARAMETER DESCRIPTION position_gt Ground truth position, shape (3,). TYPE: np . ndarray position_prediction Predicted position, shape (3,). TYPE: np . ndarray position_threshold Position threshold in meters, no threshold if None. TYPE: Optional [ float ] DEFAULT: None orientation_gt Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation orientation_prediction Predicted orientation. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation extent_gt Bounding box extents, shape (3,). Only used if IoU threshold specified. TYPE: Optional [ np . ndarray ] DEFAULT: None extent_prediction Bounding box extents, shape (3,). Only used if IoU threshold specified. TYPE: Optional [ np . ndarray ] DEFAULT: None point_gt Set of true points, shape (N,3). points_rec Set of reconstructed points, shape (M,3). degree_threshold Orientation threshold in degrees, no threshold if None. TYPE: Optional [ float ] DEFAULT: None iou_3d_threshold 3D IoU threshold, no threshold if None. TYPE: Optional [ float ] DEFAULT: None rotational_symmetry_axis Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION int 1 if error is below all provided thresholds. 0 if error is above one provided int threshold. Source code in cpas_toolbox/metrics.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int : \"\"\"Classify a pose prediction as correct or incorrect. Args: position_gt: Ground truth position, shape (3,). position_prediction: Predicted position, shape (3,). position_threshold: Position threshold in meters, no threshold if None. orientation_gt: Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame. orientation_prediction: Predicted orientation. This is the rotation that rotates points from bounding box to camera frame. extent_gt: Bounding box extents, shape (3,). Only used if IoU threshold specified. extent_prediction: Bounding box extents, shape (3,). Only used if IoU threshold specified. point_gt: Set of true points, shape (N,3). points_rec: Set of reconstructed points, shape (M,3). degree_threshold: Orientation threshold in degrees, no threshold if None. iou_3d_threshold: 3D IoU threshold, no threshold if None. rotational_symmetry_axis: Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. Returns: 1 if error is below all provided thresholds. 0 if error is above one provided threshold. \"\"\" if position_threshold is not None : position_error = np . linalg . norm ( position_gt - position_prediction ) if position_error > position_threshold : return 0 if degree_threshold is not None : rad_threshold = degree_threshold * np . pi / 180.0 if rotational_symmetry_axis is not None : p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p1 = orientation_gt . apply ( p ) p2 = orientation_prediction . apply ( p ) rad_error = np . arccos ( p1 @ p2 ) else : rad_error = ( orientation_gt * orientation_prediction . inv ()) . magnitude () if rad_error > rad_threshold : return 0 if iou_3d_threshold is not None : if rotational_symmetry_axis is not None : max_iou = 0 for r in np . linspace ( 0 , np . pi , 100 ): p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p *= r sym_rot = Rotation . from_rotvec ( r ) iou = iou_3d ( position_gt , orientation_gt , extent_gt , position_prediction , orientation_prediction * sym_rot , extent_prediction , ) max_iou = max ( iou , max_iou ) iou = max_iou else : iou = iou_3d ( position_gt , orientation_gt , extent_gt , position_prediction , orientation_prediction , extent_prediction , ) if iou < iou_3d_threshold : return 0 if fscore_threshold is not None : fscore = reconstruction_fscore ( points_gt , points_prediction , 0.01 ) if fscore < fscore_threshold : return 0 return 1 mean_accuracy mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) float ground truth points. Source code in cpas_toolbox/metrics.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) ground truth points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d ) mean_completeness mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from ground truth points to closest (in p-norm) float reconstructed points. Source code in cpas_toolbox/metrics.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from ground truth points to closest (in p-norm) reconstructed points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d ) symmetric_chamfer symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of accuracy and completeness metrics using the specified p-norm. Source code in cpas_toolbox/metrics.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of accuracy and completeness metrics using the specified p-norm. \"\"\" return ( mean_completeness ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) + mean_accuracy ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) ) / 2 completeness_thresh completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of ground truth points with closest reconstructed point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of ground truth points with closest reconstructed point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_gt . shape [ 0 ] else : return np . sum ( d < threshold ) / points_gt . shape [ 0 ] accuracy_thresh accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_rec . shape [ 0 ] else : return np . sum ( d < threshold ) / points_rec . shape [ 0 ] reconstruction_fscore reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Harmonic mean of precision (thresholded accuracy) and recall (thresholded float completeness). Source code in cpas_toolbox/metrics.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Harmonic mean of precision (thresholded accuracy) and recall (thresholded completeness). \"\"\" recall = completeness_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) precision = accuracy_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) if recall < 1e-7 or precision < 1e-7 : return 0 return 2 / ( 1 / recall + 1 / precision ) extent extent ( points : np . ndarray ) -> float Compute largest Euclidean distance between any two points. PARAMETER DESCRIPTION points_gt set of true p_norm which Minkowski p-norm is used for distance and nearest neighbor query RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def extent ( points : np . ndarray ) -> float : \"\"\"Compute largest Euclidean distance between any two points. Args: points_gt: set of true p_norm: which Minkowski p-norm is used for distance and nearest neighbor query Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" try : hull = scipy . spatial . ConvexHull ( points ) except scipy . spatial . qhull . QhullError : # fallback to brute force distance matrix return np . max ( scipy . spatial . distance_matrix ( points , points )) # this is wasteful, if too slow implement rotating caliper method return np . max ( scipy . spatial . distance_matrix ( points [ hull . vertices ], points [ hull . vertices ]) ) iou_3d_sampling iou_3d_sampling ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , num_points : int = 10000 , ) -> float Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box. PARAMETER DESCRIPTION p1 Center position of first bounding box, shape (3,). TYPE: np . ndarray r1 Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e1 Extents (i.e., side lengths) of first bounding box, shape (3,). TYPE: np . ndarray p2 Center position of second bounding box, shape (3,). TYPE: np . ndarray r2 Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e2 Extents (i.e., side lengths) of second bounding box, shape (3,). TYPE: np . ndarray num_points Number of points to sample in smaller bounding box. TYPE: int DEFAULT: 10000 RETURNS DESCRIPTION float Approximate intersection-over-union for the two oriented bounding boxes. Source code in cpas_toolbox/metrics.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def iou_3d_sampling ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , num_points : int = 10000 , ) -> float : \"\"\"Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box. Args: p1: Center position of first bounding box, shape (3,). r1: Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. e1: Extents (i.e., side lengths) of first bounding box, shape (3,). p2: Center position of second bounding box, shape (3,). r2: Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. e2: Extents (i.e., side lengths) of second bounding box, shape (3,). num_points: Number of points to sample in smaller bounding box. Returns: Approximate intersection-over-union for the two oriented bounding boxes. \"\"\" # sample smaller volume to estimate intersection vol_1 = np . prod ( e1 ) vol_2 = np . prod ( e2 ) if vol_1 < vol_2 : points_1_in_1 = e1 * np . random . rand ( num_points , 3 ) - e1 / 2 points_1_in_w = r1 . apply ( points_1_in_1 ) + p1 points_1_in_2 = r2 . inv () . apply ( points_1_in_w - p2 ) ratio_1_in_2 = ( np . sum ( np . all ( points_1_in_2 < e2 / 2 , axis = 1 ) * np . all ( - e2 / 2 < points_1_in_2 , axis = 1 ) ) / num_points ) intersection = ratio_1_in_2 * vol_1 else : points_2_in_2 = e2 * np . random . rand ( num_points , 3 ) - e2 / 2 points_2_in_w = r2 . apply ( points_2_in_2 ) + p2 points_2_in_1 = r1 . inv () . apply ( points_2_in_w - p1 ) ratio_2_in_1 = ( np . sum ( np . all ( points_2_in_1 < e1 / 2 , axis = 1 ) * np . all ( - e1 / 2 < points_2_in_1 , axis = 1 ) ) / num_points ) intersection = ratio_2_in_1 * vol_2 union = vol_1 + vol_2 - intersection return intersection / union iou_3d iou_3d ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , ) -> float Compute 3D IoU of oriented bounding boxes analytically. Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm. PARAMETER DESCRIPTION p1 Center position of first bounding box, shape (3,). TYPE: np . ndarray r1 Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e1 Extents (i.e., side lengths) of first bounding box, shape (3,). TYPE: np . ndarray p2 Center position of second bounding box, shape (3,). TYPE: np . ndarray r2 Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e2 Extents (i.e., side lengths) of second bounding box, shape (3,). TYPE: np . ndarray RETURNS DESCRIPTION float Accurate intersection-over-union for the two oriented bounding boxes. Source code in cpas_toolbox/metrics.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def iou_3d ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , ) -> float : \"\"\"Compute 3D IoU of oriented bounding boxes analytically. Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm. Args: p1: Center position of first bounding box, shape (3,). r1: Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. e1: Extents (i.e., side lengths) of first bounding box, shape (3,). p2: Center position of second bounding box, shape (3,). r2: Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. e2: Extents (i.e., side lengths) of second bounding box, shape (3,). Returns: Accurate intersection-over-union for the two oriented bounding boxes. \"\"\" # create halfspaces halfspaces = np . zeros (( 12 , 4 )) halfspaces [ 0 : 3 , 0 : 3 ] = r1 . as_matrix () . T halfspaces [ 0 : 3 , 3 ] = - halfspaces [ 0 : 3 , 0 : 3 ] @ ( r1 . apply ( e1 / 2 ) + p1 ) halfspaces [ 3 : 6 , 0 : 3 ] = - halfspaces [ 0 : 3 , 0 : 3 ] halfspaces [ 3 : 6 , 3 ] = - halfspaces [ 3 : 6 , 0 : 3 ] @ ( r1 . apply ( - e1 / 2 ) + p1 ) halfspaces [ 6 : 9 , 0 : 3 ] = r2 . as_matrix () . T halfspaces [ 6 : 9 , 3 ] = - halfspaces [ 6 : 9 , 0 : 3 ] @ ( r2 . apply ( e2 / 2 ) + p2 ) halfspaces [ 9 : 12 , 0 : 3 ] = - halfspaces [ 6 : 9 , 0 : 3 ] halfspaces [ 9 : 12 , 3 ] = - halfspaces [ 9 : 12 , 0 : 3 ] @ ( r2 . apply ( - e2 / 2 ) + p2 ) # try to find point inside both bounding boxes inside_point = _find_inside_point ( p1 , r1 , e1 , p2 , r2 , e2 , halfspaces ) if inside_point is None : return 0 # create halfspace intersection and compute IoU hs = scipy . spatial . HalfspaceIntersection ( halfspaces , inside_point ) ch = scipy . spatial . ConvexHull ( hs . intersections ) intersection = ch . volume vol_1 = np . prod ( e1 ) vol_2 = np . prod ( e2 ) union = vol_1 + vol_2 - intersection return intersection / union","title":"metrics.py"},{"location":"api_reference/metrics/#cpas_toolbox.metrics","text":"Metrics for shape evaluation.","title":"metrics"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.correct_thresh","text":"correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int Classify a pose prediction as correct or incorrect. PARAMETER DESCRIPTION position_gt Ground truth position, shape (3,). TYPE: np . ndarray position_prediction Predicted position, shape (3,). TYPE: np . ndarray position_threshold Position threshold in meters, no threshold if None. TYPE: Optional [ float ] DEFAULT: None orientation_gt Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation orientation_prediction Predicted orientation. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation extent_gt Bounding box extents, shape (3,). Only used if IoU threshold specified. TYPE: Optional [ np . ndarray ] DEFAULT: None extent_prediction Bounding box extents, shape (3,). Only used if IoU threshold specified. TYPE: Optional [ np . ndarray ] DEFAULT: None point_gt Set of true points, shape (N,3). points_rec Set of reconstructed points, shape (M,3). degree_threshold Orientation threshold in degrees, no threshold if None. TYPE: Optional [ float ] DEFAULT: None iou_3d_threshold 3D IoU threshold, no threshold if None. TYPE: Optional [ float ] DEFAULT: None rotational_symmetry_axis Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION int 1 if error is below all provided thresholds. 0 if error is above one provided int threshold. Source code in cpas_toolbox/metrics.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def correct_thresh ( position_gt : np . ndarray , position_prediction : np . ndarray , orientation_gt : Rotation , orientation_prediction : Rotation , extent_gt : Optional [ np . ndarray ] = None , extent_prediction : Optional [ np . ndarray ] = None , points_gt : Optional [ np . ndarray ] = None , points_prediction : Optional [ np . ndarray ] = None , position_threshold : Optional [ float ] = None , degree_threshold : Optional [ float ] = None , iou_3d_threshold : Optional [ float ] = None , fscore_threshold : Optional [ float ] = None , rotational_symmetry_axis : Optional [ int ] = None , ) -> int : \"\"\"Classify a pose prediction as correct or incorrect. Args: position_gt: Ground truth position, shape (3,). position_prediction: Predicted position, shape (3,). position_threshold: Position threshold in meters, no threshold if None. orientation_gt: Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame. orientation_prediction: Predicted orientation. This is the rotation that rotates points from bounding box to camera frame. extent_gt: Bounding box extents, shape (3,). Only used if IoU threshold specified. extent_prediction: Bounding box extents, shape (3,). Only used if IoU threshold specified. point_gt: Set of true points, shape (N,3). points_rec: Set of reconstructed points, shape (M,3). degree_threshold: Orientation threshold in degrees, no threshold if None. iou_3d_threshold: 3D IoU threshold, no threshold if None. rotational_symmetry_axis: Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis. Returns: 1 if error is below all provided thresholds. 0 if error is above one provided threshold. \"\"\" if position_threshold is not None : position_error = np . linalg . norm ( position_gt - position_prediction ) if position_error > position_threshold : return 0 if degree_threshold is not None : rad_threshold = degree_threshold * np . pi / 180.0 if rotational_symmetry_axis is not None : p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p1 = orientation_gt . apply ( p ) p2 = orientation_prediction . apply ( p ) rad_error = np . arccos ( p1 @ p2 ) else : rad_error = ( orientation_gt * orientation_prediction . inv ()) . magnitude () if rad_error > rad_threshold : return 0 if iou_3d_threshold is not None : if rotational_symmetry_axis is not None : max_iou = 0 for r in np . linspace ( 0 , np . pi , 100 ): p = np . array ([ 0.0 , 0.0 , 0.0 ]) p [ rotational_symmetry_axis ] = 1.0 p *= r sym_rot = Rotation . from_rotvec ( r ) iou = iou_3d ( position_gt , orientation_gt , extent_gt , position_prediction , orientation_prediction * sym_rot , extent_prediction , ) max_iou = max ( iou , max_iou ) iou = max_iou else : iou = iou_3d ( position_gt , orientation_gt , extent_gt , position_prediction , orientation_prediction , extent_prediction , ) if iou < iou_3d_threshold : return 0 if fscore_threshold is not None : fscore = reconstruction_fscore ( points_gt , points_prediction , 0.01 ) if fscore < fscore_threshold : return 0 return 1","title":"correct_thresh()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_accuracy","text":"mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) float ground truth points. Source code in cpas_toolbox/metrics.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def mean_accuracy ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute accuracy metric. Accuracy metric is the same as asymmetric chamfer distance from rec to gt. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from reconstructed points to closest (in p-norm) ground truth points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d )","title":"mean_accuracy()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_completeness","text":"mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of p-norm from ground truth points to closest (in p-norm) float reconstructed points. Source code in cpas_toolbox/metrics.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def mean_completeness ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute completeness metric. Completeness metric is the same as asymmetric chamfer distance from gt to rec. See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of p-norm from ground truth points to closest (in p-norm) reconstructed points. \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . mean ( d ) / extent ( points_gt ) else : return np . mean ( d )","title":"mean_completeness()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.symmetric_chamfer","text":"symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide result by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Arithmetic mean of accuracy and completeness metrics using the specified p-norm. Source code in cpas_toolbox/metrics.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def symmetric_chamfer ( points_gt : np . ndarray , points_rec : np . ndarray , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute symmetric chamfer distance. There are various slightly different definitions for the chamfer distance. Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two. Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide result by Euclidean extent of points_gt Returns: Arithmetic mean of accuracy and completeness metrics using the specified p-norm. \"\"\" return ( mean_completeness ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) + mean_accuracy ( points_gt , points_rec , p_norm = p_norm , normalize = normalize ) ) / 2","title":"symmetric_chamfer()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.completeness_thresh","text":"completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of ground truth points with closest reconstructed point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def completeness_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded completion metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of ground truth points with closest reconstructed point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_rec ) d , _ = kd_tree . query ( points_gt , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_gt . shape [ 0 ] else : return np . sum ( d < threshold ) / points_gt . shape [ 0 ]","title":"completeness_thresh()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.accuracy_thresh","text":"accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def accuracy_thresh ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute thresholded accuracy metric. See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020. Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" kd_tree = scipy . spatial . KDTree ( points_gt ) d , _ = kd_tree . query ( points_rec , p = p_norm ) if normalize : return np . sum ( d / extent ( points_gt ) < threshold ) / points_rec . shape [ 0 ] else : return np . sum ( d < threshold ) / points_rec . shape [ 0 ]","title":"accuracy_thresh()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.reconstruction_fscore","text":"reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 PARAMETER DESCRIPTION points_gt set of true points, expected shape (N,3) TYPE: np . ndarray points_rec set of reconstructed points, expected shape (M,3) TYPE: np . ndarray threshold distance threshold to count a point as correct TYPE: float p_norm which Minkowski p-norm is used for distance and nearest neighbor query TYPE: int DEFAULT: 2 normalize whether to divide distances by Euclidean extent of points_gt TYPE: bool DEFAULT: False RETURNS DESCRIPTION float Harmonic mean of precision (thresholded accuracy) and recall (thresholded float completeness). Source code in cpas_toolbox/metrics.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def reconstruction_fscore ( points_gt : np . ndarray , points_rec : np . ndarray , threshold : float , p_norm : int = 2 , normalize : bool = False , ) -> float : \"\"\"Compute reconstruction fscore. See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019 Args: points_gt: set of true points, expected shape (N,3) points_rec: set of reconstructed points, expected shape (M,3) threshold: distance threshold to count a point as correct p_norm: which Minkowski p-norm is used for distance and nearest neighbor query normalize: whether to divide distances by Euclidean extent of points_gt Returns: Harmonic mean of precision (thresholded accuracy) and recall (thresholded completeness). \"\"\" recall = completeness_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) precision = accuracy_thresh ( points_gt , points_rec , threshold , p_norm = p_norm , normalize = normalize ) if recall < 1e-7 or precision < 1e-7 : return 0 return 2 / ( 1 / recall + 1 / precision )","title":"reconstruction_fscore()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.extent","text":"extent ( points : np . ndarray ) -> float Compute largest Euclidean distance between any two points. PARAMETER DESCRIPTION points_gt set of true p_norm which Minkowski p-norm is used for distance and nearest neighbor query RETURNS DESCRIPTION float Ratio of reconstructed points with closest ground truth point closer than float threshold (in p-norm). Source code in cpas_toolbox/metrics.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def extent ( points : np . ndarray ) -> float : \"\"\"Compute largest Euclidean distance between any two points. Args: points_gt: set of true p_norm: which Minkowski p-norm is used for distance and nearest neighbor query Returns: Ratio of reconstructed points with closest ground truth point closer than threshold (in p-norm). \"\"\" try : hull = scipy . spatial . ConvexHull ( points ) except scipy . spatial . qhull . QhullError : # fallback to brute force distance matrix return np . max ( scipy . spatial . distance_matrix ( points , points )) # this is wasteful, if too slow implement rotating caliper method return np . max ( scipy . spatial . distance_matrix ( points [ hull . vertices ], points [ hull . vertices ]) )","title":"extent()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d_sampling","text":"iou_3d_sampling ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , num_points : int = 10000 , ) -> float Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box. PARAMETER DESCRIPTION p1 Center position of first bounding box, shape (3,). TYPE: np . ndarray r1 Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e1 Extents (i.e., side lengths) of first bounding box, shape (3,). TYPE: np . ndarray p2 Center position of second bounding box, shape (3,). TYPE: np . ndarray r2 Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e2 Extents (i.e., side lengths) of second bounding box, shape (3,). TYPE: np . ndarray num_points Number of points to sample in smaller bounding box. TYPE: int DEFAULT: 10000 RETURNS DESCRIPTION float Approximate intersection-over-union for the two oriented bounding boxes. Source code in cpas_toolbox/metrics.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def iou_3d_sampling ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , num_points : int = 10000 , ) -> float : \"\"\"Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box. Args: p1: Center position of first bounding box, shape (3,). r1: Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. e1: Extents (i.e., side lengths) of first bounding box, shape (3,). p2: Center position of second bounding box, shape (3,). r2: Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. e2: Extents (i.e., side lengths) of second bounding box, shape (3,). num_points: Number of points to sample in smaller bounding box. Returns: Approximate intersection-over-union for the two oriented bounding boxes. \"\"\" # sample smaller volume to estimate intersection vol_1 = np . prod ( e1 ) vol_2 = np . prod ( e2 ) if vol_1 < vol_2 : points_1_in_1 = e1 * np . random . rand ( num_points , 3 ) - e1 / 2 points_1_in_w = r1 . apply ( points_1_in_1 ) + p1 points_1_in_2 = r2 . inv () . apply ( points_1_in_w - p2 ) ratio_1_in_2 = ( np . sum ( np . all ( points_1_in_2 < e2 / 2 , axis = 1 ) * np . all ( - e2 / 2 < points_1_in_2 , axis = 1 ) ) / num_points ) intersection = ratio_1_in_2 * vol_1 else : points_2_in_2 = e2 * np . random . rand ( num_points , 3 ) - e2 / 2 points_2_in_w = r2 . apply ( points_2_in_2 ) + p2 points_2_in_1 = r1 . inv () . apply ( points_2_in_w - p1 ) ratio_2_in_1 = ( np . sum ( np . all ( points_2_in_1 < e1 / 2 , axis = 1 ) * np . all ( - e1 / 2 < points_2_in_1 , axis = 1 ) ) / num_points ) intersection = ratio_2_in_1 * vol_2 union = vol_1 + vol_2 - intersection return intersection / union","title":"iou_3d_sampling()"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d","text":"iou_3d ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , ) -> float Compute 3D IoU of oriented bounding boxes analytically. Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm. PARAMETER DESCRIPTION p1 Center position of first bounding box, shape (3,). TYPE: np . ndarray r1 Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e1 Extents (i.e., side lengths) of first bounding box, shape (3,). TYPE: np . ndarray p2 Center position of second bounding box, shape (3,). TYPE: np . ndarray r2 Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. TYPE: Rotation e2 Extents (i.e., side lengths) of second bounding box, shape (3,). TYPE: np . ndarray RETURNS DESCRIPTION float Accurate intersection-over-union for the two oriented bounding boxes. Source code in cpas_toolbox/metrics.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def iou_3d ( p1 : np . ndarray , r1 : Rotation , e1 : np . ndarray , p2 : np . ndarray , r2 : Rotation , e2 : np . ndarray , ) -> float : \"\"\"Compute 3D IoU of oriented bounding boxes analytically. Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm. Args: p1: Center position of first bounding box, shape (3,). r1: Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame. e1: Extents (i.e., side lengths) of first bounding box, shape (3,). p2: Center position of second bounding box, shape (3,). r2: Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame. e2: Extents (i.e., side lengths) of second bounding box, shape (3,). Returns: Accurate intersection-over-union for the two oriented bounding boxes. \"\"\" # create halfspaces halfspaces = np . zeros (( 12 , 4 )) halfspaces [ 0 : 3 , 0 : 3 ] = r1 . as_matrix () . T halfspaces [ 0 : 3 , 3 ] = - halfspaces [ 0 : 3 , 0 : 3 ] @ ( r1 . apply ( e1 / 2 ) + p1 ) halfspaces [ 3 : 6 , 0 : 3 ] = - halfspaces [ 0 : 3 , 0 : 3 ] halfspaces [ 3 : 6 , 3 ] = - halfspaces [ 3 : 6 , 0 : 3 ] @ ( r1 . apply ( - e1 / 2 ) + p1 ) halfspaces [ 6 : 9 , 0 : 3 ] = r2 . as_matrix () . T halfspaces [ 6 : 9 , 3 ] = - halfspaces [ 6 : 9 , 0 : 3 ] @ ( r2 . apply ( e2 / 2 ) + p2 ) halfspaces [ 9 : 12 , 0 : 3 ] = - halfspaces [ 6 : 9 , 0 : 3 ] halfspaces [ 9 : 12 , 3 ] = - halfspaces [ 9 : 12 , 0 : 3 ] @ ( r2 . apply ( - e2 / 2 ) + p2 ) # try to find point inside both bounding boxes inside_point = _find_inside_point ( p1 , r1 , e1 , p2 , r2 , e2 , halfspaces ) if inside_point is None : return 0 # create halfspace intersection and compute IoU hs = scipy . spatial . HalfspaceIntersection ( halfspaces , inside_point ) ch = scipy . spatial . ConvexHull ( hs . intersections ) intersection = ch . volume vol_1 = np . prod ( e1 ) vol_2 = np . prod ( e2 ) union = vol_1 + vol_2 - intersection return intersection / union","title":"iou_3d()"},{"location":"api_reference/pointset_utils/","text":"cpas_toolbox.pointset_utils Utility functions to handle pointsets. normalize_points normalize_points ( points : torch . Tensor ) -> torch . Tensor Normalize pointset to have zero mean. Normalization will be performed along second last dimension. PARAMETER DESCRIPTION points The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. TYPE: torch . Tensor Return normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. Source code in cpas_toolbox/pointset_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def normalize_points ( points : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize pointset to have zero mean. Normalization will be performed along second last dimension. Args: points: The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. Return: normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. \"\"\" centroids = torch . mean ( points , dim =- 2 , keepdim = True ) normalized_points = points - centroids return normalized_points , centroids . squeeze () depth_to_pointcloud depth_to_pointcloud ( depth_image : torch . Tensor , camera : camera_utils . Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor Convert depth image to pointcloud. PARAMETER DESCRIPTION depth_image The depth image to convert to pointcloud, shape (H,W). TYPE: torch . Tensor camera The camera used to lift the points. TYPE: camera_utils . Camera normalize Whether to normalize the pointcloud with 0 centroid. TYPE: bool DEFAULT: False mask Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. TYPE: Optional [ torch . Tensor ] DEFAULT: None convention The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward TYPE: str DEFAULT: 'opengl' RETURNS DESCRIPTION torch . Tensor The pointcloud in the camera frame, in OpenGL convention, shape (N,3). Source code in cpas_toolbox/pointset_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def depth_to_pointcloud ( depth_image : torch . Tensor , camera : camera_utils . Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor : \"\"\"Convert depth image to pointcloud. Args: depth_image: The depth image to convert to pointcloud, shape (H,W). camera: The camera used to lift the points. normalize: Whether to normalize the pointcloud with 0 centroid. mask: Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. convention: The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Returns: The pointcloud in the camera frame, in OpenGL convention, shape (N,3). \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.0 ) if mask is None : indices = torch . nonzero ( depth_image , as_tuple = True ) else : indices = torch . nonzero ( depth_image * mask , as_tuple = True ) depth_values = depth_image [ indices ] points = torch . cat ( ( indices [ 1 ][:, None ] . float (), indices [ 0 ][:, None ] . float (), depth_values [:, None ], ), dim = 1 , ) if convention == \"opengl\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = - ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = - points [:, 2 ] elif convention == \"opencv\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = points [:, 2 ] else : raise ValueError ( f \"Unsupported camera convention { convention } .\" ) if normalize : final_points , _ = normalize_points ( final_points ) return final_points change_transform_camera_convention change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor Change the camera convention for a frame A -> camera frame transform. PARAMETER DESCRIPTION in_transform Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION torch . Tensor Transformtion matrix(es) from coordinate frame A to out_convention camera frame. torch . Tensor Same shape as in_transform. Source code in cpas_toolbox/pointset_utils.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor : \"\"\"Change the camera convention for a frame A -> camera frame transform. Args: in_transform: Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Transformtion matrix(es) from coordinate frame A to out_convention camera frame. Same shape as in_transform. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_transform else : gl2cv_transform = torch . diag ( in_transform . new_tensor ([ 1.0 , - 1.0 , - 1.0 , 1.0 ]) ) # == cv2gl_transform return gl2cv_transform @ in_transform change_position_camera_convention change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for a position in a camera frame. PARAMETER DESCRIPTION in_position Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). TYPE: torch . Tensor in_convention Camera convention for the in_position. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). Source code in cpas_toolbox/pointset_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for a position in a camera frame. Args: in_position: Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). in_convention: Camera convention for the in_position. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_position else : gl2cv = in_position . new_tensor ([ 1.0 , - 1.0 , - 1.0 ]) # == cv2gl return gl2cv * in_position change_orientation_camera_convention change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). PARAMETER DESCRIPTION in_orientation_q Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Quaternion(s) which transforms from coordinate frame A to in_convention camera tuple frame. Scalar-last convention. Same shape as in_orientation_q. Source code in cpas_toolbox/pointset_utils.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). Args: in_orientation_q: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Same shape as in_orientation_q. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_orientation_q else : # rotate 180deg around x direction gl2cv_q = in_orientation_q . new_tensor ([ 1.0 , 0 , 0 , 0 ]) # == cv2gl return quaternion_utils . quaternion_multiply ( gl2cv_q , in_orientation_q ) visualize_pointset visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None Visualize pointset as 3D scatter plot. PARAMETER DESCRIPTION pointset The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. TYPE: torch . Tensor max_points Maximum number of points. If N>max_points only a random subset will be shown. TYPE: int DEFAULT: 1000 Source code in cpas_toolbox/pointset_utils.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None : \"\"\"Visualize pointset as 3D scatter plot. Args: pointset: The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. max_points: Maximum number of points. If N>max_points only a random subset will be shown. \"\"\" pointset_np = pointset . cpu () . detach () . numpy () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) if len ( pointset_np ) > max_points : indices = np . random . choice ( len ( pointset_np ), replace = False , size = max_points ) pointset_np = pointset_np [ indices ] if pointset_np . shape [ 1 ] == 6 : colors = pointset_np [:, 3 :] else : colors = None ax . scatter ( pointset_np [:, 0 ], pointset_np [:, 1 ], pointset_np [:, 2 ], c = colors ) ax . set_xlabel ( \"x\" ) ax . set_ylabel ( \"y\" ) ax . set_zlabel ( \"z\" ) ax . set_box_aspect ( pointset_np . max ( axis = 0 ) - pointset_np . min ( axis = 0 )) plt . show ()","title":"pointset_utils.py"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils","text":"Utility functions to handle pointsets.","title":"pointset_utils"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.normalize_points","text":"normalize_points ( points : torch . Tensor ) -> torch . Tensor Normalize pointset to have zero mean. Normalization will be performed along second last dimension. PARAMETER DESCRIPTION points The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. TYPE: torch . Tensor Return normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. Source code in cpas_toolbox/pointset_utils.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def normalize_points ( points : torch . Tensor ) -> torch . Tensor : \"\"\"Normalize pointset to have zero mean. Normalization will be performed along second last dimension. Args: points: The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D. Return: normalized_points: The normalized pointset, same shape as points. centroids: The means of the pointclouds used to normalize points. Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively. \"\"\" centroids = torch . mean ( points , dim =- 2 , keepdim = True ) normalized_points = points - centroids return normalized_points , centroids . squeeze ()","title":"normalize_points()"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.depth_to_pointcloud","text":"depth_to_pointcloud ( depth_image : torch . Tensor , camera : camera_utils . Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor Convert depth image to pointcloud. PARAMETER DESCRIPTION depth_image The depth image to convert to pointcloud, shape (H,W). TYPE: torch . Tensor camera The camera used to lift the points. TYPE: camera_utils . Camera normalize Whether to normalize the pointcloud with 0 centroid. TYPE: bool DEFAULT: False mask Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. TYPE: Optional [ torch . Tensor ] DEFAULT: None convention The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward TYPE: str DEFAULT: 'opengl' RETURNS DESCRIPTION torch . Tensor The pointcloud in the camera frame, in OpenGL convention, shape (N,3). Source code in cpas_toolbox/pointset_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def depth_to_pointcloud ( depth_image : torch . Tensor , camera : camera_utils . Camera , normalize : bool = False , mask : Optional [ torch . Tensor ] = None , convention : str = \"opengl\" , ) -> torch . Tensor : \"\"\"Convert depth image to pointcloud. Args: depth_image: The depth image to convert to pointcloud, shape (H,W). camera: The camera used to lift the points. normalize: Whether to normalize the pointcloud with 0 centroid. mask: Only points with mask != 0 will be added to pointcloud. No masking will be performed if None. convention: The camera frame convention to use. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Returns: The pointcloud in the camera frame, in OpenGL convention, shape (N,3). \"\"\" fx , fy , cx , cy , _ = camera . get_pinhole_camera_parameters ( 0.0 ) if mask is None : indices = torch . nonzero ( depth_image , as_tuple = True ) else : indices = torch . nonzero ( depth_image * mask , as_tuple = True ) depth_values = depth_image [ indices ] points = torch . cat ( ( indices [ 1 ][:, None ] . float (), indices [ 0 ][:, None ] . float (), depth_values [:, None ], ), dim = 1 , ) if convention == \"opengl\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = - ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = - points [:, 2 ] elif convention == \"opencv\" : final_points = torch . empty_like ( points ) final_points [:, 0 ] = ( points [:, 0 ] - cx ) * points [:, 2 ] / fx final_points [:, 1 ] = ( points [:, 1 ] - cy ) * points [:, 2 ] / fy final_points [:, 2 ] = points [:, 2 ] else : raise ValueError ( f \"Unsupported camera convention { convention } .\" ) if normalize : final_points , _ = normalize_points ( final_points ) return final_points","title":"depth_to_pointcloud()"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_transform_camera_convention","text":"change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor Change the camera convention for a frame A -> camera frame transform. PARAMETER DESCRIPTION in_transform Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION torch . Tensor Transformtion matrix(es) from coordinate frame A to out_convention camera frame. torch . Tensor Same shape as in_transform. Source code in cpas_toolbox/pointset_utils.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def change_transform_camera_convention ( in_transform : torch . Tensor , in_convention : str , out_convention : str ) -> torch . Tensor : \"\"\"Change the camera convention for a frame A -> camera frame transform. Args: in_transform: Transformtion matrix(es) from coordinate frame A to in_convention camera frame. Shape (...,4,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Transformtion matrix(es) from coordinate frame A to out_convention camera frame. Same shape as in_transform. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_transform else : gl2cv_transform = torch . diag ( in_transform . new_tensor ([ 1.0 , - 1.0 , - 1.0 , 1.0 ]) ) # == cv2gl_transform return gl2cv_transform @ in_transform","title":"change_transform_camera_convention()"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_position_camera_convention","text":"change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for a position in a camera frame. PARAMETER DESCRIPTION in_position Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). TYPE: torch . Tensor in_convention Camera convention for the in_position. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). Source code in cpas_toolbox/pointset_utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def change_position_camera_convention ( in_position : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for a position in a camera frame. Args: in_position: Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3). in_convention: Camera convention for the in_position. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3). \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_position else : gl2cv = in_position . new_tensor ([ 1.0 , - 1.0 , - 1.0 ]) # == cv2gl return gl2cv * in_position","title":"change_position_camera_convention()"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_orientation_camera_convention","text":"change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str ) -> tuple Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). PARAMETER DESCRIPTION in_orientation_q Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). TYPE: torch . Tensor in_convention Camera convention for the in_transform. One of \"opengl\", \"opencv\". TYPE: str out_convention Camera convention for the returned transform. One of \"opengl\", \"opencv\". TYPE: str RETURNS DESCRIPTION tuple Quaternion(s) which transforms from coordinate frame A to in_convention camera tuple frame. Scalar-last convention. Same shape as in_orientation_q. Source code in cpas_toolbox/pointset_utils.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def change_orientation_camera_convention ( in_orientation_q : torch . Tensor , in_convention : str , out_convention : str , ) -> tuple : \"\"\"Change the camera convention for an orientation in a camera frame. Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin). Args: in_orientation_q: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4). in_convention: Camera convention for the in_transform. One of \"opengl\", \"opencv\". out_convention: Camera convention for the returned transform. One of \"opengl\", \"opencv\". Returns: Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Same shape as in_orientation_q. \"\"\" # check whether valild convention was provided if in_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"In camera convention { in_convention } not supported.\" ) if out_convention not in [ \"opengl\" , \"opencv\" ]: raise ValueError ( f \"Out camera convention { in_convention } not supported.\" ) if in_convention == out_convention : return in_orientation_q else : # rotate 180deg around x direction gl2cv_q = in_orientation_q . new_tensor ([ 1.0 , 0 , 0 , 0 ]) # == cv2gl return quaternion_utils . quaternion_multiply ( gl2cv_q , in_orientation_q )","title":"change_orientation_camera_convention()"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.visualize_pointset","text":"visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None Visualize pointset as 3D scatter plot. PARAMETER DESCRIPTION pointset The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. TYPE: torch . Tensor max_points Maximum number of points. If N>max_points only a random subset will be shown. TYPE: int DEFAULT: 1000 Source code in cpas_toolbox/pointset_utils.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def visualize_pointset ( pointset : torch . Tensor , max_points : int = 1000 ) -> None : \"\"\"Visualize pointset as 3D scatter plot. Args: pointset: The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb. max_points: Maximum number of points. If N>max_points only a random subset will be shown. \"\"\" pointset_np = pointset . cpu () . detach () . numpy () fig = plt . figure () ax = fig . add_subplot ( projection = \"3d\" ) if len ( pointset_np ) > max_points : indices = np . random . choice ( len ( pointset_np ), replace = False , size = max_points ) pointset_np = pointset_np [ indices ] if pointset_np . shape [ 1 ] == 6 : colors = pointset_np [:, 3 :] else : colors = None ax . scatter ( pointset_np [:, 0 ], pointset_np [:, 1 ], pointset_np [:, 2 ], c = colors ) ax . set_xlabel ( \"x\" ) ax . set_ylabel ( \"y\" ) ax . set_zlabel ( \"z\" ) ax . set_box_aspect ( pointset_np . max ( axis = 0 ) - pointset_np . min ( axis = 0 )) plt . show ()","title":"visualize_pointset()"},{"location":"api_reference/quaternion_utils/","text":"cpas_toolbox.quaternion_utils Functions to handle transformations with quaternions. Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar > 0. https://github.com/facebookresearch/pytorch3d quaternion_multiply quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor Multiply two quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions_1 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor quaternions_2 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Composition of passed quaternions. Source code in cpas_toolbox/quaternion_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor : \"\"\"Multiply two quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions_1: normalized quaternions of shape (..., 4), scalar-last convention quaternions_2: normalized quaternions of shape (..., 4), scalar-last convention Returns: Composition of passed quaternions. \"\"\" ax , ay , az , aw = torch . unbind ( quaternions_1 , - 1 ) bx , by , bz , bw = torch . unbind ( quaternions_2 , - 1 ) ox = aw * bx + ax * bw + ay * bz - az * by oy = aw * by - ax * bz + ay * bw + az * bx oz = aw * bz + ax * by - ay * bx + az * bw ow = aw * bw - ax * bx - ay * by - az * bz return torch . stack (( ox , oy , oz , ow ), - 1 ) quaternion_apply quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor Rotate points by quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor points points of shape (..., 3) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Points rotated by the rotations representing quaternions. Source code in cpas_toolbox/quaternion_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor : \"\"\"Rotate points by quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions: normalized quaternions of shape (..., 4), scalar-last convention points: points of shape (..., 3) Returns: Points rotated by the rotations representing quaternions. \"\"\" points_as_quaternions = points . new_zeros ( points . shape [: - 1 ] + ( 4 ,)) points_as_quaternions [ ... , : - 1 ] = points return quaternion_multiply ( quaternion_multiply ( quaternions , points_as_quaternions ), quaternion_invert ( quaternions ), )[ ... , : - 1 ] quaternion_invert quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor Invert quaternions representing orientations. PARAMETER DESCRIPTION quaternions The quaternions to invert, shape (..., 4), scalar-last convention. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Inverted quaternions, same shape as quaternions. Source code in cpas_toolbox/quaternion_utils.py 55 56 57 58 59 60 61 62 63 64 65 def quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor : \"\"\"Invert quaternions representing orientations. Args: quaternions: The quaternions to invert, shape (..., 4), scalar-last convention. Returns: Inverted quaternions, same shape as quaternions. \"\"\" return quaternions * quaternions . new_tensor ([ - 1 , - 1 , - 1 , 1 ]) geodesic_distance geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute geodesic distances between quaternions. PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in cpas_toolbox/quaternion_utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 def geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute geodesic distances between quaternions. Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" abs_q1q2 = torch . clip ( torch . abs ( torch . sum ( q1 * q2 , dim = 1 )), 0 , 1 ) geodesic_distances = 2 * torch . acos ( abs_q1q2 ) return geodesic_distances","title":"quaternion_utils.py"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils","text":"Functions to handle transformations with quaternions. Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar > 0. https://github.com/facebookresearch/pytorch3d","title":"quaternion_utils"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_multiply","text":"quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor Multiply two quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions_1 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor quaternions_2 normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Composition of passed quaternions. Source code in cpas_toolbox/quaternion_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def quaternion_multiply ( quaternions_1 : torch . Tensor , quaternions_2 : torch . Tensor ) -> torch . Tensor : \"\"\"Multiply two quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions_1: normalized quaternions of shape (..., 4), scalar-last convention quaternions_2: normalized quaternions of shape (..., 4), scalar-last convention Returns: Composition of passed quaternions. \"\"\" ax , ay , az , aw = torch . unbind ( quaternions_1 , - 1 ) bx , by , bz , bw = torch . unbind ( quaternions_2 , - 1 ) ox = aw * bx + ax * bw + ay * bz - az * by oy = aw * by - ax * bz + ay * bw + az * bx oz = aw * bz + ax * by - ay * bx + az * bw ow = aw * bw - ax * bx - ay * by - az * bz return torch . stack (( ox , oy , oz , ow ), - 1 )","title":"quaternion_multiply()"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_apply","text":"quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor Rotate points by quaternions representing rotations. Normal broadcasting rules apply. PARAMETER DESCRIPTION quaternions normalized quaternions of shape (..., 4), scalar-last convention TYPE: torch . Tensor points points of shape (..., 3) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Points rotated by the rotations representing quaternions. Source code in cpas_toolbox/quaternion_utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def quaternion_apply ( quaternions : torch . Tensor , points : torch . Tensor ) -> torch . Tensor : \"\"\"Rotate points by quaternions representing rotations. Normal broadcasting rules apply. Args: quaternions: normalized quaternions of shape (..., 4), scalar-last convention points: points of shape (..., 3) Returns: Points rotated by the rotations representing quaternions. \"\"\" points_as_quaternions = points . new_zeros ( points . shape [: - 1 ] + ( 4 ,)) points_as_quaternions [ ... , : - 1 ] = points return quaternion_multiply ( quaternion_multiply ( quaternions , points_as_quaternions ), quaternion_invert ( quaternions ), )[ ... , : - 1 ]","title":"quaternion_apply()"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_invert","text":"quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor Invert quaternions representing orientations. PARAMETER DESCRIPTION quaternions The quaternions to invert, shape (..., 4), scalar-last convention. TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Inverted quaternions, same shape as quaternions. Source code in cpas_toolbox/quaternion_utils.py 55 56 57 58 59 60 61 62 63 64 65 def quaternion_invert ( quaternions : torch . Tensor ) -> torch . Tensor : \"\"\"Invert quaternions representing orientations. Args: quaternions: The quaternions to invert, shape (..., 4), scalar-last convention. Returns: Inverted quaternions, same shape as quaternions. \"\"\" return quaternions * quaternions . new_tensor ([ - 1 , - 1 , - 1 , 1 ])","title":"quaternion_invert()"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.geodesic_distance","text":"geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor Compute geodesic distances between quaternions. PARAMETER DESCRIPTION q1 First set of quaterions, shape (N,4). TYPE: torch . Tensor q2 Second set of quaternions, shape (N,4). TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Mean distance between the quaternions, scalar. Source code in cpas_toolbox/quaternion_utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 def geodesic_distance ( q1 : torch . Tensor , q2 : torch . Tensor ) -> torch . Tensor : \"\"\"Compute geodesic distances between quaternions. Args: q1: First set of quaterions, shape (N,4). q2: Second set of quaternions, shape (N,4). Returns: Mean distance between the quaternions, scalar. \"\"\" abs_q1q2 = torch . clip ( torch . abs ( torch . sum ( q1 * q2 , dim = 1 )), 0 , 1 ) geodesic_distances = 2 * torch . acos ( abs_q1q2 ) return geodesic_distances","title":"geodesic_distance()"},{"location":"api_reference/utils/","text":"cpas_toolbox.utils This module provides miscellaneous utility functions. str_to_object str_to_object ( name : str ) -> Any Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. PARAMETER DESCRIPTION name Name of the object to resolve. TYPE: str RETURNS DESCRIPTION Any The object which the provided name refers to. None if no object was found. Source code in cpas_toolbox/utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def str_to_object ( name : str ) -> Any : \"\"\"Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. Args: name: Name of the object to resolve. Returns: The object which the provided name refers to. None if no object was found. \"\"\" # check callers local variables caller_locals = inspect . currentframe () . f_back . f_locals if name in caller_locals : return caller_locals [ name ] # check callers global variables (i.e., imported modules etc.) caller_globals = inspect . currentframe () . f_back . f_globals if name in caller_globals : return caller_globals [ name ] # check environment return locate ( name ) resolve_path resolve_path ( path : str , search_paths : Optional [ List [ str ]] = None ) -> str Resolves a path to a full absolute path based on search_paths. This function considers paths of 5 different cases /... -> absolute path, nothing todo ~/... -> home dir, expand user ./... -> relative to current directory ../... -> relative to current parent directory ... -> relative to search paths Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists. Returns original path, if file does not exist. PARAMETER DESCRIPTION path The path to resolve. TYPE: str search_paths List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed. TYPE: Optional [ List [ str ]] DEFAULT: None Source code in cpas_toolbox/utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def resolve_path ( path : str , search_paths : Optional [ List [ str ]] = None ) -> str : \"\"\"Resolves a path to a full absolute path based on search_paths. This function considers paths of 5 different cases /... -> absolute path, nothing todo ~/... -> home dir, expand user ./... -> relative to current directory ../... -> relative to current parent directory ... -> relative to search paths Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists. Returns original path, if file does not exist. Args: path: The path to resolve. search_paths: List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed. \"\"\" if search_paths is None : search_paths = [] if os . path . isabs ( path ): return path parts = path . split ( os . sep ) if parts [ 0 ] in [ \".\" , \"..\" ]: return os . path . abspath ( path ) elif parts [ 0 ] == \"~\" : return os . path . expanduser ( path ) for search_path in search_paths : resolved_path = os . path . expanduser ( os . path . join ( search_path , path )) if os . path . exists ( resolved_path ): return os . path . abspath ( resolved_path ) return path download download ( url : str , download_path : str ) -> str Download file from URL to a specified path. Source code in cpas_toolbox/utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def download ( url : str , download_path : str ) -> str : \"\"\"Download file from URL to a specified path.\"\"\" block_size = 100 if \"drive.google.com\" in url : gdown . download ( url , download_path ) else : # adapted from https://stackoverflow.com/a/37573701 response = requests . get ( url , stream = True ) total_size_in_bytes = int ( response . headers . get ( \"content-length\" , 0 )) progress_bar = tqdm ( total = total_size_in_bytes , unit = \"iB\" , unit_scale = True ) with open ( download_path , \"wb\" ) as file : for data in response . iter_content ( block_size ): progress_bar . update ( len ( data )) file . write ( data ) progress_bar . close () if total_size_in_bytes != 0 and progress_bar . n != total_size_in_bytes : print ( \"ERROR, download failed\" ) exit ()","title":"utils.py"},{"location":"api_reference/utils/#cpas_toolbox.utils","text":"This module provides miscellaneous utility functions.","title":"utils"},{"location":"api_reference/utils/#cpas_toolbox.utils.str_to_object","text":"str_to_object ( name : str ) -> Any Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. PARAMETER DESCRIPTION name Name of the object to resolve. TYPE: str RETURNS DESCRIPTION Any The object which the provided name refers to. None if no object was found. Source code in cpas_toolbox/utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def str_to_object ( name : str ) -> Any : \"\"\"Try to find object with a given name. First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found. Args: name: Name of the object to resolve. Returns: The object which the provided name refers to. None if no object was found. \"\"\" # check callers local variables caller_locals = inspect . currentframe () . f_back . f_locals if name in caller_locals : return caller_locals [ name ] # check callers global variables (i.e., imported modules etc.) caller_globals = inspect . currentframe () . f_back . f_globals if name in caller_globals : return caller_globals [ name ] # check environment return locate ( name )","title":"str_to_object()"},{"location":"api_reference/utils/#cpas_toolbox.utils.resolve_path","text":"resolve_path ( path : str , search_paths : Optional [ List [ str ]] = None ) -> str Resolves a path to a full absolute path based on search_paths. This function considers paths of 5 different cases /... -> absolute path, nothing todo ~/... -> home dir, expand user ./... -> relative to current directory ../... -> relative to current parent directory ... -> relative to search paths Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists. Returns original path, if file does not exist. PARAMETER DESCRIPTION path The path to resolve. TYPE: str search_paths List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed. TYPE: Optional [ List [ str ]] DEFAULT: None Source code in cpas_toolbox/utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def resolve_path ( path : str , search_paths : Optional [ List [ str ]] = None ) -> str : \"\"\"Resolves a path to a full absolute path based on search_paths. This function considers paths of 5 different cases /... -> absolute path, nothing todo ~/... -> home dir, expand user ./... -> relative to current directory ../... -> relative to current parent directory ... -> relative to search paths Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists. Returns original path, if file does not exist. Args: path: The path to resolve. search_paths: List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed. \"\"\" if search_paths is None : search_paths = [] if os . path . isabs ( path ): return path parts = path . split ( os . sep ) if parts [ 0 ] in [ \".\" , \"..\" ]: return os . path . abspath ( path ) elif parts [ 0 ] == \"~\" : return os . path . expanduser ( path ) for search_path in search_paths : resolved_path = os . path . expanduser ( os . path . join ( search_path , path )) if os . path . exists ( resolved_path ): return os . path . abspath ( resolved_path ) return path","title":"resolve_path()"},{"location":"api_reference/utils/#cpas_toolbox.utils.download","text":"download ( url : str , download_path : str ) -> str Download file from URL to a specified path. Source code in cpas_toolbox/utils.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def download ( url : str , download_path : str ) -> str : \"\"\"Download file from URL to a specified path.\"\"\" block_size = 100 if \"drive.google.com\" in url : gdown . download ( url , download_path ) else : # adapted from https://stackoverflow.com/a/37573701 response = requests . get ( url , stream = True ) total_size_in_bytes = int ( response . headers . get ( \"content-length\" , 0 )) progress_bar = tqdm ( total = total_size_in_bytes , unit = \"iB\" , unit_scale = True ) with open ( download_path , \"wb\" ) as file : for data in response . iter_content ( block_size ): progress_bar . update ( len ( data )) file . write ( data ) progress_bar . close () if total_size_in_bytes != 0 and progress_bar . n != total_size_in_bytes : print ( \"ERROR, download failed\" ) exit ()","title":"download()"},{"location":"api_reference/cpas_methods/asmnet/","text":"cpas_toolbox.cpas_methods.asmnet This module defines ASMNet interface. Method is described in ASM-Net: Category-level Pose and Shape, Akizuki, 2021 Implementation based on https://github.com/sakizuki/asm-net ASMNet Bases: CPASMethod Wrapper class for ASMNet. Source code in cpas_toolbox/cpas_methods/asmnet.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class ASMNet ( CPASMethod ): \"\"\"Wrapper class for ASMNet.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for ASMNet. Attributes: model: Path to model. device: Device string for the model. models_dir: Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... asm_params_dir: Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... weights_url: URL to download model and ASM params from if they do not exist yet. categories: List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. num_points: Number of input points. deformation_dimension: Number of deformation parameters. use_mean_shape: Whether the mean shape (0) or the predicted shape deformation should be used. use_icp: Whether to use ICP to refine the pose. \"\"\" models_dir : str asm_params_dir : str weights_url : str device : str categories : List [ str ] num_points : int deformation_dimension : int use_mean_shape : bool use_icp : bool default_config : Config = { \"model_params_dir\" : None , \"asm_params_dir\" : None , \"weights_url\" : None , \"device\" : \"cuda\" , \"categories\" : [], \"num_points\" : 800 , \"deformation_dimension\" : 3 , \"use_mean_shape\" : False , \"use_icp\" : True , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ASMNet model. Args: config: ASMNet configuration. See ASMNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ASMNet . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _weights_dir_path = utils . resolve_path ( config [ \"models_dir\" ]) self . _asm_params_dir_path = utils . resolve_path ( config [ \"asm_params_dir\" ]) self . _weights_url = config [ \"weights_url\" ] self . _check_paths () synset_names = [ \"placeholder\" ] + config [ \"categories\" ] # first will be ignored self . _asmds = asmnet . cr6d_utils . load_asmds ( self . _asm_params_dir_path , synset_names ) self . _models = asmnet . cr6d_utils . load_models_release ( self . _weights_dir_path , synset_names , config [ \"deformation_dimension\" ], config [ \"num_points\" ], self . _device , ) self . _num_points = config [ \"num_points\" ] self . _use_mean_shape = config [ \"use_mean_shape\" ] self . _use_icp = config [ \"use_icp\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _weights_dir_path ) or not os . path . exists ( self . _asm_params_dir_path ): print ( \"ASM-Net model weights not found, do you want to download to \" ) print ( \" \" , self . _weights_dir_path ) print ( \" \" , self . _asm_params_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"ASM-Net model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : download_dir_path = tempfile . mkdtemp () zip_file_path = os . path . join ( download_dir_path , \"asmnetweights.zip\" ) utils . download ( self . _weights_url , zip_file_path , ) zip_file = zipfile . ZipFile ( zip_file_path ) zip_file . extractall ( download_dir_path ) zip_file . close () os . remove ( zip_file_path ) if not os . path . exists ( self . _asm_params_dir_path ): os . makedirs ( self . _asm_params_dir_path , exist_ok = True ) source_dir_path = os . path . join ( download_dir_path , \"params\" , \"asm_params\" ) file_names = os . listdir ( source_dir_path ) for fn in file_names : shutil . move ( os . path . join ( source_dir_path , fn ), self . _asm_params_dir_path ) if not os . path . exists ( self . _weights_dir_path ): os . makedirs ( self . _weights_dir_path , exist_ok = True ) source_dir_path = os . path . join ( download_dir_path , \"params\" , \"weights\" ) file_names = os . listdir ( source_dir_path ) for fn in file_names : shutil . move ( os . path . join ( source_dir_path , fn ), self . _weights_dir_path ) shutil . rmtree ( os . path . join ( download_dir_path , \"params\" )) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release \"\"\" # torch -> numpy color_image = np . uint8 ( ( color_image * 255 ) . numpy () ) # (H, W, 3), uint8, 0-255, RGB depth_image = np . uint16 (( depth_image * 1000 ) . numpy ()) # (H, W), uint16, mm instance_mask = instance_mask . numpy () # Noise reduction + pointcloud generation masked_depth = depth_image * instance_mask masked_depth = asmnet . common3Dfunc . image_statistical_outlier_removal ( masked_depth , factor = 2.0 ) pcd_obj = asmnet . cr6d_utils . get_pcd_from_rgbd ( color_image . copy (), masked_depth . copy (), self . _camera . get_o3d_pinhole_camera_parameters () . intrinsic , ) [ pcd_obj , _ ] = pcd_obj . remove_statistical_outlier ( 100 , 2.0 ) pcd_in = copy . deepcopy ( pcd_obj ) pcd_c , offset = asmnet . common3Dfunc . centering ( pcd_in ) pcd_n , scale = asmnet . common3Dfunc . size_normalization ( pcd_c ) # o3d -> torch np_pcd = np . array ( pcd_n . points ) np_input = asmnet . cr6d_utils . random_sample ( np_pcd , self . _num_points ) np_input = np_input . astype ( np . float32 ) input_points = torch . from_numpy ( np_input ) # prepare input shape input_points = input_points . unsqueeze ( 0 ) . transpose ( 2 , 1 ) . to ( self . _device ) # evaluate model with torch . no_grad (): dparam_pred , q_pred = self . _models [ category_str ]( input_points ) dparam_pred = dparam_pred . cpu () . numpy () . squeeze () pred_rot = asmnet . cr6d_utils . quaternion2rotationPT ( q_pred ) pred_rot = pred_rot . cpu () . numpy () . squeeze () pred_dp_param = dparam_pred [: - 1 ] # deformation params pred_scaling_param = dparam_pred [ - 1 ] # scale # get shape prediction pcd_pred = None if self . _use_mean_shape : pcd_pred = self . _asmds [ category_str ] . deformation ([ 0 ]) else : pcd_pred = self . _asmds [ category_str ] . deformation ( pred_dp_param ) pcd_pred = pcd_pred . remove_statistical_outlier ( 20 , 1.0 )[ 0 ] pcd_pred . scale ( pred_scaling_param , ( 0.0 , 0.0 , 0.0 )) metric_pcd = copy . deepcopy ( pcd_pred ) metric_pcd . scale ( scale , ( 0.0 , 0.0 , 0.0 )) # undo scale normalization # ICP pcd_pred_posed = copy . deepcopy ( metric_pcd ) pcd_pred_posed . rotate ( pred_rot ) # rotate metric reconstruction pcd_pred_posed . translate ( offset ) # move to center of cropped pcd pred_rt = np . identity ( 4 ) pred_rt [: 3 , : 3 ] = pred_rot if self . _use_icp : pcd_pred_posed_ds = pcd_pred_posed . voxel_down_sample ( 0.005 ) if len ( pcd_pred_posed_ds . points ) > 3 : # remove hidden points pcd_pred_posed_visible = asmnet . common3Dfunc . applyHPR ( pcd_pred_posed_ds ) pcd_in = pcd_in . voxel_down_sample ( 0.005 ) reg_result = o3d . pipelines . registration . registration_icp ( pcd_pred_posed_visible , pcd_in , max_correspondence_distance = 0.02 ) pcd_pred_posed = copy . deepcopy ( pcd_pred_posed_ds ) . transform ( reg_result . transformation ) pred_rt = np . dot ( reg_result . transformation , pred_rt ) else : print ( \"ASM-Net Warning: Couldn't perform ICP, too few points after\" \"voxel down sampling\" ) # center position maxb = pcd_pred_posed . get_max_bound () # bbox max minb = pcd_pred_posed . get_min_bound () # bbox min center = ( maxb - minb ) / 2 + minb # bbox center pred_rt [: 3 , 3 ] = center . copy () position = torch . Tensor ( pred_rt [: 3 , 3 ]) orientation_q = torch . Tensor ( Rotation . from_matrix ( pred_rt [: 3 , : 3 ]) . as_quat () ) reconstructed_points = torch . from_numpy ( np . asarray ( metric_pcd . points )) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for ASMNet. ATTRIBUTE DESCRIPTION model Path to model. device Device string for the model. TYPE: str models_dir Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... TYPE: str asm_params_dir Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... TYPE: str weights_url URL to download model and ASM params from if they do not exist yet. TYPE: str categories List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. TYPE: List [ str ] num_points Number of input points. TYPE: int deformation_dimension Number of deformation parameters. TYPE: int use_mean_shape Whether the mean shape (0) or the predicted shape deformation should be used. TYPE: bool use_icp Whether to use ICP to refine the pose. TYPE: bool Source code in cpas_toolbox/cpas_methods/asmnet.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class Config ( TypedDict ): \"\"\"Configuration dictionary for ASMNet. Attributes: model: Path to model. device: Device string for the model. models_dir: Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... asm_params_dir: Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... weights_url: URL to download model and ASM params from if they do not exist yet. categories: List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. num_points: Number of input points. deformation_dimension: Number of deformation parameters. use_mean_shape: Whether the mean shape (0) or the predicted shape deformation should be used. use_icp: Whether to use ICP to refine the pose. \"\"\" models_dir : str asm_params_dir : str weights_url : str device : str categories : List [ str ] num_points : int deformation_dimension : int use_mean_shape : bool use_icp : bool __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load ASMNet model. PARAMETER DESCRIPTION config ASMNet configuration. See ASMNet.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/asmnet.py 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ASMNet model. Args: config: ASMNet configuration. See ASMNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ASMNet . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release Source code in cpas_toolbox/cpas_methods/asmnet.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release \"\"\" # torch -> numpy color_image = np . uint8 ( ( color_image * 255 ) . numpy () ) # (H, W, 3), uint8, 0-255, RGB depth_image = np . uint16 (( depth_image * 1000 ) . numpy ()) # (H, W), uint16, mm instance_mask = instance_mask . numpy () # Noise reduction + pointcloud generation masked_depth = depth_image * instance_mask masked_depth = asmnet . common3Dfunc . image_statistical_outlier_removal ( masked_depth , factor = 2.0 ) pcd_obj = asmnet . cr6d_utils . get_pcd_from_rgbd ( color_image . copy (), masked_depth . copy (), self . _camera . get_o3d_pinhole_camera_parameters () . intrinsic , ) [ pcd_obj , _ ] = pcd_obj . remove_statistical_outlier ( 100 , 2.0 ) pcd_in = copy . deepcopy ( pcd_obj ) pcd_c , offset = asmnet . common3Dfunc . centering ( pcd_in ) pcd_n , scale = asmnet . common3Dfunc . size_normalization ( pcd_c ) # o3d -> torch np_pcd = np . array ( pcd_n . points ) np_input = asmnet . cr6d_utils . random_sample ( np_pcd , self . _num_points ) np_input = np_input . astype ( np . float32 ) input_points = torch . from_numpy ( np_input ) # prepare input shape input_points = input_points . unsqueeze ( 0 ) . transpose ( 2 , 1 ) . to ( self . _device ) # evaluate model with torch . no_grad (): dparam_pred , q_pred = self . _models [ category_str ]( input_points ) dparam_pred = dparam_pred . cpu () . numpy () . squeeze () pred_rot = asmnet . cr6d_utils . quaternion2rotationPT ( q_pred ) pred_rot = pred_rot . cpu () . numpy () . squeeze () pred_dp_param = dparam_pred [: - 1 ] # deformation params pred_scaling_param = dparam_pred [ - 1 ] # scale # get shape prediction pcd_pred = None if self . _use_mean_shape : pcd_pred = self . _asmds [ category_str ] . deformation ([ 0 ]) else : pcd_pred = self . _asmds [ category_str ] . deformation ( pred_dp_param ) pcd_pred = pcd_pred . remove_statistical_outlier ( 20 , 1.0 )[ 0 ] pcd_pred . scale ( pred_scaling_param , ( 0.0 , 0.0 , 0.0 )) metric_pcd = copy . deepcopy ( pcd_pred ) metric_pcd . scale ( scale , ( 0.0 , 0.0 , 0.0 )) # undo scale normalization # ICP pcd_pred_posed = copy . deepcopy ( metric_pcd ) pcd_pred_posed . rotate ( pred_rot ) # rotate metric reconstruction pcd_pred_posed . translate ( offset ) # move to center of cropped pcd pred_rt = np . identity ( 4 ) pred_rt [: 3 , : 3 ] = pred_rot if self . _use_icp : pcd_pred_posed_ds = pcd_pred_posed . voxel_down_sample ( 0.005 ) if len ( pcd_pred_posed_ds . points ) > 3 : # remove hidden points pcd_pred_posed_visible = asmnet . common3Dfunc . applyHPR ( pcd_pred_posed_ds ) pcd_in = pcd_in . voxel_down_sample ( 0.005 ) reg_result = o3d . pipelines . registration . registration_icp ( pcd_pred_posed_visible , pcd_in , max_correspondence_distance = 0.02 ) pcd_pred_posed = copy . deepcopy ( pcd_pred_posed_ds ) . transform ( reg_result . transformation ) pred_rt = np . dot ( reg_result . transformation , pred_rt ) else : print ( \"ASM-Net Warning: Couldn't perform ICP, too few points after\" \"voxel down sampling\" ) # center position maxb = pcd_pred_posed . get_max_bound () # bbox max minb = pcd_pred_posed . get_min_bound () # bbox min center = ( maxb - minb ) / 2 + minb # bbox center pred_rt [: 3 , 3 ] = center . copy () position = torch . Tensor ( pred_rt [: 3 , 3 ]) orientation_q = torch . Tensor ( Rotation . from_matrix ( pred_rt [: 3 , : 3 ]) . as_quat () ) reconstructed_points = torch . from_numpy ( np . asarray ( metric_pcd . points )) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"asmnet.py"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet","text":"This module defines ASMNet interface. Method is described in ASM-Net: Category-level Pose and Shape, Akizuki, 2021 Implementation based on https://github.com/sakizuki/asm-net","title":"asmnet"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet","text":"Bases: CPASMethod Wrapper class for ASMNet. Source code in cpas_toolbox/cpas_methods/asmnet.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class ASMNet ( CPASMethod ): \"\"\"Wrapper class for ASMNet.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for ASMNet. Attributes: model: Path to model. device: Device string for the model. models_dir: Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... asm_params_dir: Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... weights_url: URL to download model and ASM params from if they do not exist yet. categories: List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. num_points: Number of input points. deformation_dimension: Number of deformation parameters. use_mean_shape: Whether the mean shape (0) or the predicted shape deformation should be used. use_icp: Whether to use ICP to refine the pose. \"\"\" models_dir : str asm_params_dir : str weights_url : str device : str categories : List [ str ] num_points : int deformation_dimension : int use_mean_shape : bool use_icp : bool default_config : Config = { \"model_params_dir\" : None , \"asm_params_dir\" : None , \"weights_url\" : None , \"device\" : \"cuda\" , \"categories\" : [], \"num_points\" : 800 , \"deformation_dimension\" : 3 , \"use_mean_shape\" : False , \"use_icp\" : True , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ASMNet model. Args: config: ASMNet configuration. See ASMNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ASMNet . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _weights_dir_path = utils . resolve_path ( config [ \"models_dir\" ]) self . _asm_params_dir_path = utils . resolve_path ( config [ \"asm_params_dir\" ]) self . _weights_url = config [ \"weights_url\" ] self . _check_paths () synset_names = [ \"placeholder\" ] + config [ \"categories\" ] # first will be ignored self . _asmds = asmnet . cr6d_utils . load_asmds ( self . _asm_params_dir_path , synset_names ) self . _models = asmnet . cr6d_utils . load_models_release ( self . _weights_dir_path , synset_names , config [ \"deformation_dimension\" ], config [ \"num_points\" ], self . _device , ) self . _num_points = config [ \"num_points\" ] self . _use_mean_shape = config [ \"use_mean_shape\" ] self . _use_icp = config [ \"use_icp\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _weights_dir_path ) or not os . path . exists ( self . _asm_params_dir_path ): print ( \"ASM-Net model weights not found, do you want to download to \" ) print ( \" \" , self . _weights_dir_path ) print ( \" \" , self . _asm_params_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"ASM-Net model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : download_dir_path = tempfile . mkdtemp () zip_file_path = os . path . join ( download_dir_path , \"asmnetweights.zip\" ) utils . download ( self . _weights_url , zip_file_path , ) zip_file = zipfile . ZipFile ( zip_file_path ) zip_file . extractall ( download_dir_path ) zip_file . close () os . remove ( zip_file_path ) if not os . path . exists ( self . _asm_params_dir_path ): os . makedirs ( self . _asm_params_dir_path , exist_ok = True ) source_dir_path = os . path . join ( download_dir_path , \"params\" , \"asm_params\" ) file_names = os . listdir ( source_dir_path ) for fn in file_names : shutil . move ( os . path . join ( source_dir_path , fn ), self . _asm_params_dir_path ) if not os . path . exists ( self . _weights_dir_path ): os . makedirs ( self . _weights_dir_path , exist_ok = True ) source_dir_path = os . path . join ( download_dir_path , \"params\" , \"weights\" ) file_names = os . listdir ( source_dir_path ) for fn in file_names : shutil . move ( os . path . join ( source_dir_path , fn ), self . _weights_dir_path ) shutil . rmtree ( os . path . join ( download_dir_path , \"params\" )) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release \"\"\" # torch -> numpy color_image = np . uint8 ( ( color_image * 255 ) . numpy () ) # (H, W, 3), uint8, 0-255, RGB depth_image = np . uint16 (( depth_image * 1000 ) . numpy ()) # (H, W), uint16, mm instance_mask = instance_mask . numpy () # Noise reduction + pointcloud generation masked_depth = depth_image * instance_mask masked_depth = asmnet . common3Dfunc . image_statistical_outlier_removal ( masked_depth , factor = 2.0 ) pcd_obj = asmnet . cr6d_utils . get_pcd_from_rgbd ( color_image . copy (), masked_depth . copy (), self . _camera . get_o3d_pinhole_camera_parameters () . intrinsic , ) [ pcd_obj , _ ] = pcd_obj . remove_statistical_outlier ( 100 , 2.0 ) pcd_in = copy . deepcopy ( pcd_obj ) pcd_c , offset = asmnet . common3Dfunc . centering ( pcd_in ) pcd_n , scale = asmnet . common3Dfunc . size_normalization ( pcd_c ) # o3d -> torch np_pcd = np . array ( pcd_n . points ) np_input = asmnet . cr6d_utils . random_sample ( np_pcd , self . _num_points ) np_input = np_input . astype ( np . float32 ) input_points = torch . from_numpy ( np_input ) # prepare input shape input_points = input_points . unsqueeze ( 0 ) . transpose ( 2 , 1 ) . to ( self . _device ) # evaluate model with torch . no_grad (): dparam_pred , q_pred = self . _models [ category_str ]( input_points ) dparam_pred = dparam_pred . cpu () . numpy () . squeeze () pred_rot = asmnet . cr6d_utils . quaternion2rotationPT ( q_pred ) pred_rot = pred_rot . cpu () . numpy () . squeeze () pred_dp_param = dparam_pred [: - 1 ] # deformation params pred_scaling_param = dparam_pred [ - 1 ] # scale # get shape prediction pcd_pred = None if self . _use_mean_shape : pcd_pred = self . _asmds [ category_str ] . deformation ([ 0 ]) else : pcd_pred = self . _asmds [ category_str ] . deformation ( pred_dp_param ) pcd_pred = pcd_pred . remove_statistical_outlier ( 20 , 1.0 )[ 0 ] pcd_pred . scale ( pred_scaling_param , ( 0.0 , 0.0 , 0.0 )) metric_pcd = copy . deepcopy ( pcd_pred ) metric_pcd . scale ( scale , ( 0.0 , 0.0 , 0.0 )) # undo scale normalization # ICP pcd_pred_posed = copy . deepcopy ( metric_pcd ) pcd_pred_posed . rotate ( pred_rot ) # rotate metric reconstruction pcd_pred_posed . translate ( offset ) # move to center of cropped pcd pred_rt = np . identity ( 4 ) pred_rt [: 3 , : 3 ] = pred_rot if self . _use_icp : pcd_pred_posed_ds = pcd_pred_posed . voxel_down_sample ( 0.005 ) if len ( pcd_pred_posed_ds . points ) > 3 : # remove hidden points pcd_pred_posed_visible = asmnet . common3Dfunc . applyHPR ( pcd_pred_posed_ds ) pcd_in = pcd_in . voxel_down_sample ( 0.005 ) reg_result = o3d . pipelines . registration . registration_icp ( pcd_pred_posed_visible , pcd_in , max_correspondence_distance = 0.02 ) pcd_pred_posed = copy . deepcopy ( pcd_pred_posed_ds ) . transform ( reg_result . transformation ) pred_rt = np . dot ( reg_result . transformation , pred_rt ) else : print ( \"ASM-Net Warning: Couldn't perform ICP, too few points after\" \"voxel down sampling\" ) # center position maxb = pcd_pred_posed . get_max_bound () # bbox max minb = pcd_pred_posed . get_min_bound () # bbox min center = ( maxb - minb ) / 2 + minb # bbox center pred_rt [: 3 , 3 ] = center . copy () position = torch . Tensor ( pred_rt [: 3 , 3 ]) orientation_q = torch . Tensor ( Rotation . from_matrix ( pred_rt [: 3 , : 3 ]) . as_quat () ) reconstructed_points = torch . from_numpy ( np . asarray ( metric_pcd . points )) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"ASMNet"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.Config","text":"Bases: TypedDict Configuration dictionary for ASMNet. ATTRIBUTE DESCRIPTION model Path to model. device Device string for the model. TYPE: str models_dir Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... TYPE: str asm_params_dir Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... TYPE: str weights_url URL to download model and ASM params from if they do not exist yet. TYPE: str categories List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. TYPE: List [ str ] num_points Number of input points. TYPE: int deformation_dimension Number of deformation parameters. TYPE: int use_mean_shape Whether the mean shape (0) or the predicted shape deformation should be used. TYPE: bool use_icp Whether to use ICP to refine the pose. TYPE: bool Source code in cpas_toolbox/cpas_methods/asmnet.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class Config ( TypedDict ): \"\"\"Configuration dictionary for ASMNet. Attributes: model: Path to model. device: Device string for the model. models_dir: Path to directory containing model parameters. Must contain the following directory structure: {models_dir}/{category_0}/model.pth ... asm_params_dir: Path to direactory containing ASM parameters. Must contain the following directory structure: {asm_params_directory}/{category_0}/train/info.npz ... weights_url: URL to download model and ASM params from if they do not exist yet. categories: List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir. num_points: Number of input points. deformation_dimension: Number of deformation parameters. use_mean_shape: Whether the mean shape (0) or the predicted shape deformation should be used. use_icp: Whether to use ICP to refine the pose. \"\"\" models_dir : str asm_params_dir : str weights_url : str device : str categories : List [ str ] num_points : int deformation_dimension : int use_mean_shape : bool use_icp : bool","title":"Config"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load ASMNet model. PARAMETER DESCRIPTION config ASMNet configuration. See ASMNet.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/asmnet.py 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ASMNet model. Args: config: ASMNet configuration. See ASMNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ASMNet . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release Source code in cpas_toolbox/cpas_methods/asmnet.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on asmnet.ASM_Net.test_net_nocs2019_release \"\"\" # torch -> numpy color_image = np . uint8 ( ( color_image * 255 ) . numpy () ) # (H, W, 3), uint8, 0-255, RGB depth_image = np . uint16 (( depth_image * 1000 ) . numpy ()) # (H, W), uint16, mm instance_mask = instance_mask . numpy () # Noise reduction + pointcloud generation masked_depth = depth_image * instance_mask masked_depth = asmnet . common3Dfunc . image_statistical_outlier_removal ( masked_depth , factor = 2.0 ) pcd_obj = asmnet . cr6d_utils . get_pcd_from_rgbd ( color_image . copy (), masked_depth . copy (), self . _camera . get_o3d_pinhole_camera_parameters () . intrinsic , ) [ pcd_obj , _ ] = pcd_obj . remove_statistical_outlier ( 100 , 2.0 ) pcd_in = copy . deepcopy ( pcd_obj ) pcd_c , offset = asmnet . common3Dfunc . centering ( pcd_in ) pcd_n , scale = asmnet . common3Dfunc . size_normalization ( pcd_c ) # o3d -> torch np_pcd = np . array ( pcd_n . points ) np_input = asmnet . cr6d_utils . random_sample ( np_pcd , self . _num_points ) np_input = np_input . astype ( np . float32 ) input_points = torch . from_numpy ( np_input ) # prepare input shape input_points = input_points . unsqueeze ( 0 ) . transpose ( 2 , 1 ) . to ( self . _device ) # evaluate model with torch . no_grad (): dparam_pred , q_pred = self . _models [ category_str ]( input_points ) dparam_pred = dparam_pred . cpu () . numpy () . squeeze () pred_rot = asmnet . cr6d_utils . quaternion2rotationPT ( q_pred ) pred_rot = pred_rot . cpu () . numpy () . squeeze () pred_dp_param = dparam_pred [: - 1 ] # deformation params pred_scaling_param = dparam_pred [ - 1 ] # scale # get shape prediction pcd_pred = None if self . _use_mean_shape : pcd_pred = self . _asmds [ category_str ] . deformation ([ 0 ]) else : pcd_pred = self . _asmds [ category_str ] . deformation ( pred_dp_param ) pcd_pred = pcd_pred . remove_statistical_outlier ( 20 , 1.0 )[ 0 ] pcd_pred . scale ( pred_scaling_param , ( 0.0 , 0.0 , 0.0 )) metric_pcd = copy . deepcopy ( pcd_pred ) metric_pcd . scale ( scale , ( 0.0 , 0.0 , 0.0 )) # undo scale normalization # ICP pcd_pred_posed = copy . deepcopy ( metric_pcd ) pcd_pred_posed . rotate ( pred_rot ) # rotate metric reconstruction pcd_pred_posed . translate ( offset ) # move to center of cropped pcd pred_rt = np . identity ( 4 ) pred_rt [: 3 , : 3 ] = pred_rot if self . _use_icp : pcd_pred_posed_ds = pcd_pred_posed . voxel_down_sample ( 0.005 ) if len ( pcd_pred_posed_ds . points ) > 3 : # remove hidden points pcd_pred_posed_visible = asmnet . common3Dfunc . applyHPR ( pcd_pred_posed_ds ) pcd_in = pcd_in . voxel_down_sample ( 0.005 ) reg_result = o3d . pipelines . registration . registration_icp ( pcd_pred_posed_visible , pcd_in , max_correspondence_distance = 0.02 ) pcd_pred_posed = copy . deepcopy ( pcd_pred_posed_ds ) . transform ( reg_result . transformation ) pred_rt = np . dot ( reg_result . transformation , pred_rt ) else : print ( \"ASM-Net Warning: Couldn't perform ICP, too few points after\" \"voxel down sampling\" ) # center position maxb = pcd_pred_posed . get_max_bound () # bbox max minb = pcd_pred_posed . get_min_bound () # bbox min center = ( maxb - minb ) / 2 + minb # bbox center pred_rt [: 3 , 3 ] = center . copy () position = torch . Tensor ( pred_rt [: 3 , 3 ]) orientation_q = torch . Tensor ( Rotation . from_matrix ( pred_rt [: 3 , : 3 ]) . as_quat () ) reconstructed_points = torch . from_numpy ( np . asarray ( metric_pcd . points )) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/cass/","text":"cpas_toolbox.cpas_methods.cass This module defines CASS interface. Method is described in Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation, Chen, 2020 Implementation based on https://github.com/densechen/CASS CASS Bases: CPASMethod Wrapper class for CASS. Source code in cpas_toolbox/cpas_methods/cass.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class CASS ( CPASMethod ): \"\"\"Wrapper class for CASS.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for CASS. Attributes: model: Path to model. device: Device string for the model. \"\"\" model : str default_config : Config = { \"model\" : None , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CASS model. Args: config: CASS configuration. See CASS.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CASS . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _check_paths () self . _cass = cass . CASS ( num_points = config [ \"num_points\" ], num_obj = config [ \"num_objects\" ] ) self . _num_points = config [ \"num_points\" ] self . _cass . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _cass . to ( self . _device ) self . _cass . eval () def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ): print ( \"CASS model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"CASS model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( \"https://drive.google.com/u/0/uc?id=14K1a-Ft-YO9dUREEXxmWqF2ruUP4p7BZ&\" \"export=download\" , self . _model_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on cass.tools.eval. \"\"\" # get bounding box valid_mask = ( depth_image != 0 ) * instance_mask rmin , rmax , cmin , cmax = cass . get_bbox ( valid_mask . numpy ()) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 # prepare image crop color_input = torch . flip ( color_image , ( 2 ,)) . permute ([ 2 , 0 , 1 ]) # RGB -> BGR color_input = color_input [:, rmin : rmax , cmin : cmax ] # bb crop color_input = color_input . unsqueeze ( 0 ) # add batch dim color_input = TF . normalize ( color_input , mean = [ 0.51 , 0.47 , 0.44 ], std = [ 0.29 , 0.27 , 0.28 ] ) # prepare points (fixed number of points, randomly picked) point_indices = valid_mask . nonzero () if len ( point_indices ) > self . _num_points : subset = np . random . choice ( len ( point_indices ), replace = False , size = self . _num_points ) point_indices = point_indices [ subset ] depth_mask = torch . zeros_like ( depth_image ) depth_mask [ point_indices [:, 0 ], point_indices [:, 1 ]] = 1.0 cropped_depth_mask = depth_mask [ rmin : rmax , cmin : cmax ] point_indices_input = cropped_depth_mask . flatten () . nonzero ()[:, 0 ] # prepare pointcloud points = pointset_utils . depth_to_pointcloud ( depth_image , self . _camera , normalize = False , mask = depth_mask , convention = \"opencv\" , ) if len ( points ) < self . _num_points : wrap_indices = np . pad ( np . arange ( len ( points )), ( 0 , self . _num_points - len ( points )), mode = \"wrap\" ) points = points [ wrap_indices ] point_indices_input = point_indices_input [ wrap_indices ] # x, y inverted for some reason... points [:, 0 ] *= - 1 points [:, 1 ] *= - 1 points = points . unsqueeze ( 0 ) point_indices_input = point_indices_input . unsqueeze ( 0 ) # move inputs to device color_input = color_input . to ( self . _device ) points = points . to ( self . _device ) point_indices_input = point_indices_input . to ( self . _device ) category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # CASS model uses 0-indexed categories, same order as NOCSDataset category_index = torch . tensor ([ category_id ], device = self . _device ) # Call CASS network folding_encode = self . _cass . foldingnet . encode ( color_input , points , point_indices_input ) posenet_encode = self . _cass . estimator . encode ( color_input , points , point_indices_input ) pred_r , pred_t , pred_c = self . _cass . estimator . pose ( torch . cat ([ posenet_encode , folding_encode ], dim = 1 ), category_index ) reconstructed_points = self . _cass . foldingnet . recon ( folding_encode )[ 0 ] # Postprocess outputs reconstructed_points = reconstructed_points . view ( - 1 , 3 ) . cpu () pred_c = pred_c . view ( 1 , self . _num_points ) _ , max_index = torch . max ( pred_c , 1 ) pred_t = pred_t . view ( self . _num_points , 1 , 3 ) orientation_q = pred_r [ 0 ][ max_index [ 0 ]] . view ( - 1 ) . cpu () points = points . view ( self . _num_points , 1 , 3 ) position = ( points + pred_t )[ max_index [ 0 ]] . view ( - 1 ) . cpu () # output is scalar-first -> scalar-last orientation_q = torch . tensor ([ * orientation_q [ 1 :], orientation_q [ 0 ]]) # Flip x and y axis of position and orientation (undo flipping of points) # (x-left, y-up, z-forward) convention -> OpenCV convention position [ 0 ] *= - 1 position [ 1 ] *= - 1 cam_fix = torch . tensor ([ 0.0 , 0.0 , 1.0 , 0.0 ]) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( cam_fix , orientation_q ) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) # TODO refinement code from cass.tools.eval? (not mentioned in paper??) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 # pointset_utils.visualize_pointset(reconstructed_points) return { \"position\" : position . detach (), \"orientation\" : orientation_q . detach (), \"extents\" : extents . detach (), \"reconstructed_pointcloud\" : reconstructed_points . detach (), \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for CASS. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/cass.py 25 26 27 28 29 30 31 32 33 class Config ( TypedDict ): \"\"\"Configuration dictionary for CASS. Attributes: model: Path to model. device: Device string for the model. \"\"\" model : str __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load CASS model. PARAMETER DESCRIPTION config CASS configuration. See CASS.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/cass.py 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CASS model. Args: config: CASS configuration. See CASS.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CASS . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Based on cass.tools.eval. Source code in cpas_toolbox/cpas_methods/cass.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on cass.tools.eval. \"\"\" # get bounding box valid_mask = ( depth_image != 0 ) * instance_mask rmin , rmax , cmin , cmax = cass . get_bbox ( valid_mask . numpy ()) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 # prepare image crop color_input = torch . flip ( color_image , ( 2 ,)) . permute ([ 2 , 0 , 1 ]) # RGB -> BGR color_input = color_input [:, rmin : rmax , cmin : cmax ] # bb crop color_input = color_input . unsqueeze ( 0 ) # add batch dim color_input = TF . normalize ( color_input , mean = [ 0.51 , 0.47 , 0.44 ], std = [ 0.29 , 0.27 , 0.28 ] ) # prepare points (fixed number of points, randomly picked) point_indices = valid_mask . nonzero () if len ( point_indices ) > self . _num_points : subset = np . random . choice ( len ( point_indices ), replace = False , size = self . _num_points ) point_indices = point_indices [ subset ] depth_mask = torch . zeros_like ( depth_image ) depth_mask [ point_indices [:, 0 ], point_indices [:, 1 ]] = 1.0 cropped_depth_mask = depth_mask [ rmin : rmax , cmin : cmax ] point_indices_input = cropped_depth_mask . flatten () . nonzero ()[:, 0 ] # prepare pointcloud points = pointset_utils . depth_to_pointcloud ( depth_image , self . _camera , normalize = False , mask = depth_mask , convention = \"opencv\" , ) if len ( points ) < self . _num_points : wrap_indices = np . pad ( np . arange ( len ( points )), ( 0 , self . _num_points - len ( points )), mode = \"wrap\" ) points = points [ wrap_indices ] point_indices_input = point_indices_input [ wrap_indices ] # x, y inverted for some reason... points [:, 0 ] *= - 1 points [:, 1 ] *= - 1 points = points . unsqueeze ( 0 ) point_indices_input = point_indices_input . unsqueeze ( 0 ) # move inputs to device color_input = color_input . to ( self . _device ) points = points . to ( self . _device ) point_indices_input = point_indices_input . to ( self . _device ) category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # CASS model uses 0-indexed categories, same order as NOCSDataset category_index = torch . tensor ([ category_id ], device = self . _device ) # Call CASS network folding_encode = self . _cass . foldingnet . encode ( color_input , points , point_indices_input ) posenet_encode = self . _cass . estimator . encode ( color_input , points , point_indices_input ) pred_r , pred_t , pred_c = self . _cass . estimator . pose ( torch . cat ([ posenet_encode , folding_encode ], dim = 1 ), category_index ) reconstructed_points = self . _cass . foldingnet . recon ( folding_encode )[ 0 ] # Postprocess outputs reconstructed_points = reconstructed_points . view ( - 1 , 3 ) . cpu () pred_c = pred_c . view ( 1 , self . _num_points ) _ , max_index = torch . max ( pred_c , 1 ) pred_t = pred_t . view ( self . _num_points , 1 , 3 ) orientation_q = pred_r [ 0 ][ max_index [ 0 ]] . view ( - 1 ) . cpu () points = points . view ( self . _num_points , 1 , 3 ) position = ( points + pred_t )[ max_index [ 0 ]] . view ( - 1 ) . cpu () # output is scalar-first -> scalar-last orientation_q = torch . tensor ([ * orientation_q [ 1 :], orientation_q [ 0 ]]) # Flip x and y axis of position and orientation (undo flipping of points) # (x-left, y-up, z-forward) convention -> OpenCV convention position [ 0 ] *= - 1 position [ 1 ] *= - 1 cam_fix = torch . tensor ([ 0.0 , 0.0 , 1.0 , 0.0 ]) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( cam_fix , orientation_q ) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) # TODO refinement code from cass.tools.eval? (not mentioned in paper??) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 # pointset_utils.visualize_pointset(reconstructed_points) return { \"position\" : position . detach (), \"orientation\" : orientation_q . detach (), \"extents\" : extents . detach (), \"reconstructed_pointcloud\" : reconstructed_points . detach (), \"reconstructed_mesh\" : None , }","title":"cass.py"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass","text":"This module defines CASS interface. Method is described in Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation, Chen, 2020 Implementation based on https://github.com/densechen/CASS","title":"cass"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS","text":"Bases: CPASMethod Wrapper class for CASS. Source code in cpas_toolbox/cpas_methods/cass.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class CASS ( CPASMethod ): \"\"\"Wrapper class for CASS.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for CASS. Attributes: model: Path to model. device: Device string for the model. \"\"\" model : str default_config : Config = { \"model\" : None , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CASS model. Args: config: CASS configuration. See CASS.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CASS . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _check_paths () self . _cass = cass . CASS ( num_points = config [ \"num_points\" ], num_obj = config [ \"num_objects\" ] ) self . _num_points = config [ \"num_points\" ] self . _cass . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _cass . to ( self . _device ) self . _cass . eval () def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ): print ( \"CASS model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"CASS model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( \"https://drive.google.com/u/0/uc?id=14K1a-Ft-YO9dUREEXxmWqF2ruUP4p7BZ&\" \"export=download\" , self . _model_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on cass.tools.eval. \"\"\" # get bounding box valid_mask = ( depth_image != 0 ) * instance_mask rmin , rmax , cmin , cmax = cass . get_bbox ( valid_mask . numpy ()) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 # prepare image crop color_input = torch . flip ( color_image , ( 2 ,)) . permute ([ 2 , 0 , 1 ]) # RGB -> BGR color_input = color_input [:, rmin : rmax , cmin : cmax ] # bb crop color_input = color_input . unsqueeze ( 0 ) # add batch dim color_input = TF . normalize ( color_input , mean = [ 0.51 , 0.47 , 0.44 ], std = [ 0.29 , 0.27 , 0.28 ] ) # prepare points (fixed number of points, randomly picked) point_indices = valid_mask . nonzero () if len ( point_indices ) > self . _num_points : subset = np . random . choice ( len ( point_indices ), replace = False , size = self . _num_points ) point_indices = point_indices [ subset ] depth_mask = torch . zeros_like ( depth_image ) depth_mask [ point_indices [:, 0 ], point_indices [:, 1 ]] = 1.0 cropped_depth_mask = depth_mask [ rmin : rmax , cmin : cmax ] point_indices_input = cropped_depth_mask . flatten () . nonzero ()[:, 0 ] # prepare pointcloud points = pointset_utils . depth_to_pointcloud ( depth_image , self . _camera , normalize = False , mask = depth_mask , convention = \"opencv\" , ) if len ( points ) < self . _num_points : wrap_indices = np . pad ( np . arange ( len ( points )), ( 0 , self . _num_points - len ( points )), mode = \"wrap\" ) points = points [ wrap_indices ] point_indices_input = point_indices_input [ wrap_indices ] # x, y inverted for some reason... points [:, 0 ] *= - 1 points [:, 1 ] *= - 1 points = points . unsqueeze ( 0 ) point_indices_input = point_indices_input . unsqueeze ( 0 ) # move inputs to device color_input = color_input . to ( self . _device ) points = points . to ( self . _device ) point_indices_input = point_indices_input . to ( self . _device ) category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # CASS model uses 0-indexed categories, same order as NOCSDataset category_index = torch . tensor ([ category_id ], device = self . _device ) # Call CASS network folding_encode = self . _cass . foldingnet . encode ( color_input , points , point_indices_input ) posenet_encode = self . _cass . estimator . encode ( color_input , points , point_indices_input ) pred_r , pred_t , pred_c = self . _cass . estimator . pose ( torch . cat ([ posenet_encode , folding_encode ], dim = 1 ), category_index ) reconstructed_points = self . _cass . foldingnet . recon ( folding_encode )[ 0 ] # Postprocess outputs reconstructed_points = reconstructed_points . view ( - 1 , 3 ) . cpu () pred_c = pred_c . view ( 1 , self . _num_points ) _ , max_index = torch . max ( pred_c , 1 ) pred_t = pred_t . view ( self . _num_points , 1 , 3 ) orientation_q = pred_r [ 0 ][ max_index [ 0 ]] . view ( - 1 ) . cpu () points = points . view ( self . _num_points , 1 , 3 ) position = ( points + pred_t )[ max_index [ 0 ]] . view ( - 1 ) . cpu () # output is scalar-first -> scalar-last orientation_q = torch . tensor ([ * orientation_q [ 1 :], orientation_q [ 0 ]]) # Flip x and y axis of position and orientation (undo flipping of points) # (x-left, y-up, z-forward) convention -> OpenCV convention position [ 0 ] *= - 1 position [ 1 ] *= - 1 cam_fix = torch . tensor ([ 0.0 , 0.0 , 1.0 , 0.0 ]) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( cam_fix , orientation_q ) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) # TODO refinement code from cass.tools.eval? (not mentioned in paper??) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 # pointset_utils.visualize_pointset(reconstructed_points) return { \"position\" : position . detach (), \"orientation\" : orientation_q . detach (), \"extents\" : extents . detach (), \"reconstructed_pointcloud\" : reconstructed_points . detach (), \"reconstructed_mesh\" : None , }","title":"CASS"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.Config","text":"Bases: TypedDict Configuration dictionary for CASS. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/cass.py 25 26 27 28 29 30 31 32 33 class Config ( TypedDict ): \"\"\"Configuration dictionary for CASS. Attributes: model: Path to model. device: Device string for the model. \"\"\" model : str","title":"Config"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load CASS model. PARAMETER DESCRIPTION config CASS configuration. See CASS.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/cass.py 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CASS model. Args: config: CASS configuration. See CASS.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CASS . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Based on cass.tools.eval. Source code in cpas_toolbox/cpas_methods/cass.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference. Based on cass.tools.eval. \"\"\" # get bounding box valid_mask = ( depth_image != 0 ) * instance_mask rmin , rmax , cmin , cmax = cass . get_bbox ( valid_mask . numpy ()) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 # prepare image crop color_input = torch . flip ( color_image , ( 2 ,)) . permute ([ 2 , 0 , 1 ]) # RGB -> BGR color_input = color_input [:, rmin : rmax , cmin : cmax ] # bb crop color_input = color_input . unsqueeze ( 0 ) # add batch dim color_input = TF . normalize ( color_input , mean = [ 0.51 , 0.47 , 0.44 ], std = [ 0.29 , 0.27 , 0.28 ] ) # prepare points (fixed number of points, randomly picked) point_indices = valid_mask . nonzero () if len ( point_indices ) > self . _num_points : subset = np . random . choice ( len ( point_indices ), replace = False , size = self . _num_points ) point_indices = point_indices [ subset ] depth_mask = torch . zeros_like ( depth_image ) depth_mask [ point_indices [:, 0 ], point_indices [:, 1 ]] = 1.0 cropped_depth_mask = depth_mask [ rmin : rmax , cmin : cmax ] point_indices_input = cropped_depth_mask . flatten () . nonzero ()[:, 0 ] # prepare pointcloud points = pointset_utils . depth_to_pointcloud ( depth_image , self . _camera , normalize = False , mask = depth_mask , convention = \"opencv\" , ) if len ( points ) < self . _num_points : wrap_indices = np . pad ( np . arange ( len ( points )), ( 0 , self . _num_points - len ( points )), mode = \"wrap\" ) points = points [ wrap_indices ] point_indices_input = point_indices_input [ wrap_indices ] # x, y inverted for some reason... points [:, 0 ] *= - 1 points [:, 1 ] *= - 1 points = points . unsqueeze ( 0 ) point_indices_input = point_indices_input . unsqueeze ( 0 ) # move inputs to device color_input = color_input . to ( self . _device ) points = points . to ( self . _device ) point_indices_input = point_indices_input . to ( self . _device ) category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # CASS model uses 0-indexed categories, same order as NOCSDataset category_index = torch . tensor ([ category_id ], device = self . _device ) # Call CASS network folding_encode = self . _cass . foldingnet . encode ( color_input , points , point_indices_input ) posenet_encode = self . _cass . estimator . encode ( color_input , points , point_indices_input ) pred_r , pred_t , pred_c = self . _cass . estimator . pose ( torch . cat ([ posenet_encode , folding_encode ], dim = 1 ), category_index ) reconstructed_points = self . _cass . foldingnet . recon ( folding_encode )[ 0 ] # Postprocess outputs reconstructed_points = reconstructed_points . view ( - 1 , 3 ) . cpu () pred_c = pred_c . view ( 1 , self . _num_points ) _ , max_index = torch . max ( pred_c , 1 ) pred_t = pred_t . view ( self . _num_points , 1 , 3 ) orientation_q = pred_r [ 0 ][ max_index [ 0 ]] . view ( - 1 ) . cpu () points = points . view ( self . _num_points , 1 , 3 ) position = ( points + pred_t )[ max_index [ 0 ]] . view ( - 1 ) . cpu () # output is scalar-first -> scalar-last orientation_q = torch . tensor ([ * orientation_q [ 1 :], orientation_q [ 0 ]]) # Flip x and y axis of position and orientation (undo flipping of points) # (x-left, y-up, z-forward) convention -> OpenCV convention position [ 0 ] *= - 1 position [ 1 ] *= - 1 cam_fix = torch . tensor ([ 0.0 , 0.0 , 1.0 , 0.0 ]) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( cam_fix , orientation_q ) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) # TODO refinement code from cass.tools.eval? (not mentioned in paper??) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 # pointset_utils.visualize_pointset(reconstructed_points) return { \"position\" : position . detach (), \"orientation\" : orientation_q . detach (), \"extents\" : extents . detach (), \"reconstructed_pointcloud\" : reconstructed_points . detach (), \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/crnet/","text":"cpas_toolbox.cpas_methods.crnet This module defines CR-Net interface. Method is described in Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks, Wang, 2021. Implementation based on https://github.com/JeremyWANGJZ/Category-6D-Pose CRNet Bases: CPASMethod Wrapper class for CRNet. Source code in cpas_toolbox/cpas_methods/crnet.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class CRNet ( CPASMethod ): \"\"\"Wrapper class for CRNet.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for CRNet. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CRNet model. Args: config: CRNet configuration. See CRNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CRNet . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _crnet = crnet . DeformNet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ] ) self . _crnet . cuda () self . _crnet = torch . nn . DataParallel ( self . _crnet , device_ids = [ self . _device ]) self . _crnet . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _crnet . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ) or not os . path . exists ( self . _mean_shape_path ): print ( \"CRNet model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) print ( \" \" , self . _mean_shape_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"CRNet model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_path , ) if not os . path . exists ( self . _mean_shape_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = crnet . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call CRNet assign_matrix , deltas = self . _crnet ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = crnet . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for CRNet. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int device Device string for the model. TYPE: int Source code in cpas_toolbox/cpas_methods/crnet.py 28 29 30 31 32 33 34 35 36 37 38 39 class Config ( TypedDict ): \"\"\"Configuration dictionary for CRNet. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load CRNet model. PARAMETER DESCRIPTION config CRNet configuration. See CRNet.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/crnet.py 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CRNet model. Args: config: CRNet configuration. See CRNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CRNet . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py Source code in cpas_toolbox/cpas_methods/crnet.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = crnet . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call CRNet assign_matrix , deltas = self . _crnet ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = crnet . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"crnet.py"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet","text":"This module defines CR-Net interface. Method is described in Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks, Wang, 2021. Implementation based on https://github.com/JeremyWANGJZ/Category-6D-Pose","title":"crnet"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet","text":"Bases: CPASMethod Wrapper class for CRNet. Source code in cpas_toolbox/cpas_methods/crnet.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 class CRNet ( CPASMethod ): \"\"\"Wrapper class for CRNet.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for CRNet. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CRNet model. Args: config: CRNet configuration. See CRNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CRNet . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _crnet = crnet . DeformNet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ] ) self . _crnet . cuda () self . _crnet = torch . nn . DataParallel ( self . _crnet , device_ids = [ self . _device ]) self . _crnet . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _crnet . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ) or not os . path . exists ( self . _mean_shape_path ): print ( \"CRNet model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) print ( \" \" , self . _mean_shape_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"CRNet model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_path , ) if not os . path . exists ( self . _mean_shape_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = crnet . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call CRNet assign_matrix , deltas = self . _crnet ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = crnet . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"CRNet"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.Config","text":"Bases: TypedDict Configuration dictionary for CRNet. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int device Device string for the model. TYPE: int Source code in cpas_toolbox/cpas_methods/crnet.py 28 29 30 31 32 33 34 35 36 37 38 39 class Config ( TypedDict ): \"\"\"Configuration dictionary for CRNet. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int","title":"Config"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load CRNet model. PARAMETER DESCRIPTION config CRNet configuration. See CRNet.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/crnet.py 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load CRNet model. Args: config: CRNet configuration. See CRNet.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = CRNet . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py Source code in cpas_toolbox/cpas_methods/crnet.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = crnet . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call CRNet assign_matrix , deltas = self . _crnet ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = crnet . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/dpdn/","text":"cpas_toolbox.cpas_methods.dpdn This module defines DPDN interface. Method is described in Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks, Lin, 2022. Implementation based on https://github.com/JiehongLin/Self-DPDN . DPDN Bases: CPASMethod Wrapper class for DPDN. Source code in cpas_toolbox/cpas_methods/dpdn.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class DPDN ( CPASMethod ): \"\"\"Wrapper class for DPDN.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for DPDN. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"image_size\" : None , \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load DPDN model. Args: config: DPDN configuration. See DPDN.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = DPDN . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _dpdn = dpdn . Net ( config [ \"num_categories\" ], config [ \"num_shape_points\" ]) self . _dpdn = self . _dpdn . to ( self . _device ) checkpoint = torch . load ( self . _model_file_path , map_location = self . _device ) if \"model\" in checkpoint : state_dict = checkpoint [ \"model\" ] elif \"state_dict\" : state_dict = checkpoint [ \"state_dict\" ] else : state_dict = checkpoint self . _dpdn . load_state_dict ( state_dict ) self . _dpdn . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"DPDN model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"DPDN model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): dl_dir_path = tempfile . mkdtemp () print ( dl_dir_path ) zip_file_path = os . path . join ( dl_dir_path , \"temp\" ) os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) utils . download ( self . _model_url , zip_file_path , ) z = zipfile . ZipFile ( zip_file_path ) z . extract ( \"log/supervised/epoch_30.pth\" , dl_dir_path ) z . close () os . remove ( zip_file_path ) shutil . move ( os . path . join ( dl_dir_path , \"log\" , \"supervised\" , \"epoch_30.pth\" ), self . _model_file_path , ) shutil . rmtree ( dl_dir_path ) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] input_dict = {} # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = dpdn . get_bbox ([ y1 , x1 , y2 , x2 ]) # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) input_dict [ \"rgb\" ] = color_input . unsqueeze ( 0 ) . to ( self . _device ) # Prepare point indices mask = ( depth_image != 0 ) * instance_mask cropped_mask = mask [ rmin : rmax , cmin : cmax ] cropped_mask_indices = cropped_mask . numpy () . flatten () . nonzero ()[ 0 ] if len ( cropped_mask_indices ) <= self . _num_input_points : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points ) else : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points , replace = False ) chosen_cropped_indices = cropped_mask_indices [ indices ] # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = chosen_cropped_indices % crop_w row_idx = chosen_cropped_indices // crop_w final_cropped_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) input_dict [ \"choose\" ] = ( torch . LongTensor ( final_cropped_indices ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height depth_image_np = depth_image . numpy () depth_image_np = dpdn . fill_missing ( depth_image_np * 1000.0 , 1000.0 , 1 ) / 1000.0 xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) pts2 = depth_image_np . copy () pts0 = ( xmap - cx ) * pts2 / fx pts1 = ( ymap - cy ) * pts2 / fy pts_map = np . stack ([ pts0 , pts1 , pts2 ]) pts_map = np . transpose ( pts_map , ( 1 , 2 , 0 )) . astype ( np . float32 ) cropped_pts_map = pts_map [ rmin : rmax , cmin : cmax , :] input_points = cropped_pts_map . reshape (( - 1 , 3 ))[ chosen_cropped_indices , :] input_dict [ \"pts\" ] = ( torch . FloatTensor ( input_points ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare prior input_dict [ \"prior\" ] = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare category id input_dict [ \"category_label\" ] = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) # Call DPDN outputs = self . _dpdn ( input_dict ) # Convert outputs to expected format position = outputs [ \"pred_translation\" ][ 0 ] . detach () . cpu () orientation_mat = outputs [ \"pred_rotation\" ][ 0 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . detach () . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = outputs [ \"pred_size\" ][ 0 ] . detach () . cpu () reconstructed_points = outputs [ \"pred_qv\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points *= scale # Recenter for mug category # TODO not really sure if this is correct, but seems to give best results if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for DPDN. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/dpdn.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Config ( TypedDict ): \"\"\"Configuration dictionary for DPDN. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int model : str model_url : str mean_shape : str mean_shape_url : str device : str __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load DPDN model. PARAMETER DESCRIPTION config DPDN configuration. See DPDN.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/dpdn.py 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load DPDN model. Args: config: DPDN configuration. See DPDN.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = DPDN . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py Source code in cpas_toolbox/cpas_methods/dpdn.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] input_dict = {} # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = dpdn . get_bbox ([ y1 , x1 , y2 , x2 ]) # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) input_dict [ \"rgb\" ] = color_input . unsqueeze ( 0 ) . to ( self . _device ) # Prepare point indices mask = ( depth_image != 0 ) * instance_mask cropped_mask = mask [ rmin : rmax , cmin : cmax ] cropped_mask_indices = cropped_mask . numpy () . flatten () . nonzero ()[ 0 ] if len ( cropped_mask_indices ) <= self . _num_input_points : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points ) else : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points , replace = False ) chosen_cropped_indices = cropped_mask_indices [ indices ] # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = chosen_cropped_indices % crop_w row_idx = chosen_cropped_indices // crop_w final_cropped_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) input_dict [ \"choose\" ] = ( torch . LongTensor ( final_cropped_indices ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height depth_image_np = depth_image . numpy () depth_image_np = dpdn . fill_missing ( depth_image_np * 1000.0 , 1000.0 , 1 ) / 1000.0 xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) pts2 = depth_image_np . copy () pts0 = ( xmap - cx ) * pts2 / fx pts1 = ( ymap - cy ) * pts2 / fy pts_map = np . stack ([ pts0 , pts1 , pts2 ]) pts_map = np . transpose ( pts_map , ( 1 , 2 , 0 )) . astype ( np . float32 ) cropped_pts_map = pts_map [ rmin : rmax , cmin : cmax , :] input_points = cropped_pts_map . reshape (( - 1 , 3 ))[ chosen_cropped_indices , :] input_dict [ \"pts\" ] = ( torch . FloatTensor ( input_points ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare prior input_dict [ \"prior\" ] = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare category id input_dict [ \"category_label\" ] = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) # Call DPDN outputs = self . _dpdn ( input_dict ) # Convert outputs to expected format position = outputs [ \"pred_translation\" ][ 0 ] . detach () . cpu () orientation_mat = outputs [ \"pred_rotation\" ][ 0 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . detach () . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = outputs [ \"pred_size\" ][ 0 ] . detach () . cpu () reconstructed_points = outputs [ \"pred_qv\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points *= scale # Recenter for mug category # TODO not really sure if this is correct, but seems to give best results if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"dpdn.py"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn","text":"This module defines DPDN interface. Method is described in Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks, Lin, 2022. Implementation based on https://github.com/JiehongLin/Self-DPDN .","title":"dpdn"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN","text":"Bases: CPASMethod Wrapper class for DPDN. Source code in cpas_toolbox/cpas_methods/dpdn.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class DPDN ( CPASMethod ): \"\"\"Wrapper class for DPDN.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for DPDN. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"image_size\" : None , \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load DPDN model. Args: config: DPDN configuration. See DPDN.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = DPDN . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _dpdn = dpdn . Net ( config [ \"num_categories\" ], config [ \"num_shape_points\" ]) self . _dpdn = self . _dpdn . to ( self . _device ) checkpoint = torch . load ( self . _model_file_path , map_location = self . _device ) if \"model\" in checkpoint : state_dict = checkpoint [ \"model\" ] elif \"state_dict\" : state_dict = checkpoint [ \"state_dict\" ] else : state_dict = checkpoint self . _dpdn . load_state_dict ( state_dict ) self . _dpdn . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"DPDN model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"DPDN model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): dl_dir_path = tempfile . mkdtemp () print ( dl_dir_path ) zip_file_path = os . path . join ( dl_dir_path , \"temp\" ) os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) utils . download ( self . _model_url , zip_file_path , ) z = zipfile . ZipFile ( zip_file_path ) z . extract ( \"log/supervised/epoch_30.pth\" , dl_dir_path ) z . close () os . remove ( zip_file_path ) shutil . move ( os . path . join ( dl_dir_path , \"log\" , \"supervised\" , \"epoch_30.pth\" ), self . _model_file_path , ) shutil . rmtree ( dl_dir_path ) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] input_dict = {} # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = dpdn . get_bbox ([ y1 , x1 , y2 , x2 ]) # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) input_dict [ \"rgb\" ] = color_input . unsqueeze ( 0 ) . to ( self . _device ) # Prepare point indices mask = ( depth_image != 0 ) * instance_mask cropped_mask = mask [ rmin : rmax , cmin : cmax ] cropped_mask_indices = cropped_mask . numpy () . flatten () . nonzero ()[ 0 ] if len ( cropped_mask_indices ) <= self . _num_input_points : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points ) else : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points , replace = False ) chosen_cropped_indices = cropped_mask_indices [ indices ] # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = chosen_cropped_indices % crop_w row_idx = chosen_cropped_indices // crop_w final_cropped_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) input_dict [ \"choose\" ] = ( torch . LongTensor ( final_cropped_indices ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height depth_image_np = depth_image . numpy () depth_image_np = dpdn . fill_missing ( depth_image_np * 1000.0 , 1000.0 , 1 ) / 1000.0 xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) pts2 = depth_image_np . copy () pts0 = ( xmap - cx ) * pts2 / fx pts1 = ( ymap - cy ) * pts2 / fy pts_map = np . stack ([ pts0 , pts1 , pts2 ]) pts_map = np . transpose ( pts_map , ( 1 , 2 , 0 )) . astype ( np . float32 ) cropped_pts_map = pts_map [ rmin : rmax , cmin : cmax , :] input_points = cropped_pts_map . reshape (( - 1 , 3 ))[ chosen_cropped_indices , :] input_dict [ \"pts\" ] = ( torch . FloatTensor ( input_points ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare prior input_dict [ \"prior\" ] = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare category id input_dict [ \"category_label\" ] = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) # Call DPDN outputs = self . _dpdn ( input_dict ) # Convert outputs to expected format position = outputs [ \"pred_translation\" ][ 0 ] . detach () . cpu () orientation_mat = outputs [ \"pred_rotation\" ][ 0 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . detach () . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = outputs [ \"pred_size\" ][ 0 ] . detach () . cpu () reconstructed_points = outputs [ \"pred_qv\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points *= scale # Recenter for mug category # TODO not really sure if this is correct, but seems to give best results if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"DPDN"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.Config","text":"Bases: TypedDict Configuration dictionary for DPDN. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/dpdn.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Config ( TypedDict ): \"\"\"Configuration dictionary for DPDN. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int model : str model_url : str mean_shape : str mean_shape_url : str device : str","title":"Config"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load DPDN model. PARAMETER DESCRIPTION config DPDN configuration. See DPDN.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/dpdn.py 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load DPDN model. Args: config: DPDN configuration. See DPDN.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = DPDN . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py Source code in cpas_toolbox/cpas_methods/dpdn.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] input_dict = {} # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = dpdn . get_bbox ([ y1 , x1 , y2 , x2 ]) # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) input_dict [ \"rgb\" ] = color_input . unsqueeze ( 0 ) . to ( self . _device ) # Prepare point indices mask = ( depth_image != 0 ) * instance_mask cropped_mask = mask [ rmin : rmax , cmin : cmax ] cropped_mask_indices = cropped_mask . numpy () . flatten () . nonzero ()[ 0 ] if len ( cropped_mask_indices ) <= self . _num_input_points : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points ) else : indices = np . random . choice ( len ( cropped_mask_indices ), self . _num_input_points , replace = False ) chosen_cropped_indices = cropped_mask_indices [ indices ] # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = chosen_cropped_indices % crop_w row_idx = chosen_cropped_indices // crop_w final_cropped_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) input_dict [ \"choose\" ] = ( torch . LongTensor ( final_cropped_indices ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height depth_image_np = depth_image . numpy () depth_image_np = dpdn . fill_missing ( depth_image_np * 1000.0 , 1000.0 , 1 ) / 1000.0 xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) pts2 = depth_image_np . copy () pts0 = ( xmap - cx ) * pts2 / fx pts1 = ( ymap - cy ) * pts2 / fy pts_map = np . stack ([ pts0 , pts1 , pts2 ]) pts_map = np . transpose ( pts_map , ( 1 , 2 , 0 )) . astype ( np . float32 ) cropped_pts_map = pts_map [ rmin : rmax , cmin : cmax , :] input_points = cropped_pts_map . reshape (( - 1 , 3 ))[ chosen_cropped_indices , :] input_dict [ \"pts\" ] = ( torch . FloatTensor ( input_points ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare prior input_dict [ \"prior\" ] = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare category id input_dict [ \"category_label\" ] = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) # Call DPDN outputs = self . _dpdn ( input_dict ) # Convert outputs to expected format position = outputs [ \"pred_translation\" ][ 0 ] . detach () . cpu () orientation_mat = outputs [ \"pred_rotation\" ][ 0 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . detach () . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = outputs [ \"pred_size\" ][ 0 ] . detach () . cpu () reconstructed_points = outputs [ \"pred_qv\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points *= scale # Recenter for mug category # TODO not really sure if this is correct, but seems to give best results if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/icaps/","text":"cpas_toolbox.cpas_methods.icaps This module defines iCaps interface. Method is described in iCaps Iterative Category-Level Object Pose and Shape Estimation, Deng, 2022. Implementation based on https://github.com/aerogjy/iCaps ICaps Bases: CPASMethod Wrapper class for iCaps. Source code in cpas_toolbox/cpas_methods/icaps.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 class ICaps ( CPASMethod ): \"\"\"Wrapper class for iCaps.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for iCaps. Attributes: pf_config_dir: Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints. latentnet_checkpoint_dir: Directory containing LatentNet checkpoints. aae_checkpoint_dir: Directory containing auto-encoder checkpoints. checkpoints_url: URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). categories: List of category strings. Each category requires corresponding directories in each checkpoint dir. \"\"\" pf_config_dir : str deepsdf_checkpoint_dir : str latentnet_checkpoint_dir : str aae_checkpoint_dir : str checkpoints_url : str categories : List [ str ] default_config : Config = { \"pf_config_dir\" : None , \"deepsdf_checkpoint_dir\" : None , \"latentnet_checkpoint_dir\" : None , \"aae_checkpoint_dir\" : None , \"checkpoints_url\" : None , \"categories\" : [ \"bottle\" , \"bowl\" , \"camera\" , \"can\" , \"laptop\" , \"mug\" ], } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ICaps models. Args: config: iCaps configuration. See ICaps.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ICaps . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _num_points = 10000 self . _category_strs = config [ \"categories\" ] self . _checkpoints_url = config [ \"checkpoints_url\" ] self . _pose_rbpfs = {} pf_cfg_dir_path = utils . resolve_path ( config [ \"pf_config_dir\" ], search_paths = [ \".\" , \"~/.cpas_toolbox\" , os . path . join ( os . path . dirname ( __file__ ), \"config\" ), os . path . dirname ( __file__ ), ], ) self . _deepsdf_ckp_dir_path = utils . resolve_path ( config [ \"deepsdf_checkpoint_dir\" ] ) self . _latentnet_ckp_dir_path = utils . resolve_path ( config [ \"latentnet_checkpoint_dir\" ] ) self . _aae_ckp_dir_path = utils . resolve_path ( config [ \"aae_checkpoint_dir\" ]) self . _check_paths () for category_str in self . _category_strs : full_ckpt_dir_path = os . path . join ( self . _aae_ckp_dir_path , category_str ) train_cfg_file = os . path . join ( full_ckpt_dir_path , \"config.yml\" ) icaps . icaps_config . cfg_from_file ( train_cfg_file ) test_cfg_file = os . path . join ( pf_cfg_dir_path , category_str + \".yml\" ) icaps . icaps_config . cfg_from_file ( test_cfg_file ) obj_list = icaps . icaps_config . cfg . TEST . OBJECTS cfg_list = [] cfg_list . append ( copy . deepcopy ( icaps . icaps_config . cfg )) self . _pose_rbpfs [ category_str ] = icaps . PoseRBPF ( obj_list , cfg_list , full_ckpt_dir_path , self . _deepsdf_ckp_dir_path , self . _latentnet_ckp_dir_path , ) self . _pose_rbpfs [ category_str ] . set_target_obj ( icaps . icaps_config . cfg . TEST . OBJECTS [ 0 ] ) def _check_paths ( self ) -> None : path_exists = ( os . path . exists ( p ) for p in [ self . _aae_ckp_dir_path , self . _deepsdf_ckp_dir_path , self . _latentnet_ckp_dir_path , ] ) if not all ( path_exists ): print ( \"iCaps model weights not found, do you want to download to \" ) print ( \" \" , self . _aae_ckp_dir_path ) print ( \" \" , self . _deepsdf_ckp_dir_path ) print ( \" \" , self . _latentnet_ckp_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"iCaps model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : dl_dir_path = tempfile . mkdtemp () tar_file_path = os . path . join ( dl_dir_path , \"temp\" ) print ( self . _checkpoints_url , tar_file_path ) utils . download ( self . _checkpoints_url , tar_file_path , ) tar_file = tarfile . open ( tar_file_path ) print ( \"Extracting weights... (this might take a while)\" ) tar_file . extractall ( dl_dir_path ) if not os . path . exists ( self . _latentnet_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"latentnet_ckpts\" ) shutil . move ( src_dir_path , self . _latentnet_ckp_dir_path ) if not os . path . exists ( self . _deepsdf_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"deepsdf_ckpts\" ) shutil . move ( src_dir_path , self . _deepsdf_ckp_dir_path ) if not os . path . exists ( self . _aae_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"aae_ckpts\" ) shutil . move ( src_dir_path , self . _aae_ckp_dir_path ) # normalize names for category_str in self . _category_strs : for aae_category_dir in os . listdir ( self . _aae_ckp_dir_path ): if category_str in aae_category_dir : os . rename ( aae_category_dir , os . path . join ( self . _aae_ckp_dir_path , category_str ), ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset \"\"\" # prepare data as expected by iCaps functions (same as nocs_real_dataset) color_image = color_image * 255 # see icaps.datasets.nocs_real_dataset l71 depth_image = depth_image . unsqueeze ( 2 ) # (...)nocs_real_dataset l79 instance_mask = instance_mask . float () # (...)nocs_real_dataset l100 intrinsics = torch . eye ( 3 ) fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) intrinsics [ 0 , 0 ] = fx intrinsics [ 1 , 1 ] = fy intrinsics [ 0 , 2 ] = cx intrinsics [ 1 , 2 ] = cy x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () bbox = [ y1 , y2 , x1 , x2 ] # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset pose_rbpf = self . _pose_rbpfs [ category_str ] pose_rbpf . reset () # like init but without loading models self . _pose_rbpfs [ category_str ] . set_target_obj ( category_str ) pose_rbpf . data_intrinsics = intrinsics . numpy () pose_rbpf . intrinsics = intrinsics . numpy () pose_rbpf . target_obj_cfg . PF . FU = pose_rbpf . intrinsics [ 0 , 0 ] pose_rbpf . target_obj_cfg . PF . FV = pose_rbpf . intrinsics [ 1 , 1 ] pose_rbpf . target_obj_cfg . PF . U0 = pose_rbpf . intrinsics [ 0 , 2 ] pose_rbpf . target_obj_cfg . PF . V0 = pose_rbpf . intrinsics [ 1 , 2 ] pose_rbpf . data_with_est_center = False pose_rbpf . data_with_gt = False # should this be False now? pose_rbpf . mask_raw = instance_mask [:, :] . cpu () . numpy () pose_rbpf . mask = scipy . ndimage . binary_erosion ( pose_rbpf . mask_raw , iterations = 2 ) . astype ( pose_rbpf . mask_raw . dtype ) pose_rbpf . prior_uv [ 0 ] = ( bbox [ 2 ] + bbox [ 3 ]) / 2 pose_rbpf . prior_uv [ 1 ] = ( bbox [ 0 ] + bbox [ 1 ]) / 2 # what is this ?? if pose_rbpf . aae_full . angle_diff . shape [ 0 ] != 0 : pose_rbpf . aae_full . angle_diff = np . array ([]) if pose_rbpf . target_obj_cfg . PF . USE_DEPTH : depth_data = depth_image else : depth_data = None try : pose_rbpf . initialize_poserbpf ( color_image , pose_rbpf . data_intrinsics , pose_rbpf . prior_uv [: 2 ], pose_rbpf . target_obj_cfg . PF . N_INIT , scale_prior = pose_rbpf . target_obj_cfg . PF . SCALE_PRIOR , depth = depth_data , ) pose_rbpf . process_poserbpf ( color_image , intrinsics . unsqueeze ( 0 ), depth = depth_data , run_deep_sdf = False , ) # 3 * 50 iters by default pose_rbpf . refine_pose_and_shape ( depth_data , intrinsics . unsqueeze ( 0 )) position_cv = torch . tensor ( pose_rbpf . rbpf . trans_bar ) orientation_q = torch . Tensor ( Rotation . from_matrix ( pose_rbpf . rbpf . rot_bar ) . as_quat () ) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) orientation_cv = orientation_q extents = torch . tensor ([ 0.5 , 0.5 , 0.5 ]) mesh_dir_path = tempfile . mkdtemp () mesh_file_path = os . path . join ( mesh_dir_path , \"mesh.ply\" ) point_set = pose_rbpf . evaluator . latent_vec_to_points ( pose_rbpf . latent_vec_refine , N = 64 , num_points = self . _num_points , silent = True , fname = mesh_file_path , ) if point_set is None : point_set = torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]) # failed / no isosurface reconstructed_mesh = None else : scale = pose_rbpf . size_est / pose_rbpf . ratio point_set *= scale reconstructed_mesh = o3d . io . read_triangle_mesh ( mesh_file_path ) reconstructed_mesh . scale ( scale . item (), np . array ([ 0 , 0 , 0 ])) reconstructed_points = torch . tensor ( point_set ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents . detach () . cpu (), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : reconstructed_mesh , } except : print ( \"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\" ) return { \"position\" : torch . tensor ([ 0.0 , 0.0 , 0.0 ]), \"orientation\" : torch . tensor ([ 0.0 , 0.0 , 0.0 , 1.0 ]), \"extents\" : torch . tensor ([ 0.5 , 0.5 , 0.5 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]), \"reconstructed_mesh\" : None , # TODO if time } Config Bases: TypedDict Configuration dictionary for iCaps. ATTRIBUTE DESCRIPTION pf_config_dir Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. TYPE: str deepsdf_checkpoint_dir Directory containing DeepSDF checkpoints. TYPE: str latentnet_checkpoint_dir Directory containing LatentNet checkpoints. TYPE: str aae_checkpoint_dir Directory containing auto-encoder checkpoints. TYPE: str checkpoints_url URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). TYPE: str categories List of category strings. Each category requires corresponding directories in each checkpoint dir. TYPE: List [ str ] Source code in cpas_toolbox/cpas_methods/icaps.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Config ( TypedDict ): \"\"\"Configuration dictionary for iCaps. Attributes: pf_config_dir: Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints. latentnet_checkpoint_dir: Directory containing LatentNet checkpoints. aae_checkpoint_dir: Directory containing auto-encoder checkpoints. checkpoints_url: URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). categories: List of category strings. Each category requires corresponding directories in each checkpoint dir. \"\"\" pf_config_dir : str deepsdf_checkpoint_dir : str latentnet_checkpoint_dir : str aae_checkpoint_dir : str checkpoints_url : str categories : List [ str ] __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load ICaps models. PARAMETER DESCRIPTION config iCaps configuration. See ICaps.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/icaps.py 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ICaps models. Args: config: iCaps configuration. See ICaps.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ICaps . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset Source code in cpas_toolbox/cpas_methods/icaps.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset \"\"\" # prepare data as expected by iCaps functions (same as nocs_real_dataset) color_image = color_image * 255 # see icaps.datasets.nocs_real_dataset l71 depth_image = depth_image . unsqueeze ( 2 ) # (...)nocs_real_dataset l79 instance_mask = instance_mask . float () # (...)nocs_real_dataset l100 intrinsics = torch . eye ( 3 ) fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) intrinsics [ 0 , 0 ] = fx intrinsics [ 1 , 1 ] = fy intrinsics [ 0 , 2 ] = cx intrinsics [ 1 , 2 ] = cy x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () bbox = [ y1 , y2 , x1 , x2 ] # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset pose_rbpf = self . _pose_rbpfs [ category_str ] pose_rbpf . reset () # like init but without loading models self . _pose_rbpfs [ category_str ] . set_target_obj ( category_str ) pose_rbpf . data_intrinsics = intrinsics . numpy () pose_rbpf . intrinsics = intrinsics . numpy () pose_rbpf . target_obj_cfg . PF . FU = pose_rbpf . intrinsics [ 0 , 0 ] pose_rbpf . target_obj_cfg . PF . FV = pose_rbpf . intrinsics [ 1 , 1 ] pose_rbpf . target_obj_cfg . PF . U0 = pose_rbpf . intrinsics [ 0 , 2 ] pose_rbpf . target_obj_cfg . PF . V0 = pose_rbpf . intrinsics [ 1 , 2 ] pose_rbpf . data_with_est_center = False pose_rbpf . data_with_gt = False # should this be False now? pose_rbpf . mask_raw = instance_mask [:, :] . cpu () . numpy () pose_rbpf . mask = scipy . ndimage . binary_erosion ( pose_rbpf . mask_raw , iterations = 2 ) . astype ( pose_rbpf . mask_raw . dtype ) pose_rbpf . prior_uv [ 0 ] = ( bbox [ 2 ] + bbox [ 3 ]) / 2 pose_rbpf . prior_uv [ 1 ] = ( bbox [ 0 ] + bbox [ 1 ]) / 2 # what is this ?? if pose_rbpf . aae_full . angle_diff . shape [ 0 ] != 0 : pose_rbpf . aae_full . angle_diff = np . array ([]) if pose_rbpf . target_obj_cfg . PF . USE_DEPTH : depth_data = depth_image else : depth_data = None try : pose_rbpf . initialize_poserbpf ( color_image , pose_rbpf . data_intrinsics , pose_rbpf . prior_uv [: 2 ], pose_rbpf . target_obj_cfg . PF . N_INIT , scale_prior = pose_rbpf . target_obj_cfg . PF . SCALE_PRIOR , depth = depth_data , ) pose_rbpf . process_poserbpf ( color_image , intrinsics . unsqueeze ( 0 ), depth = depth_data , run_deep_sdf = False , ) # 3 * 50 iters by default pose_rbpf . refine_pose_and_shape ( depth_data , intrinsics . unsqueeze ( 0 )) position_cv = torch . tensor ( pose_rbpf . rbpf . trans_bar ) orientation_q = torch . Tensor ( Rotation . from_matrix ( pose_rbpf . rbpf . rot_bar ) . as_quat () ) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) orientation_cv = orientation_q extents = torch . tensor ([ 0.5 , 0.5 , 0.5 ]) mesh_dir_path = tempfile . mkdtemp () mesh_file_path = os . path . join ( mesh_dir_path , \"mesh.ply\" ) point_set = pose_rbpf . evaluator . latent_vec_to_points ( pose_rbpf . latent_vec_refine , N = 64 , num_points = self . _num_points , silent = True , fname = mesh_file_path , ) if point_set is None : point_set = torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]) # failed / no isosurface reconstructed_mesh = None else : scale = pose_rbpf . size_est / pose_rbpf . ratio point_set *= scale reconstructed_mesh = o3d . io . read_triangle_mesh ( mesh_file_path ) reconstructed_mesh . scale ( scale . item (), np . array ([ 0 , 0 , 0 ])) reconstructed_points = torch . tensor ( point_set ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents . detach () . cpu (), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : reconstructed_mesh , } except : print ( \"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\" ) return { \"position\" : torch . tensor ([ 0.0 , 0.0 , 0.0 ]), \"orientation\" : torch . tensor ([ 0.0 , 0.0 , 0.0 , 1.0 ]), \"extents\" : torch . tensor ([ 0.5 , 0.5 , 0.5 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]), \"reconstructed_mesh\" : None , # TODO if time }","title":"icaps.py"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps","text":"This module defines iCaps interface. Method is described in iCaps Iterative Category-Level Object Pose and Shape Estimation, Deng, 2022. Implementation based on https://github.com/aerogjy/iCaps","title":"icaps"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps","text":"Bases: CPASMethod Wrapper class for iCaps. Source code in cpas_toolbox/cpas_methods/icaps.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 class ICaps ( CPASMethod ): \"\"\"Wrapper class for iCaps.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for iCaps. Attributes: pf_config_dir: Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints. latentnet_checkpoint_dir: Directory containing LatentNet checkpoints. aae_checkpoint_dir: Directory containing auto-encoder checkpoints. checkpoints_url: URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). categories: List of category strings. Each category requires corresponding directories in each checkpoint dir. \"\"\" pf_config_dir : str deepsdf_checkpoint_dir : str latentnet_checkpoint_dir : str aae_checkpoint_dir : str checkpoints_url : str categories : List [ str ] default_config : Config = { \"pf_config_dir\" : None , \"deepsdf_checkpoint_dir\" : None , \"latentnet_checkpoint_dir\" : None , \"aae_checkpoint_dir\" : None , \"checkpoints_url\" : None , \"categories\" : [ \"bottle\" , \"bowl\" , \"camera\" , \"can\" , \"laptop\" , \"mug\" ], } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ICaps models. Args: config: iCaps configuration. See ICaps.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ICaps . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _num_points = 10000 self . _category_strs = config [ \"categories\" ] self . _checkpoints_url = config [ \"checkpoints_url\" ] self . _pose_rbpfs = {} pf_cfg_dir_path = utils . resolve_path ( config [ \"pf_config_dir\" ], search_paths = [ \".\" , \"~/.cpas_toolbox\" , os . path . join ( os . path . dirname ( __file__ ), \"config\" ), os . path . dirname ( __file__ ), ], ) self . _deepsdf_ckp_dir_path = utils . resolve_path ( config [ \"deepsdf_checkpoint_dir\" ] ) self . _latentnet_ckp_dir_path = utils . resolve_path ( config [ \"latentnet_checkpoint_dir\" ] ) self . _aae_ckp_dir_path = utils . resolve_path ( config [ \"aae_checkpoint_dir\" ]) self . _check_paths () for category_str in self . _category_strs : full_ckpt_dir_path = os . path . join ( self . _aae_ckp_dir_path , category_str ) train_cfg_file = os . path . join ( full_ckpt_dir_path , \"config.yml\" ) icaps . icaps_config . cfg_from_file ( train_cfg_file ) test_cfg_file = os . path . join ( pf_cfg_dir_path , category_str + \".yml\" ) icaps . icaps_config . cfg_from_file ( test_cfg_file ) obj_list = icaps . icaps_config . cfg . TEST . OBJECTS cfg_list = [] cfg_list . append ( copy . deepcopy ( icaps . icaps_config . cfg )) self . _pose_rbpfs [ category_str ] = icaps . PoseRBPF ( obj_list , cfg_list , full_ckpt_dir_path , self . _deepsdf_ckp_dir_path , self . _latentnet_ckp_dir_path , ) self . _pose_rbpfs [ category_str ] . set_target_obj ( icaps . icaps_config . cfg . TEST . OBJECTS [ 0 ] ) def _check_paths ( self ) -> None : path_exists = ( os . path . exists ( p ) for p in [ self . _aae_ckp_dir_path , self . _deepsdf_ckp_dir_path , self . _latentnet_ckp_dir_path , ] ) if not all ( path_exists ): print ( \"iCaps model weights not found, do you want to download to \" ) print ( \" \" , self . _aae_ckp_dir_path ) print ( \" \" , self . _deepsdf_ckp_dir_path ) print ( \" \" , self . _latentnet_ckp_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"iCaps model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : dl_dir_path = tempfile . mkdtemp () tar_file_path = os . path . join ( dl_dir_path , \"temp\" ) print ( self . _checkpoints_url , tar_file_path ) utils . download ( self . _checkpoints_url , tar_file_path , ) tar_file = tarfile . open ( tar_file_path ) print ( \"Extracting weights... (this might take a while)\" ) tar_file . extractall ( dl_dir_path ) if not os . path . exists ( self . _latentnet_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"latentnet_ckpts\" ) shutil . move ( src_dir_path , self . _latentnet_ckp_dir_path ) if not os . path . exists ( self . _deepsdf_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"deepsdf_ckpts\" ) shutil . move ( src_dir_path , self . _deepsdf_ckp_dir_path ) if not os . path . exists ( self . _aae_ckp_dir_path ): src_dir_path = os . path . join ( dl_dir_path , \"checkpoints\" , \"aae_ckpts\" ) shutil . move ( src_dir_path , self . _aae_ckp_dir_path ) # normalize names for category_str in self . _category_strs : for aae_category_dir in os . listdir ( self . _aae_ckp_dir_path ): if category_str in aae_category_dir : os . rename ( aae_category_dir , os . path . join ( self . _aae_ckp_dir_path , category_str ), ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset \"\"\" # prepare data as expected by iCaps functions (same as nocs_real_dataset) color_image = color_image * 255 # see icaps.datasets.nocs_real_dataset l71 depth_image = depth_image . unsqueeze ( 2 ) # (...)nocs_real_dataset l79 instance_mask = instance_mask . float () # (...)nocs_real_dataset l100 intrinsics = torch . eye ( 3 ) fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) intrinsics [ 0 , 0 ] = fx intrinsics [ 1 , 1 ] = fy intrinsics [ 0 , 2 ] = cx intrinsics [ 1 , 2 ] = cy x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () bbox = [ y1 , y2 , x1 , x2 ] # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset pose_rbpf = self . _pose_rbpfs [ category_str ] pose_rbpf . reset () # like init but without loading models self . _pose_rbpfs [ category_str ] . set_target_obj ( category_str ) pose_rbpf . data_intrinsics = intrinsics . numpy () pose_rbpf . intrinsics = intrinsics . numpy () pose_rbpf . target_obj_cfg . PF . FU = pose_rbpf . intrinsics [ 0 , 0 ] pose_rbpf . target_obj_cfg . PF . FV = pose_rbpf . intrinsics [ 1 , 1 ] pose_rbpf . target_obj_cfg . PF . U0 = pose_rbpf . intrinsics [ 0 , 2 ] pose_rbpf . target_obj_cfg . PF . V0 = pose_rbpf . intrinsics [ 1 , 2 ] pose_rbpf . data_with_est_center = False pose_rbpf . data_with_gt = False # should this be False now? pose_rbpf . mask_raw = instance_mask [:, :] . cpu () . numpy () pose_rbpf . mask = scipy . ndimage . binary_erosion ( pose_rbpf . mask_raw , iterations = 2 ) . astype ( pose_rbpf . mask_raw . dtype ) pose_rbpf . prior_uv [ 0 ] = ( bbox [ 2 ] + bbox [ 3 ]) / 2 pose_rbpf . prior_uv [ 1 ] = ( bbox [ 0 ] + bbox [ 1 ]) / 2 # what is this ?? if pose_rbpf . aae_full . angle_diff . shape [ 0 ] != 0 : pose_rbpf . aae_full . angle_diff = np . array ([]) if pose_rbpf . target_obj_cfg . PF . USE_DEPTH : depth_data = depth_image else : depth_data = None try : pose_rbpf . initialize_poserbpf ( color_image , pose_rbpf . data_intrinsics , pose_rbpf . prior_uv [: 2 ], pose_rbpf . target_obj_cfg . PF . N_INIT , scale_prior = pose_rbpf . target_obj_cfg . PF . SCALE_PRIOR , depth = depth_data , ) pose_rbpf . process_poserbpf ( color_image , intrinsics . unsqueeze ( 0 ), depth = depth_data , run_deep_sdf = False , ) # 3 * 50 iters by default pose_rbpf . refine_pose_and_shape ( depth_data , intrinsics . unsqueeze ( 0 )) position_cv = torch . tensor ( pose_rbpf . rbpf . trans_bar ) orientation_q = torch . Tensor ( Rotation . from_matrix ( pose_rbpf . rbpf . rot_bar ) . as_quat () ) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) orientation_cv = orientation_q extents = torch . tensor ([ 0.5 , 0.5 , 0.5 ]) mesh_dir_path = tempfile . mkdtemp () mesh_file_path = os . path . join ( mesh_dir_path , \"mesh.ply\" ) point_set = pose_rbpf . evaluator . latent_vec_to_points ( pose_rbpf . latent_vec_refine , N = 64 , num_points = self . _num_points , silent = True , fname = mesh_file_path , ) if point_set is None : point_set = torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]) # failed / no isosurface reconstructed_mesh = None else : scale = pose_rbpf . size_est / pose_rbpf . ratio point_set *= scale reconstructed_mesh = o3d . io . read_triangle_mesh ( mesh_file_path ) reconstructed_mesh . scale ( scale . item (), np . array ([ 0 , 0 , 0 ])) reconstructed_points = torch . tensor ( point_set ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents . detach () . cpu (), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : reconstructed_mesh , } except : print ( \"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\" ) return { \"position\" : torch . tensor ([ 0.0 , 0.0 , 0.0 ]), \"orientation\" : torch . tensor ([ 0.0 , 0.0 , 0.0 , 1.0 ]), \"extents\" : torch . tensor ([ 0.5 , 0.5 , 0.5 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]), \"reconstructed_mesh\" : None , # TODO if time }","title":"ICaps"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.Config","text":"Bases: TypedDict Configuration dictionary for iCaps. ATTRIBUTE DESCRIPTION pf_config_dir Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. TYPE: str deepsdf_checkpoint_dir Directory containing DeepSDF checkpoints. TYPE: str latentnet_checkpoint_dir Directory containing LatentNet checkpoints. TYPE: str aae_checkpoint_dir Directory containing auto-encoder checkpoints. TYPE: str checkpoints_url URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). TYPE: str categories List of category strings. Each category requires corresponding directories in each checkpoint dir. TYPE: List [ str ] Source code in cpas_toolbox/cpas_methods/icaps.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Config ( TypedDict ): \"\"\"Configuration dictionary for iCaps. Attributes: pf_config_dir: Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str. deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints. latentnet_checkpoint_dir: Directory containing LatentNet checkpoints. aae_checkpoint_dir: Directory containing auto-encoder checkpoints. checkpoints_url: URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file). categories: List of category strings. Each category requires corresponding directories in each checkpoint dir. \"\"\" pf_config_dir : str deepsdf_checkpoint_dir : str latentnet_checkpoint_dir : str aae_checkpoint_dir : str checkpoints_url : str categories : List [ str ]","title":"Config"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load ICaps models. PARAMETER DESCRIPTION config iCaps configuration. See ICaps.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/icaps.py 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load ICaps models. Args: config: iCaps configuration. See ICaps.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = ICaps . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset Source code in cpas_toolbox/cpas_methods/icaps.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset \"\"\" # prepare data as expected by iCaps functions (same as nocs_real_dataset) color_image = color_image * 255 # see icaps.datasets.nocs_real_dataset l71 depth_image = depth_image . unsqueeze ( 2 ) # (...)nocs_real_dataset l79 instance_mask = instance_mask . float () # (...)nocs_real_dataset l100 intrinsics = torch . eye ( 3 ) fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) intrinsics [ 0 , 0 ] = fx intrinsics [ 1 , 1 ] = fy intrinsics [ 0 , 2 ] = cx intrinsics [ 1 , 2 ] = cy x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () bbox = [ y1 , y2 , x1 , x2 ] # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset pose_rbpf = self . _pose_rbpfs [ category_str ] pose_rbpf . reset () # like init but without loading models self . _pose_rbpfs [ category_str ] . set_target_obj ( category_str ) pose_rbpf . data_intrinsics = intrinsics . numpy () pose_rbpf . intrinsics = intrinsics . numpy () pose_rbpf . target_obj_cfg . PF . FU = pose_rbpf . intrinsics [ 0 , 0 ] pose_rbpf . target_obj_cfg . PF . FV = pose_rbpf . intrinsics [ 1 , 1 ] pose_rbpf . target_obj_cfg . PF . U0 = pose_rbpf . intrinsics [ 0 , 2 ] pose_rbpf . target_obj_cfg . PF . V0 = pose_rbpf . intrinsics [ 1 , 2 ] pose_rbpf . data_with_est_center = False pose_rbpf . data_with_gt = False # should this be False now? pose_rbpf . mask_raw = instance_mask [:, :] . cpu () . numpy () pose_rbpf . mask = scipy . ndimage . binary_erosion ( pose_rbpf . mask_raw , iterations = 2 ) . astype ( pose_rbpf . mask_raw . dtype ) pose_rbpf . prior_uv [ 0 ] = ( bbox [ 2 ] + bbox [ 3 ]) / 2 pose_rbpf . prior_uv [ 1 ] = ( bbox [ 0 ] + bbox [ 1 ]) / 2 # what is this ?? if pose_rbpf . aae_full . angle_diff . shape [ 0 ] != 0 : pose_rbpf . aae_full . angle_diff = np . array ([]) if pose_rbpf . target_obj_cfg . PF . USE_DEPTH : depth_data = depth_image else : depth_data = None try : pose_rbpf . initialize_poserbpf ( color_image , pose_rbpf . data_intrinsics , pose_rbpf . prior_uv [: 2 ], pose_rbpf . target_obj_cfg . PF . N_INIT , scale_prior = pose_rbpf . target_obj_cfg . PF . SCALE_PRIOR , depth = depth_data , ) pose_rbpf . process_poserbpf ( color_image , intrinsics . unsqueeze ( 0 ), depth = depth_data , run_deep_sdf = False , ) # 3 * 50 iters by default pose_rbpf . refine_pose_and_shape ( depth_data , intrinsics . unsqueeze ( 0 )) position_cv = torch . tensor ( pose_rbpf . rbpf . trans_bar ) orientation_q = torch . Tensor ( Rotation . from_matrix ( pose_rbpf . rbpf . rot_bar ) . as_quat () ) # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ( [ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )] ) # CASS object to ShapeNet object orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) orientation_cv = orientation_q extents = torch . tensor ([ 0.5 , 0.5 , 0.5 ]) mesh_dir_path = tempfile . mkdtemp () mesh_file_path = os . path . join ( mesh_dir_path , \"mesh.ply\" ) point_set = pose_rbpf . evaluator . latent_vec_to_points ( pose_rbpf . latent_vec_refine , N = 64 , num_points = self . _num_points , silent = True , fname = mesh_file_path , ) if point_set is None : point_set = torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]) # failed / no isosurface reconstructed_mesh = None else : scale = pose_rbpf . size_est / pose_rbpf . ratio point_set *= scale reconstructed_mesh = o3d . io . read_triangle_mesh ( mesh_file_path ) reconstructed_mesh . scale ( scale . item (), np . array ([ 0 , 0 , 0 ])) reconstructed_points = torch . tensor ( point_set ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents . detach () . cpu (), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : reconstructed_mesh , } except : print ( \"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\" ) return { \"position\" : torch . tensor ([ 0.0 , 0.0 , 0.0 ]), \"orientation\" : torch . tensor ([ 0.0 , 0.0 , 0.0 , 1.0 ]), \"extents\" : torch . tensor ([ 0.5 , 0.5 , 0.5 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0.0 , 0.0 , 0.0 ]]), \"reconstructed_mesh\" : None , # TODO if time }","title":"inference()"},{"location":"api_reference/cpas_methods/rbppose/","text":"cpas_toolbox.cpas_methods.rbppose This module defines RBPPose interface. Method is described in RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation, Zhang, 2022 Implementation based on https://github.com/lolrudy/RBP_Pose . RBPPose Bases: CPASMethod Wrapper class for RBPPose. Source code in cpas_toolbox/cpas_methods/rbppose.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 class RBPPose ( CPASMethod ): \"\"\"Wrapper class for RBPPose.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for RBPPose. Attributes: model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load RBPPose model. Args: config: RBPPose configuration. See RBPPose.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = RBPPose . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] # Check if files are available and download if not self . _check_paths () # Initialize model self . _net = rbppose . SSPN ( ... ) self . _net = self . _net . to ( self . _device ) state_dict = torch . load ( self . _model_file_path , map_location = self . _device ) cleaned_state_dict = copy . copy ( state_dict ) for key in state_dict . keys (): if \"face_recon\" in key : cleaned_state_dict . pop ( key ) elif \"pcl_encoder_prior\" in key : cleaned_state_dict . pop ( key ) current_model_dict = self . _net . state_dict () current_model_dict . update ( cleaned_state_dict ) self . _net . load_state_dict ( current_model_dict ) self . _net . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"RBPPose model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"RBPPose model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_file_path , ) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # Handle camera information fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( 0 ) width = self . _camera . width height = self . _camera . height camera_matrix = np . array ([[ fx , 0 , cx ], [ 0 , fy , cy ], [ 0 , 0 , 1 ]]) camera_matrix = torch . FloatTensor ( camera_matrix ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare RGB crop (not used by default config) rgb_cv = color_image . numpy ()[:, :, :: - 1 ] # BGR x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = rbppose . get_bbox ([ y1 , x1 , y2 , x2 ]) cx = 0.5 * ( cmin + cmax ) cy = 0.5 * ( rmin + rmax ) bbox_center = np . array ([ cx , cy ]) # (w/2, h/2) scale = min ( max ( cmax - cmin , rmax - rmin ), max ( height , width )) rgb_crop = rbppose . crop_resize_by_warp_affine ( rgb_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) rgb_crop = torch . FloatTensor ( rgb_crop ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare depth crop (expected in mm) depth_cv = depth_image . numpy () * 1000 depth_crop = rbppose . crop_resize_by_warp_affine ( depth_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) depth_crop = torch . FloatTensor ( depth_crop )[ None , None ] . to ( self . _device ) # Prepare category category_input = torch . LongTensor ([ category_id ]) . to ( self . _device ) # Prepare ROI Mask mask_np = instance_mask . float () . numpy () roi_mask = rbppose . crop_resize_by_warp_affine ( mask_np , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) roi_mask = torch . FloatTensor ( roi_mask )[ None , None ] . to ( self . _device ) # Prepare mean shape (size?) mean_shape = rbppose . get_mean_shape ( category_str ) / 1000.0 mean_shape = torch . FloatTensor ( mean_shape ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare shape prior mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] shape_prior = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare 2D coordinates coord_2d = rbppose . get_2d_coord_np ( width , height ) . transpose ( 1 , 2 , 0 ) roi_coord_2d = rbppose . crop_resize_by_warp_affine ( coord_2d , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) roi_coord_2d = torch . FloatTensor ( roi_coord_2d ) . unsqueeze ( 0 ) . to ( self . _device ) output_dict = self . _net ( rgb = rgb_crop , depth = depth_crop , obj_id = category_input , camK = camera_matrix , def_mask = roi_mask , mean_shape = mean_shape , shape_prior = shape_prior , gt_2D = roi_coord_2d , ) p_green_R_vec = output_dict [ \"p_green_R\" ] . detach () . cpu () p_red_R_vec = output_dict [ \"p_red_R\" ] . detach () . cpu () p_T = output_dict [ \"Pred_T\" ] . detach () . cpu () f_green_R = output_dict [ \"f_green_R\" ] . detach () . cpu () f_red_R = output_dict [ \"f_red_R\" ] . detach () . cpu () sym = torch . FloatTensor ( rbppose . get_sym_info ( category_str )) . unsqueeze ( 0 ) pred_RT = rbppose . generate_RT ( [ p_green_R_vec , p_red_R_vec ], [ f_green_R , f_red_R ], p_T , mode = \"vec\" , sym = sym , )[ 0 ] position = output_dict [ \"Pred_T\" ][ 0 ] . detach () . cpu () orientation_mat = pred_RT [: 3 , : 3 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = output_dict [ \"Pred_s\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points = output_dict [ \"recon_model\" ][ 0 ] . detach () . cpu () reconstructed_points *= scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for RBPPose. ATTRIBUTE DESCRIPTION model File path for model weights. TYPE: str model_url URL to download model weights if file is not found. TYPE: str mean_shape File path for mean shape file. TYPE: str mean_shape_url URL to download mean shape file if it is not found. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/rbppose.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Config ( TypedDict ): \"\"\"Configuration dictionary for RBPPose. Attributes: model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str model_url : str mean_shape : str mean_shape_url : str device : str __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load RBPPose model. PARAMETER DESCRIPTION config RBPPose configuration. See RBPPose.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/rbppose.py 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load RBPPose model. Args: config: RBPPose configuration. See RBPPose.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = RBPPose . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py Source code in cpas_toolbox/cpas_methods/rbppose.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # Handle camera information fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( 0 ) width = self . _camera . width height = self . _camera . height camera_matrix = np . array ([[ fx , 0 , cx ], [ 0 , fy , cy ], [ 0 , 0 , 1 ]]) camera_matrix = torch . FloatTensor ( camera_matrix ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare RGB crop (not used by default config) rgb_cv = color_image . numpy ()[:, :, :: - 1 ] # BGR x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = rbppose . get_bbox ([ y1 , x1 , y2 , x2 ]) cx = 0.5 * ( cmin + cmax ) cy = 0.5 * ( rmin + rmax ) bbox_center = np . array ([ cx , cy ]) # (w/2, h/2) scale = min ( max ( cmax - cmin , rmax - rmin ), max ( height , width )) rgb_crop = rbppose . crop_resize_by_warp_affine ( rgb_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) rgb_crop = torch . FloatTensor ( rgb_crop ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare depth crop (expected in mm) depth_cv = depth_image . numpy () * 1000 depth_crop = rbppose . crop_resize_by_warp_affine ( depth_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) depth_crop = torch . FloatTensor ( depth_crop )[ None , None ] . to ( self . _device ) # Prepare category category_input = torch . LongTensor ([ category_id ]) . to ( self . _device ) # Prepare ROI Mask mask_np = instance_mask . float () . numpy () roi_mask = rbppose . crop_resize_by_warp_affine ( mask_np , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) roi_mask = torch . FloatTensor ( roi_mask )[ None , None ] . to ( self . _device ) # Prepare mean shape (size?) mean_shape = rbppose . get_mean_shape ( category_str ) / 1000.0 mean_shape = torch . FloatTensor ( mean_shape ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare shape prior mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] shape_prior = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare 2D coordinates coord_2d = rbppose . get_2d_coord_np ( width , height ) . transpose ( 1 , 2 , 0 ) roi_coord_2d = rbppose . crop_resize_by_warp_affine ( coord_2d , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) roi_coord_2d = torch . FloatTensor ( roi_coord_2d ) . unsqueeze ( 0 ) . to ( self . _device ) output_dict = self . _net ( rgb = rgb_crop , depth = depth_crop , obj_id = category_input , camK = camera_matrix , def_mask = roi_mask , mean_shape = mean_shape , shape_prior = shape_prior , gt_2D = roi_coord_2d , ) p_green_R_vec = output_dict [ \"p_green_R\" ] . detach () . cpu () p_red_R_vec = output_dict [ \"p_red_R\" ] . detach () . cpu () p_T = output_dict [ \"Pred_T\" ] . detach () . cpu () f_green_R = output_dict [ \"f_green_R\" ] . detach () . cpu () f_red_R = output_dict [ \"f_red_R\" ] . detach () . cpu () sym = torch . FloatTensor ( rbppose . get_sym_info ( category_str )) . unsqueeze ( 0 ) pred_RT = rbppose . generate_RT ( [ p_green_R_vec , p_red_R_vec ], [ f_green_R , f_red_R ], p_T , mode = \"vec\" , sym = sym , )[ 0 ] position = output_dict [ \"Pred_T\" ][ 0 ] . detach () . cpu () orientation_mat = pred_RT [: 3 , : 3 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = output_dict [ \"Pred_s\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points = output_dict [ \"recon_model\" ][ 0 ] . detach () . cpu () reconstructed_points *= scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"rbppose.py"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose","text":"This module defines RBPPose interface. Method is described in RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation, Zhang, 2022 Implementation based on https://github.com/lolrudy/RBP_Pose .","title":"rbppose"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose","text":"Bases: CPASMethod Wrapper class for RBPPose. Source code in cpas_toolbox/cpas_methods/rbppose.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 class RBPPose ( CPASMethod ): \"\"\"Wrapper class for RBPPose.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for RBPPose. Attributes: model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load RBPPose model. Args: config: RBPPose configuration. See RBPPose.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = RBPPose . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] # Check if files are available and download if not self . _check_paths () # Initialize model self . _net = rbppose . SSPN ( ... ) self . _net = self . _net . to ( self . _device ) state_dict = torch . load ( self . _model_file_path , map_location = self . _device ) cleaned_state_dict = copy . copy ( state_dict ) for key in state_dict . keys (): if \"face_recon\" in key : cleaned_state_dict . pop ( key ) elif \"pcl_encoder_prior\" in key : cleaned_state_dict . pop ( key ) current_model_dict = self . _net . state_dict () current_model_dict . update ( cleaned_state_dict ) self . _net . load_state_dict ( current_model_dict ) self . _net . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"RBPPose model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"RBPPose model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_file_path , ) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # Handle camera information fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( 0 ) width = self . _camera . width height = self . _camera . height camera_matrix = np . array ([[ fx , 0 , cx ], [ 0 , fy , cy ], [ 0 , 0 , 1 ]]) camera_matrix = torch . FloatTensor ( camera_matrix ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare RGB crop (not used by default config) rgb_cv = color_image . numpy ()[:, :, :: - 1 ] # BGR x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = rbppose . get_bbox ([ y1 , x1 , y2 , x2 ]) cx = 0.5 * ( cmin + cmax ) cy = 0.5 * ( rmin + rmax ) bbox_center = np . array ([ cx , cy ]) # (w/2, h/2) scale = min ( max ( cmax - cmin , rmax - rmin ), max ( height , width )) rgb_crop = rbppose . crop_resize_by_warp_affine ( rgb_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) rgb_crop = torch . FloatTensor ( rgb_crop ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare depth crop (expected in mm) depth_cv = depth_image . numpy () * 1000 depth_crop = rbppose . crop_resize_by_warp_affine ( depth_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) depth_crop = torch . FloatTensor ( depth_crop )[ None , None ] . to ( self . _device ) # Prepare category category_input = torch . LongTensor ([ category_id ]) . to ( self . _device ) # Prepare ROI Mask mask_np = instance_mask . float () . numpy () roi_mask = rbppose . crop_resize_by_warp_affine ( mask_np , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) roi_mask = torch . FloatTensor ( roi_mask )[ None , None ] . to ( self . _device ) # Prepare mean shape (size?) mean_shape = rbppose . get_mean_shape ( category_str ) / 1000.0 mean_shape = torch . FloatTensor ( mean_shape ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare shape prior mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] shape_prior = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare 2D coordinates coord_2d = rbppose . get_2d_coord_np ( width , height ) . transpose ( 1 , 2 , 0 ) roi_coord_2d = rbppose . crop_resize_by_warp_affine ( coord_2d , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) roi_coord_2d = torch . FloatTensor ( roi_coord_2d ) . unsqueeze ( 0 ) . to ( self . _device ) output_dict = self . _net ( rgb = rgb_crop , depth = depth_crop , obj_id = category_input , camK = camera_matrix , def_mask = roi_mask , mean_shape = mean_shape , shape_prior = shape_prior , gt_2D = roi_coord_2d , ) p_green_R_vec = output_dict [ \"p_green_R\" ] . detach () . cpu () p_red_R_vec = output_dict [ \"p_red_R\" ] . detach () . cpu () p_T = output_dict [ \"Pred_T\" ] . detach () . cpu () f_green_R = output_dict [ \"f_green_R\" ] . detach () . cpu () f_red_R = output_dict [ \"f_red_R\" ] . detach () . cpu () sym = torch . FloatTensor ( rbppose . get_sym_info ( category_str )) . unsqueeze ( 0 ) pred_RT = rbppose . generate_RT ( [ p_green_R_vec , p_red_R_vec ], [ f_green_R , f_red_R ], p_T , mode = \"vec\" , sym = sym , )[ 0 ] position = output_dict [ \"Pred_T\" ][ 0 ] . detach () . cpu () orientation_mat = pred_RT [: 3 , : 3 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = output_dict [ \"Pred_s\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points = output_dict [ \"recon_model\" ][ 0 ] . detach () . cpu () reconstructed_points *= scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"RBPPose"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.Config","text":"Bases: TypedDict Configuration dictionary for RBPPose. ATTRIBUTE DESCRIPTION model File path for model weights. TYPE: str model_url URL to download model weights if file is not found. TYPE: str mean_shape File path for mean shape file. TYPE: str mean_shape_url URL to download mean shape file if it is not found. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/rbppose.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Config ( TypedDict ): \"\"\"Configuration dictionary for RBPPose. Attributes: model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str model_url : str mean_shape : str mean_shape_url : str device : str","title":"Config"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load RBPPose model. PARAMETER DESCRIPTION config RBPPose configuration. See RBPPose.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/rbppose.py 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load RBPPose model. Args: config: RBPPose configuration. See RBPPose.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = RBPPose . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py Source code in cpas_toolbox/cpas_methods/rbppose.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] # Handle camera information fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( 0 ) width = self . _camera . width height = self . _camera . height camera_matrix = np . array ([[ fx , 0 , cx ], [ 0 , fy , cy ], [ 0 , 0 , 1 ]]) camera_matrix = torch . FloatTensor ( camera_matrix ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare RGB crop (not used by default config) rgb_cv = color_image . numpy ()[:, :, :: - 1 ] # BGR x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = rbppose . get_bbox ([ y1 , x1 , y2 , x2 ]) cx = 0.5 * ( cmin + cmax ) cy = 0.5 * ( rmin + rmax ) bbox_center = np . array ([ cx , cy ]) # (w/2, h/2) scale = min ( max ( cmax - cmin , rmax - rmin ), max ( height , width )) rgb_crop = rbppose . crop_resize_by_warp_affine ( rgb_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) rgb_crop = torch . FloatTensor ( rgb_crop ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare depth crop (expected in mm) depth_cv = depth_image . numpy () * 1000 depth_crop = rbppose . crop_resize_by_warp_affine ( depth_cv , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) depth_crop = torch . FloatTensor ( depth_crop )[ None , None ] . to ( self . _device ) # Prepare category category_input = torch . LongTensor ([ category_id ]) . to ( self . _device ) # Prepare ROI Mask mask_np = instance_mask . float () . numpy () roi_mask = rbppose . crop_resize_by_warp_affine ( mask_np , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) roi_mask = torch . FloatTensor ( roi_mask )[ None , None ] . to ( self . _device ) # Prepare mean shape (size?) mean_shape = rbppose . get_mean_shape ( category_str ) / 1000.0 mean_shape = torch . FloatTensor ( mean_shape ) . unsqueeze ( 0 ) . to ( self . _device ) # Prepare shape prior mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] shape_prior = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Prepare 2D coordinates coord_2d = rbppose . get_2d_coord_np ( width , height ) . transpose ( 1 , 2 , 0 ) roi_coord_2d = rbppose . crop_resize_by_warp_affine ( coord_2d , bbox_center , scale , rbppose . FLAGS . img_size , interpolation = cv2 . INTER_NEAREST , ) . transpose ( 2 , 0 , 1 ) roi_coord_2d = torch . FloatTensor ( roi_coord_2d ) . unsqueeze ( 0 ) . to ( self . _device ) output_dict = self . _net ( rgb = rgb_crop , depth = depth_crop , obj_id = category_input , camK = camera_matrix , def_mask = roi_mask , mean_shape = mean_shape , shape_prior = shape_prior , gt_2D = roi_coord_2d , ) p_green_R_vec = output_dict [ \"p_green_R\" ] . detach () . cpu () p_red_R_vec = output_dict [ \"p_red_R\" ] . detach () . cpu () p_T = output_dict [ \"Pred_T\" ] . detach () . cpu () f_green_R = output_dict [ \"f_green_R\" ] . detach () . cpu () f_red_R = output_dict [ \"f_red_R\" ] . detach () . cpu () sym = torch . FloatTensor ( rbppose . get_sym_info ( category_str )) . unsqueeze ( 0 ) pred_RT = rbppose . generate_RT ( [ p_green_R_vec , p_red_R_vec ], [ f_green_R , f_red_R ], p_T , mode = \"vec\" , sym = sym , )[ 0 ] position = output_dict [ \"Pred_T\" ][ 0 ] . detach () . cpu () orientation_mat = pred_RT [: 3 , : 3 ] . detach () . cpu () orientation = Rotation . from_matrix ( orientation_mat . numpy ()) orientation_q = torch . FloatTensor ( orientation . as_quat ()) extents = output_dict [ \"Pred_s\" ][ 0 ] . detach () . cpu () scale = torch . linalg . norm ( extents ) reconstructed_points = output_dict [ \"recon_model\" ][ 0 ] . detach () . cpu () reconstructed_points *= scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents = torch . FloatTensor ([ extents [ 2 ], extents [ 1 ], extents [ 0 ]]) return { \"position\" : position , \"orientation\" : orientation_q , \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/sdfest/","text":"cpas_toolbox.cpas_methods.sdfest This module defines SDFEst interface. Method is described in SDFEst: Categorical Pose and Shape Estimation of Objects From RGB-D Using Signed Distance Fields, Bruns, 2022. Implementation based on https://github.com/roym899/sdfest/ SDFEst Bases: CPASMethod Wrapper class for SDFEst. Source code in cpas_toolbox/cpas_methods/sdfest.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class SDFEst ( CPASMethod ): \"\"\"Wrapper class for SDFEst.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. Attributes: sdfest_default_config_file: Default configuration file loaded first. sdfest_category_config_files: Per-category configuration file loaded second. device: Device used for computation. num_points: Numbner of points extracted from mesh. prior: Prior distribution to modify orientation distribution. visualize_optimization: Whether to show additional optimization visualization. \"\"\" default_config : Config = { \"sdfest_default_config_file\" : \"estimation/configs/default.yaml\" , \"sdfest_category_config_files\" : { \"bottle\" : \"estimation/configs/models/bottle.yaml\" , \"bowl\" : \"estimation/configs/models/bowl.yaml\" , \"laptop\" : \"estimation/configs/models/laptop.yaml\" , \"can\" : \"estimation/configs/models/can.yaml\" , \"camera\" : \"estimation/configs/models/camera.yaml\" , \"mug\" : \"estimation/configs/models/mug.yaml\" , }, } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. \"\"\" default_config = yoco . load_config_from_file ( config [ \"sdfest_default_config_file\" ], search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) self . _pipeline_dict = {} # maps category to category-specific pipeline self . _device = config [ \"device\" ] self . _visualize_optimization = config [ \"visualize_optimization\" ] self . _num_points = config [ \"num_points\" ] self . _prior = config [ \"prior\" ] if \"prior\" in config else None # create per-categry models for category_str in config [ \"sdfest_category_config_files\" ] . keys (): category_config = yoco . load_config_from_file ( config [ \"sdfest_category_config_files\" ][ category_str ], current_dict = default_config , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) category_config = yoco . load_config ( config , category_config ) self . _pipeline_dict [ category_str ] = SDFPipeline ( category_config ) self . _pipeline_dict [ category_str ] . cam = camera def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference.\"\"\" # skip unsupported category if category_str not in self . _pipeline_dict : return { \"position\" : torch . tensor ([ 0 , 0 , 0 ]), \"orientation\" : torch . tensor ([ 0 , 0 , 0 , 1 ]), \"extents\" : torch . tensor ([ 1 , 1 , 1 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0 , 0 , 0 ]]), \"reconstructed_mesh\" : None , } pipeline = self . _pipeline_dict [ category_str ] # move inputs to device color_image = color_image . to ( self . _device ) depth_image = depth_image . to ( self . _device , copy = True ) instance_mask = instance_mask . to ( self . _device ) if self . _prior is not None : prior = torch . tensor ( self . _prior [ category_str ], device = self . _device ) prior /= torch . sum ( prior ) else : prior = None position , orientation , scale , shape = pipeline ( depth_image , instance_mask , color_image , visualize = self . _visualize_optimization , prior_orientation_distribution = prior , ) # outputs of SDFEst are OpenGL camera, ShapeNet object convention position_cv = pointset_utils . change_position_camera_convention ( position [ 0 ], \"opengl\" , \"opencv\" ) orientation_cv = pointset_utils . change_orientation_camera_convention ( orientation [ 0 ], \"opengl\" , \"opencv\" ) # reconstruction + extent mesh = pipeline . generate_mesh ( shape , scale , True ) . get_transformed_o3d_geometry () reconstructed_points = torch . from_numpy ( np . asarray ( mesh . sample_points_uniformly ( self . _num_points ) . points ) ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : mesh , } Config Bases: TypedDict Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. ATTRIBUTE DESCRIPTION sdfest_default_config_file Default configuration file loaded first. sdfest_category_config_files Per-category configuration file loaded second. device Device used for computation. num_points Numbner of points extracted from mesh. prior Prior distribution to modify orientation distribution. visualize_optimization Whether to show additional optimization visualization. Source code in cpas_toolbox/cpas_methods/sdfest.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class Config ( TypedDict ): \"\"\"Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. Attributes: sdfest_default_config_file: Default configuration file loaded first. sdfest_category_config_files: Per-category configuration file loaded second. device: Device used for computation. num_points: Numbner of points extracted from mesh. prior: Prior distribution to modify orientation distribution. visualize_optimization: Whether to show additional optimization visualization. \"\"\" __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. Source code in cpas_toolbox/cpas_methods/sdfest.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. \"\"\" default_config = yoco . load_config_from_file ( config [ \"sdfest_default_config_file\" ], search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) self . _pipeline_dict = {} # maps category to category-specific pipeline self . _device = config [ \"device\" ] self . _visualize_optimization = config [ \"visualize_optimization\" ] self . _num_points = config [ \"num_points\" ] self . _prior = config [ \"prior\" ] if \"prior\" in config else None # create per-categry models for category_str in config [ \"sdfest_category_config_files\" ] . keys (): category_config = yoco . load_config_from_file ( config [ \"sdfest_category_config_files\" ][ category_str ], current_dict = default_config , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) category_config = yoco . load_config ( config , category_config ) self . _pipeline_dict [ category_str ] = SDFPipeline ( category_config ) self . _pipeline_dict [ category_str ] . cam = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Source code in cpas_toolbox/cpas_methods/sdfest.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference.\"\"\" # skip unsupported category if category_str not in self . _pipeline_dict : return { \"position\" : torch . tensor ([ 0 , 0 , 0 ]), \"orientation\" : torch . tensor ([ 0 , 0 , 0 , 1 ]), \"extents\" : torch . tensor ([ 1 , 1 , 1 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0 , 0 , 0 ]]), \"reconstructed_mesh\" : None , } pipeline = self . _pipeline_dict [ category_str ] # move inputs to device color_image = color_image . to ( self . _device ) depth_image = depth_image . to ( self . _device , copy = True ) instance_mask = instance_mask . to ( self . _device ) if self . _prior is not None : prior = torch . tensor ( self . _prior [ category_str ], device = self . _device ) prior /= torch . sum ( prior ) else : prior = None position , orientation , scale , shape = pipeline ( depth_image , instance_mask , color_image , visualize = self . _visualize_optimization , prior_orientation_distribution = prior , ) # outputs of SDFEst are OpenGL camera, ShapeNet object convention position_cv = pointset_utils . change_position_camera_convention ( position [ 0 ], \"opengl\" , \"opencv\" ) orientation_cv = pointset_utils . change_orientation_camera_convention ( orientation [ 0 ], \"opengl\" , \"opencv\" ) # reconstruction + extent mesh = pipeline . generate_mesh ( shape , scale , True ) . get_transformed_o3d_geometry () reconstructed_points = torch . from_numpy ( np . asarray ( mesh . sample_points_uniformly ( self . _num_points ) . points ) ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : mesh , }","title":"sdfest.py"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest","text":"This module defines SDFEst interface. Method is described in SDFEst: Categorical Pose and Shape Estimation of Objects From RGB-D Using Signed Distance Fields, Bruns, 2022. Implementation based on https://github.com/roym899/sdfest/","title":"sdfest"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst","text":"Bases: CPASMethod Wrapper class for SDFEst. Source code in cpas_toolbox/cpas_methods/sdfest.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class SDFEst ( CPASMethod ): \"\"\"Wrapper class for SDFEst.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. Attributes: sdfest_default_config_file: Default configuration file loaded first. sdfest_category_config_files: Per-category configuration file loaded second. device: Device used for computation. num_points: Numbner of points extracted from mesh. prior: Prior distribution to modify orientation distribution. visualize_optimization: Whether to show additional optimization visualization. \"\"\" default_config : Config = { \"sdfest_default_config_file\" : \"estimation/configs/default.yaml\" , \"sdfest_category_config_files\" : { \"bottle\" : \"estimation/configs/models/bottle.yaml\" , \"bowl\" : \"estimation/configs/models/bowl.yaml\" , \"laptop\" : \"estimation/configs/models/laptop.yaml\" , \"can\" : \"estimation/configs/models/can.yaml\" , \"camera\" : \"estimation/configs/models/camera.yaml\" , \"mug\" : \"estimation/configs/models/mug.yaml\" , }, } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. \"\"\" default_config = yoco . load_config_from_file ( config [ \"sdfest_default_config_file\" ], search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) self . _pipeline_dict = {} # maps category to category-specific pipeline self . _device = config [ \"device\" ] self . _visualize_optimization = config [ \"visualize_optimization\" ] self . _num_points = config [ \"num_points\" ] self . _prior = config [ \"prior\" ] if \"prior\" in config else None # create per-categry models for category_str in config [ \"sdfest_category_config_files\" ] . keys (): category_config = yoco . load_config_from_file ( config [ \"sdfest_category_config_files\" ][ category_str ], current_dict = default_config , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) category_config = yoco . load_config ( config , category_config ) self . _pipeline_dict [ category_str ] = SDFPipeline ( category_config ) self . _pipeline_dict [ category_str ] . cam = camera def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference.\"\"\" # skip unsupported category if category_str not in self . _pipeline_dict : return { \"position\" : torch . tensor ([ 0 , 0 , 0 ]), \"orientation\" : torch . tensor ([ 0 , 0 , 0 , 1 ]), \"extents\" : torch . tensor ([ 1 , 1 , 1 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0 , 0 , 0 ]]), \"reconstructed_mesh\" : None , } pipeline = self . _pipeline_dict [ category_str ] # move inputs to device color_image = color_image . to ( self . _device ) depth_image = depth_image . to ( self . _device , copy = True ) instance_mask = instance_mask . to ( self . _device ) if self . _prior is not None : prior = torch . tensor ( self . _prior [ category_str ], device = self . _device ) prior /= torch . sum ( prior ) else : prior = None position , orientation , scale , shape = pipeline ( depth_image , instance_mask , color_image , visualize = self . _visualize_optimization , prior_orientation_distribution = prior , ) # outputs of SDFEst are OpenGL camera, ShapeNet object convention position_cv = pointset_utils . change_position_camera_convention ( position [ 0 ], \"opengl\" , \"opencv\" ) orientation_cv = pointset_utils . change_orientation_camera_convention ( orientation [ 0 ], \"opengl\" , \"opencv\" ) # reconstruction + extent mesh = pipeline . generate_mesh ( shape , scale , True ) . get_transformed_o3d_geometry () reconstructed_points = torch . from_numpy ( np . asarray ( mesh . sample_points_uniformly ( self . _num_points ) . points ) ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : mesh , }","title":"SDFEst"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.Config","text":"Bases: TypedDict Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. ATTRIBUTE DESCRIPTION sdfest_default_config_file Default configuration file loaded first. sdfest_category_config_files Per-category configuration file loaded second. device Device used for computation. num_points Numbner of points extracted from mesh. prior Prior distribution to modify orientation distribution. visualize_optimization Whether to show additional optimization visualization. Source code in cpas_toolbox/cpas_methods/sdfest.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class Config ( TypedDict ): \"\"\"Configuration dictionary for SDFEst. All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only. The two keys sdfest_..._config_files will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation. Attributes: sdfest_default_config_file: Default configuration file loaded first. sdfest_category_config_files: Per-category configuration file loaded second. device: Device used for computation. num_points: Numbner of points extracted from mesh. prior: Prior distribution to modify orientation distribution. visualize_optimization: Whether to show additional optimization visualization. \"\"\"","title":"Config"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. Source code in cpas_toolbox/cpas_methods/sdfest.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SDFEst models. Configuration loaded in following order sdfest_default_config_file -> sdfest_category_config_files -> all other keys I.e., keys specified directly will take precedence over keys specified in default file. \"\"\" default_config = yoco . load_config_from_file ( config [ \"sdfest_default_config_file\" ], search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) self . _pipeline_dict = {} # maps category to category-specific pipeline self . _device = config [ \"device\" ] self . _visualize_optimization = config [ \"visualize_optimization\" ] self . _num_points = config [ \"num_points\" ] self . _prior = config [ \"prior\" ] if \"prior\" in config else None # create per-categry models for category_str in config [ \"sdfest_category_config_files\" ] . keys (): category_config = yoco . load_config_from_file ( config [ \"sdfest_category_config_files\" ][ category_str ], current_dict = default_config , search_paths = [ \".\" , \"~/.sdfest/\" , sdfest . __path__ [ 0 ]], ) category_config = yoco . load_config ( config , category_config ) self . _pipeline_dict [ category_str ] = SDFPipeline ( category_config ) self . _pipeline_dict [ category_str ] . cam = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See CPASMethod.inference. Source code in cpas_toolbox/cpas_methods/sdfest.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See CPASMethod.inference.\"\"\" # skip unsupported category if category_str not in self . _pipeline_dict : return { \"position\" : torch . tensor ([ 0 , 0 , 0 ]), \"orientation\" : torch . tensor ([ 0 , 0 , 0 , 1 ]), \"extents\" : torch . tensor ([ 1 , 1 , 1 ]), \"reconstructed_pointcloud\" : torch . tensor ([[ 0 , 0 , 0 ]]), \"reconstructed_mesh\" : None , } pipeline = self . _pipeline_dict [ category_str ] # move inputs to device color_image = color_image . to ( self . _device ) depth_image = depth_image . to ( self . _device , copy = True ) instance_mask = instance_mask . to ( self . _device ) if self . _prior is not None : prior = torch . tensor ( self . _prior [ category_str ], device = self . _device ) prior /= torch . sum ( prior ) else : prior = None position , orientation , scale , shape = pipeline ( depth_image , instance_mask , color_image , visualize = self . _visualize_optimization , prior_orientation_distribution = prior , ) # outputs of SDFEst are OpenGL camera, ShapeNet object convention position_cv = pointset_utils . change_position_camera_convention ( position [ 0 ], \"opengl\" , \"opencv\" ) orientation_cv = pointset_utils . change_orientation_camera_convention ( orientation [ 0 ], \"opengl\" , \"opencv\" ) # reconstruction + extent mesh = pipeline . generate_mesh ( shape , scale , True ) . get_transformed_o3d_geometry () reconstructed_points = torch . from_numpy ( np . asarray ( mesh . sample_points_uniformly ( self . _num_points ) . points ) ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : position_cv . detach () . cpu (), \"orientation\" : orientation_cv . detach () . cpu (), \"extents\" : extents , \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : mesh , }","title":"inference()"},{"location":"api_reference/cpas_methods/sgpa/","text":"cpas_toolbox.cpas_methods.sgpa This module defines CR-Net interface. Method is described in SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation, Chen, 2021. Implementation based on https://github.com/ck-kai/SGPA SGPA Bases: CPASMethod Wrapper class for SGPA. Source code in cpas_toolbox/cpas_methods/sgpa.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class SGPA ( CPASMethod ): \"\"\"Wrapper class for SGPA.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SGPA. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of 3D input points. num_structure_points: Number of keypoints used for pose estimation. image_size: Image size consumed by network (crop will be resized to this). model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int num_structure_points : int image_size : int model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"num_structure_points\" : None , \"image_size\" : None , \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SGPA model. Args: config: SGPA configuration. See SGPA.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SGPA . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _sgpa = sgpa . SGPANet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ], num_structure_points = config [ \"num_structure_points\" ], ) self . _sgpa . cuda () self . _sgpa = torch . nn . DataParallel ( self . _sgpa , device_ids = [ self . _device ]) self . _sgpa . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _sgpa . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ) or not os . path . exists ( self . _mean_shape_path ): print ( \"SGPA model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) print ( \" \" , self . _mean_shape_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"SGPA model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_path , ) if not os . path . exists ( self . _mean_shape_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = sgpa . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SGPA _ , assign_matrix , deltas = self . _sgpa ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = sgpa . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for SGPA. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int num_input_points Number of 3D input points. TYPE: int num_structure_points Number of keypoints used for pose estimation. TYPE: int image_size Image size consumed by network (crop will be resized to this). TYPE: int model File path for model weights. TYPE: str model_url URL to download model weights if file is not found. TYPE: str mean_shape File path for mean shape file. TYPE: str mean_shape_url URL to download mean shape file if it is not found. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/sgpa.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Config ( TypedDict ): \"\"\"Configuration dictionary for SGPA. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of 3D input points. num_structure_points: Number of keypoints used for pose estimation. image_size: Image size consumed by network (crop will be resized to this). model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int num_structure_points : int image_size : int model : str model_url : str mean_shape : str mean_shape_url : str device : str __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SGPA model. PARAMETER DESCRIPTION config SGPA configuration. See SGPA.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/sgpa.py 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SGPA model. Args: config: SGPA configuration. See SGPA.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SGPA . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py Source code in cpas_toolbox/cpas_methods/sgpa.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = sgpa . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SGPA _ , assign_matrix , deltas = self . _sgpa ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = sgpa . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"sgpa.py"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa","text":"This module defines CR-Net interface. Method is described in SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation, Chen, 2021. Implementation based on https://github.com/ck-kai/SGPA","title":"sgpa"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA","text":"Bases: CPASMethod Wrapper class for SGPA. Source code in cpas_toolbox/cpas_methods/sgpa.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class SGPA ( CPASMethod ): \"\"\"Wrapper class for SGPA.\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SGPA. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of 3D input points. num_structure_points: Number of keypoints used for pose estimation. image_size: Image size consumed by network (crop will be resized to this). model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int num_structure_points : int image_size : int model : str model_url : str mean_shape : str mean_shape_url : str device : str default_config : Config = { \"model\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"num_structure_points\" : None , \"image_size\" : None , \"model\" : None , \"model_url\" : None , \"mean_shape\" : None , \"mean_shape_url\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SGPA model. Args: config: SGPA configuration. See SGPA.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SGPA . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_path = utils . resolve_path ( config [ \"model\" ]) self . _model_url = config [ \"model_url\" ] self . _mean_shape_path = utils . resolve_path ( config [ \"mean_shape\" ]) self . _mean_shape_url = config [ \"mean_shape_url\" ] self . _check_paths () self . _sgpa = sgpa . SGPANet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ], num_structure_points = config [ \"num_structure_points\" ], ) self . _sgpa . cuda () self . _sgpa = torch . nn . DataParallel ( self . _sgpa , device_ids = [ self . _device ]) self . _sgpa . load_state_dict ( torch . load ( self . _model_path , map_location = self . _device ) ) self . _sgpa . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_path ) or not os . path . exists ( self . _mean_shape_path ): print ( \"SGPA model weights not found, do you want to download to \" ) print ( \" \" , self . _model_path ) print ( \" \" , self . _mean_shape_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"SGPA model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_path ): os . makedirs ( os . path . dirname ( self . _model_path ), exist_ok = True ) utils . download ( self . _model_url , self . _model_path , ) if not os . path . exists ( self . _mean_shape_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_path ), exist_ok = True ) utils . download ( self . _mean_shape_url , self . _mean_shape_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = sgpa . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SGPA _ , assign_matrix , deltas = self . _sgpa ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = sgpa . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"SGPA"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.Config","text":"Bases: TypedDict Configuration dictionary for SGPA. ATTRIBUTE DESCRIPTION model Path to model. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int num_input_points Number of 3D input points. TYPE: int num_structure_points Number of keypoints used for pose estimation. TYPE: int image_size Image size consumed by network (crop will be resized to this). TYPE: int model File path for model weights. TYPE: str model_url URL to download model weights if file is not found. TYPE: str mean_shape File path for mean shape file. TYPE: str mean_shape_url URL to download mean shape file if it is not found. TYPE: str device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/sgpa.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Config ( TypedDict ): \"\"\"Configuration dictionary for SGPA. Attributes: model: Path to model. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of 3D input points. num_structure_points: Number of keypoints used for pose estimation. image_size: Image size consumed by network (crop will be resized to this). model: File path for model weights. model_url: URL to download model weights if file is not found. mean_shape: File path for mean shape file. mean_shape_url: URL to download mean shape file if it is not found. device: Device string for the model. \"\"\" model : str num_categories : int num_shape_points : int num_input_points : int num_structure_points : int image_size : int model : str model_url : str mean_shape : str mean_shape_url : str device : str","title":"Config"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SGPA model. PARAMETER DESCRIPTION config SGPA configuration. See SGPA.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/sgpa.py 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SGPA model. Args: config: SGPA configuration. See SGPA.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SGPA . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py Source code in cpas_toolbox/cpas_methods/sgpa.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference. Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # Get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = sgpa . get_bbox ([ y1 , x1 , y2 , x2 ]) valid_mask = ( depth_image != 0 ) * instance_mask # Prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) # Prepare input points fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) # if len(choose) < 32: # f_sRT[i] = np.identity(4, dtype=float) # f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0) # continue # else: # valid_inst.append(i) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # Move inputs to device and convert to right shape color_input = color_input . unsqueeze ( 0 ) . to ( self . _device ) points = torch . cuda . FloatTensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . cuda . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . FloatTensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SGPA _ , assign_matrix , deltas = self . _sgpa ( points , color_input , point_indices , category_id , mean_shape_pointset , ) # Postprocess output inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = sgpa . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/cpas_methods/spd/","text":"cpas_toolbox.cpas_methods.spd This module defines SPD interface. Method is described in Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation, Tian, 2020. Implementation based on https://github.com/mentian/object-deformnet SPD Bases: CPASMethod Wrapper class for Shape Prior Deformation (SPD). Source code in cpas_toolbox/cpas_methods/spd.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class SPD ( CPASMethod ): \"\"\"Wrapper class for Shape Prior Deformation (SPD).\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SPD. Attributes: model_file: Path to model. mean_shape_file: Path to mean shape file. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of input points. image_size: Size of image crop. device: Device string for the model. \"\"\" model_file : str mean_shape_file : str num_categories : int num_shape_points : int num_input_points : int image_size : int device : str default_config : Config = { \"model_file\" : None , \"mean_shape_file\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"image_size\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SPD model. Args: config: SPD configuration. See SPD.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SPD . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model_file\" ]) self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape_file\" ]) self . _check_paths () self . _spd_net = spd . DeformNet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ] ) self . _spd_net . to ( self . _device ) self . _spd_net . load_state_dict ( torch . load ( self . _model_file_path , map_location = self . _device ) ) self . _spd_net . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"SPD model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"SPD model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) download_dir_path = os . path . dirname ( self . _model_file_path ) zip_path = os . path . join ( download_dir_path , \"temp.zip\" ) utils . download ( \"https://drive.google.com/u/0/uc?id=1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc&\" \"export=download\" , zip_path , ) z = zipfile . ZipFile ( zip_path ) z . extract ( \"deformnet_eval/real/model_50.pth\" , download_dir_path ) z . close () os . remove ( zip_path ) shutil . move ( os . path . join ( download_dir_path , \"deformnet_eval\" , \"real\" , \"model_50.pth\" ), download_dir_path , ) shutil . rmtree ( os . path . join ( download_dir_path , \"deformnet_eval\" )) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( \"https://github.com/mentian/object-deformnet/raw/master/assets/\" \"mean_points_emb.npy\" , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on spd.evaluate. \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = spd . get_bbox ([ y1 , x1 , y2 , x2 ]) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 valid_mask = ( depth_image != 0 ) * instance_mask # prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) color_input = color_input . unsqueeze ( 0 ) # add batch dim # convert depth to pointcloud fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # move inputs to device color_input = color_input . to ( self . _device ) points = torch . Tensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . Tensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SPD network assign_matrix , deltas = self . _spd_net ( points , color_input , point_indices , category_id , mean_shape_pointset ) # Postprocess outputs inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = spd . align . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , } Config Bases: TypedDict Configuration dictionary for SPD. ATTRIBUTE DESCRIPTION model_file Path to model. TYPE: str mean_shape_file Path to mean shape file. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int num_input_points Number of input points. TYPE: int image_size Size of image crop. TYPE: int device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/spd.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Config ( TypedDict ): \"\"\"Configuration dictionary for SPD. Attributes: model_file: Path to model. mean_shape_file: Path to mean shape file. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of input points. image_size: Size of image crop. device: Device string for the model. \"\"\" model_file : str mean_shape_file : str num_categories : int num_shape_points : int num_input_points : int image_size : int device : str __init__ __init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SPD model. PARAMETER DESCRIPTION config SPD configuration. See SPD.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/spd.py 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SPD model. Args: config: SPD configuration. See SPD.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SPD . default_config ) self . _parse_config ( config ) self . _camera = camera inference inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See MethodWrapper.inference. Based on spd.evaluate. Source code in cpas_toolbox/cpas_methods/spd.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on spd.evaluate. \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = spd . get_bbox ([ y1 , x1 , y2 , x2 ]) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 valid_mask = ( depth_image != 0 ) * instance_mask # prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) color_input = color_input . unsqueeze ( 0 ) # add batch dim # convert depth to pointcloud fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # move inputs to device color_input = color_input . to ( self . _device ) points = torch . Tensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . Tensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SPD network assign_matrix , deltas = self . _spd_net ( points , color_input , point_indices , category_id , mean_shape_pointset ) # Postprocess outputs inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = spd . align . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"spd.py"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd","text":"This module defines SPD interface. Method is described in Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation, Tian, 2020. Implementation based on https://github.com/mentian/object-deformnet","title":"spd"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD","text":"Bases: CPASMethod Wrapper class for Shape Prior Deformation (SPD). Source code in cpas_toolbox/cpas_methods/spd.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class SPD ( CPASMethod ): \"\"\"Wrapper class for Shape Prior Deformation (SPD).\"\"\" class Config ( TypedDict ): \"\"\"Configuration dictionary for SPD. Attributes: model_file: Path to model. mean_shape_file: Path to mean shape file. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of input points. image_size: Size of image crop. device: Device string for the model. \"\"\" model_file : str mean_shape_file : str num_categories : int num_shape_points : int num_input_points : int image_size : int device : str default_config : Config = { \"model_file\" : None , \"mean_shape_file\" : None , \"num_categories\" : None , \"num_shape_points\" : None , \"num_input_points\" : None , \"image_size\" : None , \"device\" : \"cuda\" , } def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SPD model. Args: config: SPD configuration. See SPD.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SPD . default_config ) self . _parse_config ( config ) self . _camera = camera def _parse_config ( self , config : Config ) -> None : self . _device = config [ \"device\" ] self . _model_file_path = utils . resolve_path ( config [ \"model_file\" ]) self . _mean_shape_file_path = utils . resolve_path ( config [ \"mean_shape_file\" ]) self . _check_paths () self . _spd_net = spd . DeformNet ( config [ \"num_categories\" ], config [ \"num_shape_points\" ] ) self . _spd_net . to ( self . _device ) self . _spd_net . load_state_dict ( torch . load ( self . _model_file_path , map_location = self . _device ) ) self . _spd_net . eval () self . _mean_shape_pointsets = np . load ( self . _mean_shape_file_path ) self . _num_input_points = config [ \"num_input_points\" ] self . _image_size = config [ \"image_size\" ] def _check_paths ( self ) -> None : if not os . path . exists ( self . _model_file_path ) or not os . path . exists ( self . _mean_shape_file_path ): print ( \"SPD model weights not found, do you want to download to \" ) print ( \" \" , self . _model_file_path ) print ( \" \" , self . _mean_shape_file_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_weights () break elif decision == \"n\" : print ( \"SPD model weights not found. Aborting.\" ) exit ( 0 ) def _download_weights ( self ) -> None : if not os . path . exists ( self . _model_file_path ): os . makedirs ( os . path . dirname ( self . _model_file_path ), exist_ok = True ) download_dir_path = os . path . dirname ( self . _model_file_path ) zip_path = os . path . join ( download_dir_path , \"temp.zip\" ) utils . download ( \"https://drive.google.com/u/0/uc?id=1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc&\" \"export=download\" , zip_path , ) z = zipfile . ZipFile ( zip_path ) z . extract ( \"deformnet_eval/real/model_50.pth\" , download_dir_path ) z . close () os . remove ( zip_path ) shutil . move ( os . path . join ( download_dir_path , \"deformnet_eval\" , \"real\" , \"model_50.pth\" ), download_dir_path , ) shutil . rmtree ( os . path . join ( download_dir_path , \"deformnet_eval\" )) if not os . path . exists ( self . _mean_shape_file_path ): os . makedirs ( os . path . dirname ( self . _mean_shape_file_path ), exist_ok = True ) utils . download ( \"https://github.com/mentian/object-deformnet/raw/master/assets/\" \"mean_points_emb.npy\" , self . _mean_shape_file_path , ) def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on spd.evaluate. \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = spd . get_bbox ([ y1 , x1 , y2 , x2 ]) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 valid_mask = ( depth_image != 0 ) * instance_mask # prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) color_input = color_input . unsqueeze ( 0 ) # add batch dim # convert depth to pointcloud fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # move inputs to device color_input = color_input . to ( self . _device ) points = torch . Tensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . Tensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SPD network assign_matrix , deltas = self . _spd_net ( points , color_input , point_indices , category_id , mean_shape_pointset ) # Postprocess outputs inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = spd . align . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"SPD"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.Config","text":"Bases: TypedDict Configuration dictionary for SPD. ATTRIBUTE DESCRIPTION model_file Path to model. TYPE: str mean_shape_file Path to mean shape file. TYPE: str num_categories Number of categories used by model. TYPE: int num_shape_points Number of points in shape prior. TYPE: int num_input_points Number of input points. TYPE: int image_size Size of image crop. TYPE: int device Device string for the model. TYPE: str Source code in cpas_toolbox/cpas_methods/spd.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Config ( TypedDict ): \"\"\"Configuration dictionary for SPD. Attributes: model_file: Path to model. mean_shape_file: Path to mean shape file. num_categories: Number of categories used by model. num_shape_points: Number of points in shape prior. num_input_points: Number of input points. image_size: Size of image crop. device: Device string for the model. \"\"\" model_file : str mean_shape_file : str num_categories : int num_shape_points : int num_input_points : int image_size : int device : str","title":"Config"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.__init__","text":"__init__ ( config : Config , camera : camera_utils . Camera ) -> None Initialize and load SPD model. PARAMETER DESCRIPTION config SPD configuration. See SPD.Config for more information. TYPE: Config camera Camera used for the input image. TYPE: camera_utils . Camera Source code in cpas_toolbox/cpas_methods/spd.py 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , config : Config , camera : camera_utils . Camera ) -> None : \"\"\"Initialize and load SPD model. Args: config: SPD configuration. See SPD.Config for more information. camera: Camera used for the input image. \"\"\" config = yoco . load_config ( config , current_dict = SPD . default_config ) self . _parse_config ( config ) self . _camera = camera","title":"__init__()"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.inference","text":"inference ( color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict See MethodWrapper.inference. Based on spd.evaluate. Source code in cpas_toolbox/cpas_methods/spd.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def inference ( self , color_image : torch . Tensor , depth_image : torch . Tensor , instance_mask : torch . Tensor , category_str : str , ) -> PredictionDict : \"\"\"See MethodWrapper.inference. Based on spd.evaluate. \"\"\" category_str_to_id = { \"bottle\" : 0 , \"bowl\" : 1 , \"camera\" : 2 , \"can\" : 3 , \"laptop\" : 4 , \"mug\" : 5 , } category_id = category_str_to_id [ category_str ] mean_shape_pointset = self . _mean_shape_pointsets [ category_id ] # get bounding box x1 = min ( instance_mask . nonzero ()[:, 1 ]) . item () y1 = min ( instance_mask . nonzero ()[:, 0 ]) . item () x2 = max ( instance_mask . nonzero ()[:, 1 ]) . item () y2 = max ( instance_mask . nonzero ()[:, 0 ]) . item () rmin , rmax , cmin , cmax = spd . get_bbox ([ y1 , x1 , y2 , x2 ]) bb_mask = torch . zeros_like ( depth_image ) bb_mask [ rmin : rmax , cmin : cmax ] = 1.0 valid_mask = ( depth_image != 0 ) * instance_mask # prepare image crop color_input = color_image [ rmin : rmax , cmin : cmax , :] . numpy () # bb crop color_input = cv2 . resize ( color_input , ( self . _image_size , self . _image_size ), interpolation = cv2 . INTER_LINEAR , ) color_input = TF . normalize ( TF . to_tensor ( color_input ), # (H, W, C) -> (C, H, W), RGB mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ], ) color_input = color_input . unsqueeze ( 0 ) # add batch dim # convert depth to pointcloud fx , fy , cx , cy , _ = self . _camera . get_pinhole_camera_parameters ( pixel_center = 0.0 ) width = self . _camera . width height = self . _camera . height point_indices = valid_mask [ rmin : rmax , cmin : cmax ] . numpy () . flatten () . nonzero ()[ 0 ] xmap = np . array ([[ i for i in range ( width )] for _ in range ( height )]) ymap = np . array ([[ j for _ in range ( width )] for j in range ( height )]) if len ( point_indices ) > self . _num_input_points : # take subset of points if two many depth points point_indices_mask = np . zeros ( len ( point_indices ), dtype = int ) point_indices_mask [: self . _num_input_points ] = 1 np . random . shuffle ( point_indices_mask ) point_indices = point_indices [ point_indices_mask . nonzero ()] else : point_indices = np . pad ( point_indices , ( 0 , self . _num_input_points - len ( point_indices )), \"wrap\" ) # repeat points if not enough depth observation depth_masked = depth_image [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][ :, None ] xmap_masked = xmap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] ymap_masked = ymap [ rmin : rmax , cmin : cmax ] . flatten ()[ point_indices ][:, None ] pt2 = depth_masked . numpy () pt0 = ( xmap_masked - cx ) * pt2 / fx pt1 = ( ymap_masked - cy ) * pt2 / fy points = np . concatenate (( pt0 , pt1 , pt2 ), axis = 1 ) # adjust indices for resizing of color image crop_w = rmax - rmin ratio = self . _image_size / crop_w col_idx = point_indices % crop_w row_idx = point_indices // crop_w point_indices = ( np . floor ( row_idx * ratio ) * self . _image_size + np . floor ( col_idx * ratio ) ) . astype ( np . int64 ) # move inputs to device color_input = color_input . to ( self . _device ) points = torch . Tensor ( points ) . unsqueeze ( 0 ) . to ( self . _device ) point_indices = torch . LongTensor ( point_indices ) . unsqueeze ( 0 ) . to ( self . _device ) category_id = torch . LongTensor ([ category_id ]) . to ( self . _device ) mean_shape_pointset = ( torch . Tensor ( mean_shape_pointset ) . unsqueeze ( 0 ) . to ( self . _device ) ) # Call SPD network assign_matrix , deltas = self . _spd_net ( points , color_input , point_indices , category_id , mean_shape_pointset ) # Postprocess outputs inst_shape = mean_shape_pointset + deltas assign_matrix = torch . softmax ( assign_matrix , dim = 2 ) coords = torch . bmm ( assign_matrix , inst_shape ) # (1, n_pts, 3) point_indices = point_indices [ 0 ] . cpu () . numpy () _ , point_indices = np . unique ( point_indices , return_index = True ) nocs_coords = coords [ 0 , point_indices , :] . detach () . cpu () . numpy () extents = 2 * np . amax ( np . abs ( inst_shape [ 0 ] . detach () . cpu () . numpy ()), axis = 0 ) points = points [ 0 , point_indices , :] . cpu () . numpy () scale , orientation_m , position , _ = spd . align . estimateSimilarityTransform ( nocs_coords , points ) orientation_q = torch . Tensor ( Rotation . from_matrix ( orientation_m ) . as_quat ()) reconstructed_points = inst_shape [ 0 ] . detach () . cpu () * scale # Recenter for mug category if category_str == \"mug\" : # undo mug translation x_offset = ( ( self . _mean_shape_pointsets [ 5 ] . max ( axis = 0 )[ 0 ] + self . _mean_shape_pointsets [ 5 ] . min ( axis = 0 )[ 0 ] ) / 2 * scale ) reconstructed_points [:, 0 ] -= x_offset position += quaternion_utils . quaternion_apply ( orientation_q , torch . FloatTensor ([ x_offset , 0 , 0 ]) ) . numpy () # NOCS Object -> ShapeNet Object convention obj_fix = torch . tensor ([ 0.0 , - 1 / np . sqrt ( 2.0 ), 0.0 , 1 / np . sqrt ( 2.0 )]) orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , obj_fix ) reconstructed_points = quaternion_utils . quaternion_apply ( quaternion_utils . quaternion_invert ( obj_fix ), reconstructed_points , ) extents , _ = reconstructed_points . abs () . max ( dim = 0 ) extents *= 2.0 return { \"position\" : torch . Tensor ( position ), \"orientation\" : orientation_q , \"extents\" : torch . Tensor ( extents ), \"reconstructed_pointcloud\" : reconstructed_points , \"reconstructed_mesh\" : None , }","title":"inference()"},{"location":"api_reference/datasets/nocs_dataset/","text":"cpas_toolbox.datasets.nocs_dataset Module providing dataset class for NOCS datasets (CAMERA / REAL). NOCSDataset Bases: torch . utils . data . Dataset Dataset class for NOCS dataset. CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/cpas_toolbox/... Source code in cpas_toolbox/datasets/nocs_dataset.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 class NOCSDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for NOCS dataset. CAMERA* and REAL* are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format: {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/cpas_toolbox/... \"\"\" num_categories = 7 category_id_to_str = { 0 : \"unknown\" , 1 : \"bottle\" , 2 : \"bowl\" , 3 : \"camera\" , 4 : \"can\" , 5 : \"laptop\" , 6 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"split\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _split = config [ \"split\" ] self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir_path , \"cpas_toolbox\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] def _check_dirs ( self ) -> None : directories = self . _get_dirs () # required directories if all ( os . path . exists ( directory ) for directory in directories ): pass else : print ( f \"NOCS dataset ( { self . _split } split) not found, do you want to download\" \" it into the following directory:\" ) print ( \" \" , self . _root_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_dataset () break elif decision == \"n\" : print ( \"Dataset not found. Aborting.\" ) exit ( 0 ) def _get_dirs ( self ) -> List [ str ]: \"\"\"Return required list of directories for current split.\"\"\" dirs = [] # gts only available for real test and camera val if self . _split in [ \"real_test\" , \"camera_val\" ]: dirs . append ( os . path . join ( self . _root_dir_path , \"gts\" )) dirs . append ( os . path . join ( self . _root_dir_path , \"obj_models\" )) # Fixed object model, need to be downloaded separately if self . _split == \"real_test\" : dirs . append ( os . path . join ( self . _root_dir_path , \"fixed_real_test_obj_models\" )) # full depths for CAMERA if self . _split in [ \"camera_val\" , \"camera_train\" ]: dirs . append ( os . path . join ( self . _root_dir_path , \"camera_full_depths\" )) if self . _split == \"camera_train\" : dirs . append ( os . path . join ( self . _root_dir_path , \"train\" )) elif self . _split == \"camera_val\" : dirs . append ( os . path . join ( self . _root_dir_path , \"val\" )) elif self . _split in [ \"real_train\" , \"real_test\" ]: dirs . append ( os . path . join ( self . _root_dir_path , self . _split )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return dirs def _download_dataset ( self ) -> None : missing_dirs = [ d for d in self . _get_dirs () if not os . path . exists ( d )] for missing_dir in missing_dirs : download_dir , identifier = os . path . split ( missing_dir ) os . makedirs ( download_dir , exist_ok = True ) if identifier == \"gts\" : zip_path = os . path . join ( download_dir , \"gts.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/gts.zip\" , zip_path ) elif identifier == \"obj_models\" : zip_path = os . path . join ( download_dir , \"obj_models.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/obj_models.zip\" , zip_path , ) elif identifier == \"camera_full_depths\" : zip_path = os . path . join ( download_dir , \"camera_full_depths.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip\" , zip_path , ) z = zipfile . ZipFile ( zip_path ) elif identifier == \"train\" : zip_path = os . path . join ( download_dir , \"train.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_train.zip\" , zip_path , ) elif identifier == \"val\" : zip_path = os . path . join ( download_dir , \"val.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip\" , zip_path , ) elif identifier == \"real_train\" : zip_path = os . path . join ( download_dir , \"real_train.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/real_train.zip\" , zip_path , ) elif identifier == \"real_test\" : zip_path = os . path . join ( download_dir , \"real_test.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/real_test.zip\" , zip_path ) elif identifier == \"fixed_real_test_obj_models\" : zip_path = os . path . join ( download_dir , \"fixed_real_test_obj_models.zip\" ) utils . download ( \"https://drive.google.com/u/0/uc?id=1grAWfmWRm4gDmZnLRf9KF7-_eHX\" \"-12BO&export=download\" , zip_path , ) else : raise ValueError ( f \"Downloading dir { missing_dir } unsupported.\" ) z = zipfile . ZipFile ( zip_path ) z . extractall ( download_dir ) z . close () os . remove ( zip_path ) def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample def _preprocess_dataset ( self ) -> None : \"\"\"Create preprocessing files for the current split. One file per sample, which currently means per valid object mask will be created. Preprocessing will be stored on disk to {root_dir}/cpas_toolbox/... This function will not store the preprocessing, so it still has to be loaded afterwards. \"\"\" os . makedirs ( self . _preprocess_path ) self . _fix_obj_models () self . _start_time = time . time () self . _color_paths = self . _get_color_files () Parallel ( n_jobs =- 1 )( ( delayed ( self . _preprocess_color_path )( i , color_path ) for i , color_path in enumerate ( self . _color_paths ) ) ) # store dictionary to map category to files sample_files = self . _get_sample_files () category_str_to_files = { category_str : [] for category_str in NOCSDataset . category_id_to_str . values () } for sample_file in tqdm ( sample_files ): sample_data = pickle . load ( open ( sample_file , \"rb\" )) category_id = sample_data [ \"category_id\" ] category_str = NOCSDataset . category_id_to_str [ category_id ] _ , file_name = os . path . split ( sample_file ) category_str_to_files [ category_str ] . append ( file_name ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"w\" ) as f : json . dump ( dict ( category_str_to_files ), f ) print ( f \"Finished preprocessing for { self . _split } .\" , end = \" \\033 [K \\n \" ) def _fix_obj_models ( self ) -> None : \"\"\"Fix issues with fileextensions. Some png files have jpg extension. This function fixes these models. \"\"\" glob_pattern = os . path . join ( self . _root_dir_path , \"**\" , \"*.jpg\" ) files = glob ( glob_pattern , recursive = True ) for filepath in files : what = imghdr . what ( filepath ) if what == \"png\" : print ( \"Fixing: \" , filepath ) obj_dir_path , problematic_filename = os . path . split ( filepath ) name , _ = problematic_filename . split ( \".\" ) fixed_filename = f \"fixed_ { name } .png\" fixed_filepath = os . path . join ( obj_dir_path , fixed_filename ) mtl_filepath = os . path . join ( obj_dir_path , \"model.mtl\" ) bu_mtl_filepath = os . path . join ( obj_dir_path , \"model.mtl.old\" ) copyfile ( mtl_filepath , bu_mtl_filepath ) copyfile ( filepath , fixed_filepath ) def _update_preprocess_progress ( self , image_id : int ) -> None : current_time = time . time () duration = current_time - self . _start_time imgs_per_sec = image_id / duration if image_id > 10 : remaining_imgs = len ( self . _color_paths ) - image_id remaining_secs = remaining_imgs / imgs_per_sec remaining_time_str = str ( datetime . timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Preprocessing image: { image_id : >10 } / { len ( self . _color_paths ) } \" f \" { image_id / len ( self . _color_paths ) * 100 : >6.2f } %\" # progress percentage f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous ) def _preprocess_color_path ( self , image_id : int , color_path : str ) -> None : counter = 0 self . _update_preprocess_progress ( image_id ) depth_path = self . _depth_path_from_color_path ( color_path ) if not os . path . isfile ( depth_path ): print ( f \"Missing depth file { depth_path } . Skipping.\" , end = \" \\033 [K \\n \" ) return mask_path = self . _mask_path_from_color_path ( color_path ) meta_path = self . _meta_path_from_color_path ( color_path ) meta_data = pd . read_csv ( meta_path , sep = \" \" , header = None , converters = { 2 : lambda x : str ( x )} ) instances_mask = self . _load_mask ( mask_path ) mask_ids = np . unique ( instances_mask ) . tolist () gt_id = 0 # GT only contains valid objects of interests and is 0-indexed for mask_id in mask_ids : if mask_id == 255 : # 255 is background continue match = meta_data [ meta_data . iloc [:, 0 ] == mask_id ] if match . empty : print ( f \"Warning: mask { mask_id } not found in { meta_path } \" , end = \" \\033 [K \\n \" ) elif match . shape [ 0 ] != 1 : print ( f \"Warning: mask { mask_id } not unique in { meta_path } \" , end = \" \\033 [K \\n \" ) meta_row = match . iloc [ 0 ] category_id = meta_row . iloc [ 1 ] if category_id == 0 : # unknown / distractor object continue try : ( position , orientation_q , extents , nocs_transform , ) = self . _get_pose_and_scale ( color_path , mask_id , gt_id , meta_row ) except nocs_utils . PoseEstimationError : print ( \"Insufficient data for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue except ObjectError : print ( \"Insufficient object mesh for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue obj_path = self . _get_obj_path ( meta_row ) sample_info = { \"color_path\" : color_path , \"depth_path\" : depth_path , \"mask_path\" : mask_path , \"mask_id\" : mask_id , \"category_id\" : category_id , \"obj_path\" : obj_path , \"nocs_transform\" : nocs_transform , \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"nocs_scale\" : torch . linalg . norm ( extents ), \"max_extent\" : torch . max ( extents ), } out_file = os . path . join ( self . _preprocess_path , f \" { image_id : 08 } _ { counter } .pkl\" ) pickle . dump ( sample_info , open ( out_file , \"wb\" )) counter += 1 gt_id += 1 def _get_color_files ( self ) -> list : \"\"\"Return list of paths of color images of the selected split.\"\"\" if self . _split == \"camera_train\" : glob_pattern = os . path . join ( self . _root_dir_path , \"train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"camera_val\" : glob_pattern = os . path . join ( self . _root_dir_path , \"val\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_train\" : glob_pattern = os . path . join ( self . _root_dir_path , \"real_train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_test\" : glob_pattern = os . path . join ( self . _root_dir_path , \"real_test\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _get_sample_files ( self , category_str : Optional [ str ] = None ) -> list : \"\"\"Return sorted list of sample file paths. Sample files are generated by NOCSDataset._preprocess_dataset. Args: category_str: If not None, only instances of the provided category will be returned. Returns: List of sample_data files. \"\"\" glob_pattern = os . path . join ( self . _preprocess_path , \"*.pkl\" ) sample_files = glob ( glob_pattern ) sample_files . sort () if category_str is None : return sample_files if category_str not in NOCSDataset . category_str_to_id : raise ValueError ( f \"Unsupported category_str { category_str } .\" ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"r\" ) as f : category_str_to_filenames = json . load ( f ) filtered_sample_files = [ os . path . join ( self . _preprocess_path , fn ) for fn in category_str_to_filenames [ category_str ] ] return filtered_sample_files def _get_split_camera ( self ) -> None : \"\"\"Return camera information for selected split.\"\"\" # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py if self . _split in [ \"real_train\" , \"real_test\" ]: return camera_utils . Camera ( width = 640 , height = 480 , fx = 591.0125 , fy = 590.16775 , cx = 322.525 , cy = 244.11084 , pixel_center = 0.0 , ) elif self . _split in [ \"camera_train\" , \"camera_val\" ]: return camera_utils . Camera ( width = 640 , height = 480 , fx = 577.5 , fy = 577.5 , cx = 319.5 , cy = 239.5 , pixel_center = 0.0 , ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _sample_from_sample_data ( self , sample_data : dict ) -> dict : \"\"\"Create dictionary containing a single sample.\"\"\" color = torch . from_numpy ( np . asarray ( Image . open ( sample_data [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( sample_data [ \"depth_path\" ]) instances_mask = self . _load_mask ( sample_data [ \"mask_path\" ]) instance_mask = instances_mask == sample_data [ \"mask_id\" ] pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( sample_data [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( sample_data [ \"orientation_q\" ], sample_data [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( sample_data , extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : sample_data [ \"color_path\" ], \"obj_path\" : sample_data [ \"obj_path\" ], \"category_id\" : sample_data [ \"category_id\" ], \"category_str\" : NOCSDataset . category_id_to_str [ sample_data [ \"category_id\" ]], } return sample def _depth_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to depth file from color filepath.\"\"\" if self . _split in [ \"real_train\" , \"real_test\" ]: depth_path = color_path . replace ( \"color\" , \"depth\" ) elif self . _split in [ \"camera_train\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/train/\" , \"/camera_full_depths/train/\" ) elif self . _split in [ \"camera_val\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/val/\" , \"/camera_full_depths/val/\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return depth_path def _mask_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to mask file from color filepath.\"\"\" mask_path = color_path . replace ( \"color\" , \"mask\" ) return mask_path def _meta_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to meta file from color filepath.\"\"\" meta_path = color_path . replace ( \"color.png\" , \"meta.txt\" ) return meta_path def _nocs_map_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return NOCS map filepath from color filepath.\"\"\" nocs_map_path = color_path . replace ( \"color.png\" , \"coord.png\" ) return nocs_map_path def _get_pose_and_scale ( self , color_path : str , mask_id : int , gt_id : int , meta_row : pd . Series ) -> tuple : \"\"\"Return position, orientation, scale and NOCS transform. All of those follow OpenCV (x right, y down, z forward) convention. Args: color_path: Path to the color file. mask_id: Instance id in the instances mask. gt_id: Ground truth id. This is 0-indexed id of valid instances in meta file. meta_row: Matching row of meta file. Contains necessary information about mesh. Returns: position (torch.Tensor): Position of object center in camera frame. Shape (3,). quaternion (torch.Tensor): Orientation of object in camera frame. Scalar-last quaternion, shape (4,). extents (torch.Tensor): Bounding box side lengths. nocs_transformation (torch.Tensor): Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera. \"\"\" gts_path = self . _get_gts_path ( color_path ) obj_path = self . _get_obj_path ( meta_row ) if self . _split == \"real_test\" : # only use gt for real test data, since there are errors in camera val gts_data = pickle . load ( open ( gts_path , \"rb\" )) nocs_transform = gts_data [ \"gt_RTs\" ][ gt_id ] position = nocs_transform [ 0 : 3 , 3 ] rot_scale = nocs_transform [ 0 : 3 , 0 : 3 ] nocs_scales = np . sqrt ( np . sum ( rot_scale ** 2 , axis = 0 )) rotation_matrix = rot_scale / nocs_scales [:, None ] nocs_scale = nocs_scales [ 0 ] else : # camera_train, camera_val, real_train # use ground truth NOCS mask to perform alignment ( position , rotation_matrix , nocs_scale , nocs_transform , ) = self . _estimate_object ( color_path , mask_id ) orientation_q = Rotation . from_matrix ( rotation_matrix ) . as_quat () mesh_extents = self . _get_mesh_extents_from_obj ( obj_path ) if \"camera\" in self . _split : # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1 # get metric extents by scaling with the diagonal extents = nocs_scale * mesh_extents elif \"real\" in self . _split : # REAL object meshes are not normalized extents = mesh_extents else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) position = torch . Tensor ( position ) orientation_q = torch . Tensor ( orientation_q ) extents = torch . Tensor ( extents ) nocs_transform = torch . Tensor ( nocs_transform ) return position , orientation_q , extents , nocs_transform def _get_gts_path ( self , color_path : str ) -> Optional [ str ]: \"\"\"Return path to gts file from color filepath. Return None if split does not have ground truth information. \"\"\" if self . _split == \"real_test\" : gts_dir_path = os . path . join ( self . _root_dir_path , \"gts\" , \"real_test\" ) elif self . _split == \"camera_val\" : gts_dir_path = os . path . join ( self . _root_dir_path , \"gts\" , \"val\" ) else : return None path = os . path . normpath ( color_path ) split_path = path . split ( os . sep ) number = path [ - 14 : - 10 ] gts_filename = f \"results_ { split_path [ - 3 ] } _ { split_path [ - 2 ] } _ { number } .pkl\" gts_path = os . path . join ( gts_dir_path , gts_filename ) return gts_path def _get_obj_path ( self , meta_row : pd . Series ) -> str : \"\"\"Return path to object file from meta data row.\"\"\" if \"camera\" in self . _split : # ShapeNet mesh synset_id = meta_row . iloc [ 2 ] object_id = meta_row . iloc [ 3 ] obj_path = os . path . join ( self . _root_dir_path , \"obj_models\" , self . _split . replace ( \"camera_\" , \"\" ), synset_id , object_id , \"model.obj\" , ) elif \"real_test\" in self . _split : # Fixed REAL test meshes object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir_path , \"fixed_real_test_obj_models\" , object_id + \".obj\" , ) elif \"real_train\" in self . _split : # REAL train mesh (not complete) object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir_path , \"obj_models\" , self . _split , object_id + \".obj\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return obj_path def _get_mesh_extents_from_obj ( self , obj_path : str ) -> torch . Tensor : \"\"\"Return maximum extent of bounding box from obj filepath. Note that this is normalized extent (with diagonal == 1) in the case of CAMERA dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset. \"\"\" mesh = o3d . io . read_triangle_mesh ( obj_path ) vertices = np . asarray ( mesh . vertices ) if len ( vertices ) == 0 : raise ObjectError () extents = np . max ( vertices , axis = 0 ) - np . min ( vertices , axis = 0 ) return torch . Tensor ( extents ) def _load_mask ( self , mask_path : str ) -> torch . Tensor : \"\"\"Load mask from mask filepath.\"\"\" mask_img = np . asarray ( Image . open ( mask_path ), dtype = np . uint8 ) if mask_img . ndim == 3 and mask_img . shape [ 2 ] == 4 : # CAMERA masks are RGBA instances_mask = mask_img [:, :, 0 ] # use first channel only else : # REAL masks are grayscale instances_mask = mask_img return torch . from_numpy ( instances_mask ) def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _load_nocs_map ( self , nocs_map_path : str ) -> torch . Tensor : \"\"\"Load NOCS map from NOCS map filepath. Returns: NOCS map where each channel corresponds to one dimension in NOCS. Coordinates are normalized to [0,1], shape (H,W,3). \"\"\" nocs_map = torch . from_numpy ( np . asarray ( Image . open ( nocs_map_path ), dtype = np . float32 ) / 255 ) # z-coordinate has to be flipped # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501 nocs_map [:, :, 2 ] = 1 - nocs_map [:, :, 2 ] return nocs_map [:, :, : 3 ] def _estimate_object ( self , color_path : str , mask_id : int ) -> tuple : \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\" position = rotation_matrix = scale = out_transform = None depth_path = self . _depth_path_from_color_path ( color_path ) depth = self . _load_depth ( depth_path ) mask_path = self . _mask_path_from_color_path ( color_path ) instances_mask = self . _load_mask ( mask_path ) instance_mask = instances_mask == mask_id nocs_map_path = self . _nocs_map_path_from_color_path ( color_path ) nocs_map = self . _load_nocs_map ( nocs_map_path ) valid_instance_mask = instance_mask * depth != 0 nocs_map [ ~ valid_instance_mask ] = 0 centered_nocs_points = nocs_map [ valid_instance_mask ] - 0.5 measured_points = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = valid_instance_mask , convention = \"opencv\" ) # require at least 30 point correspondences to prevent outliers if len ( measured_points ) < 30 : raise nocs_utils . PoseEstimationError () # skip object if it cointains errorneous depth if torch . max ( depth [ valid_instance_mask ]) > 32.0 : print ( \"Erroneous depth detected.\" , end = \" \\033 [K \\n \" ) raise nocs_utils . PoseEstimationError () ( position , rotation_matrix , scale , out_transform , ) = nocs_utils . estimate_similarity_transform ( centered_nocs_points , measured_points , verbose = False ) if position is None : raise nocs_utils . PoseEstimationError () return position , rotation_matrix , scale , out_transform def _get_scale ( self , sample_data : dict , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return sample_data [ \"nocs_scale\" ] elif self . _scale_convention == \"max\" : return sample_data [ \"max_extent\" ] elif self . _scale_convention == \"half_max\" : return 0.5 * sample_data [ \"max_extent\" ] elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convnetion } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh Config Bases: TypedDict Configuration dictionary for NOCSDataset. ATTRIBUTE DESCRIPTION root_dir See NOCSDataset docstring. TYPE: str split The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. Currently only \"quaternion\" supported. TYPE: str remap_y_axis If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in cpas_toolbox/datasets/nocs_dataset.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] __init__ __init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. TYPE: Config Source code in cpas_toolbox/datasets/nocs_dataset.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _split = config [ \"split\" ] self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir_path , \"cpas_toolbox\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] __len__ __len__ () -> int Return number of sample in dataset. Source code in cpas_toolbox/datasets/nocs_dataset.py 285 286 287 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) __getitem__ __getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" Source code in cpas_toolbox/datasets/nocs_dataset.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample load_mesh load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in cpas_toolbox/datasets/nocs_dataset.py 933 934 935 936 937 938 939 940 941 942 943 944 945 946 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh ObjectError Bases: Exception Error if something with the mesh is wrong. Source code in cpas_toolbox/datasets/nocs_dataset.py 949 950 951 952 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"nocs_dataset.py"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset","text":"Module providing dataset class for NOCS datasets (CAMERA / REAL).","title":"nocs_dataset"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset","text":"Bases: torch . utils . data . Dataset Dataset class for NOCS dataset. CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/cpas_toolbox/... Source code in cpas_toolbox/datasets/nocs_dataset.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 class NOCSDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for NOCS dataset. CAMERA* and REAL* are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275. Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master Expected directory format: {root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/... Which is easily obtained by downloading all the provided files and extracting them into the same directory. Necessary preprocessing of this data is performed during first initialization per and is saved to {root_dir}/cpas_toolbox/... \"\"\" num_categories = 7 category_id_to_str = { 0 : \"unknown\" , 1 : \"bottle\" , 2 : \"bowl\" , 3 : \"camera\" , 4 : \"can\" , 5 : \"laptop\" , 6 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"split\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _split = config [ \"split\" ] self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir_path , \"cpas_toolbox\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] def _check_dirs ( self ) -> None : directories = self . _get_dirs () # required directories if all ( os . path . exists ( directory ) for directory in directories ): pass else : print ( f \"NOCS dataset ( { self . _split } split) not found, do you want to download\" \" it into the following directory:\" ) print ( \" \" , self . _root_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_dataset () break elif decision == \"n\" : print ( \"Dataset not found. Aborting.\" ) exit ( 0 ) def _get_dirs ( self ) -> List [ str ]: \"\"\"Return required list of directories for current split.\"\"\" dirs = [] # gts only available for real test and camera val if self . _split in [ \"real_test\" , \"camera_val\" ]: dirs . append ( os . path . join ( self . _root_dir_path , \"gts\" )) dirs . append ( os . path . join ( self . _root_dir_path , \"obj_models\" )) # Fixed object model, need to be downloaded separately if self . _split == \"real_test\" : dirs . append ( os . path . join ( self . _root_dir_path , \"fixed_real_test_obj_models\" )) # full depths for CAMERA if self . _split in [ \"camera_val\" , \"camera_train\" ]: dirs . append ( os . path . join ( self . _root_dir_path , \"camera_full_depths\" )) if self . _split == \"camera_train\" : dirs . append ( os . path . join ( self . _root_dir_path , \"train\" )) elif self . _split == \"camera_val\" : dirs . append ( os . path . join ( self . _root_dir_path , \"val\" )) elif self . _split in [ \"real_train\" , \"real_test\" ]: dirs . append ( os . path . join ( self . _root_dir_path , self . _split )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return dirs def _download_dataset ( self ) -> None : missing_dirs = [ d for d in self . _get_dirs () if not os . path . exists ( d )] for missing_dir in missing_dirs : download_dir , identifier = os . path . split ( missing_dir ) os . makedirs ( download_dir , exist_ok = True ) if identifier == \"gts\" : zip_path = os . path . join ( download_dir , \"gts.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/gts.zip\" , zip_path ) elif identifier == \"obj_models\" : zip_path = os . path . join ( download_dir , \"obj_models.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/obj_models.zip\" , zip_path , ) elif identifier == \"camera_full_depths\" : zip_path = os . path . join ( download_dir , \"camera_full_depths.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip\" , zip_path , ) z = zipfile . ZipFile ( zip_path ) elif identifier == \"train\" : zip_path = os . path . join ( download_dir , \"train.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_train.zip\" , zip_path , ) elif identifier == \"val\" : zip_path = os . path . join ( download_dir , \"val.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip\" , zip_path , ) elif identifier == \"real_train\" : zip_path = os . path . join ( download_dir , \"real_train.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/real_train.zip\" , zip_path , ) elif identifier == \"real_test\" : zip_path = os . path . join ( download_dir , \"real_test.zip\" ) utils . download ( \"http://download.cs.stanford.edu/orion/nocs/real_test.zip\" , zip_path ) elif identifier == \"fixed_real_test_obj_models\" : zip_path = os . path . join ( download_dir , \"fixed_real_test_obj_models.zip\" ) utils . download ( \"https://drive.google.com/u/0/uc?id=1grAWfmWRm4gDmZnLRf9KF7-_eHX\" \"-12BO&export=download\" , zip_path , ) else : raise ValueError ( f \"Downloading dir { missing_dir } unsupported.\" ) z = zipfile . ZipFile ( zip_path ) z . extractall ( download_dir ) z . close () os . remove ( zip_path ) def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample def _preprocess_dataset ( self ) -> None : \"\"\"Create preprocessing files for the current split. One file per sample, which currently means per valid object mask will be created. Preprocessing will be stored on disk to {root_dir}/cpas_toolbox/... This function will not store the preprocessing, so it still has to be loaded afterwards. \"\"\" os . makedirs ( self . _preprocess_path ) self . _fix_obj_models () self . _start_time = time . time () self . _color_paths = self . _get_color_files () Parallel ( n_jobs =- 1 )( ( delayed ( self . _preprocess_color_path )( i , color_path ) for i , color_path in enumerate ( self . _color_paths ) ) ) # store dictionary to map category to files sample_files = self . _get_sample_files () category_str_to_files = { category_str : [] for category_str in NOCSDataset . category_id_to_str . values () } for sample_file in tqdm ( sample_files ): sample_data = pickle . load ( open ( sample_file , \"rb\" )) category_id = sample_data [ \"category_id\" ] category_str = NOCSDataset . category_id_to_str [ category_id ] _ , file_name = os . path . split ( sample_file ) category_str_to_files [ category_str ] . append ( file_name ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"w\" ) as f : json . dump ( dict ( category_str_to_files ), f ) print ( f \"Finished preprocessing for { self . _split } .\" , end = \" \\033 [K \\n \" ) def _fix_obj_models ( self ) -> None : \"\"\"Fix issues with fileextensions. Some png files have jpg extension. This function fixes these models. \"\"\" glob_pattern = os . path . join ( self . _root_dir_path , \"**\" , \"*.jpg\" ) files = glob ( glob_pattern , recursive = True ) for filepath in files : what = imghdr . what ( filepath ) if what == \"png\" : print ( \"Fixing: \" , filepath ) obj_dir_path , problematic_filename = os . path . split ( filepath ) name , _ = problematic_filename . split ( \".\" ) fixed_filename = f \"fixed_ { name } .png\" fixed_filepath = os . path . join ( obj_dir_path , fixed_filename ) mtl_filepath = os . path . join ( obj_dir_path , \"model.mtl\" ) bu_mtl_filepath = os . path . join ( obj_dir_path , \"model.mtl.old\" ) copyfile ( mtl_filepath , bu_mtl_filepath ) copyfile ( filepath , fixed_filepath ) def _update_preprocess_progress ( self , image_id : int ) -> None : current_time = time . time () duration = current_time - self . _start_time imgs_per_sec = image_id / duration if image_id > 10 : remaining_imgs = len ( self . _color_paths ) - image_id remaining_secs = remaining_imgs / imgs_per_sec remaining_time_str = str ( datetime . timedelta ( seconds = round ( remaining_secs ))) else : remaining_time_str = \"N/A\" print ( f \"Preprocessing image: { image_id : >10 } / { len ( self . _color_paths ) } \" f \" { image_id / len ( self . _color_paths ) * 100 : >6.2f } %\" # progress percentage f \" Remaining time: { remaining_time_str } \" # remaining time \" \\033 [K\" , # clear until end of line end = \" \\r \" , # overwrite previous ) def _preprocess_color_path ( self , image_id : int , color_path : str ) -> None : counter = 0 self . _update_preprocess_progress ( image_id ) depth_path = self . _depth_path_from_color_path ( color_path ) if not os . path . isfile ( depth_path ): print ( f \"Missing depth file { depth_path } . Skipping.\" , end = \" \\033 [K \\n \" ) return mask_path = self . _mask_path_from_color_path ( color_path ) meta_path = self . _meta_path_from_color_path ( color_path ) meta_data = pd . read_csv ( meta_path , sep = \" \" , header = None , converters = { 2 : lambda x : str ( x )} ) instances_mask = self . _load_mask ( mask_path ) mask_ids = np . unique ( instances_mask ) . tolist () gt_id = 0 # GT only contains valid objects of interests and is 0-indexed for mask_id in mask_ids : if mask_id == 255 : # 255 is background continue match = meta_data [ meta_data . iloc [:, 0 ] == mask_id ] if match . empty : print ( f \"Warning: mask { mask_id } not found in { meta_path } \" , end = \" \\033 [K \\n \" ) elif match . shape [ 0 ] != 1 : print ( f \"Warning: mask { mask_id } not unique in { meta_path } \" , end = \" \\033 [K \\n \" ) meta_row = match . iloc [ 0 ] category_id = meta_row . iloc [ 1 ] if category_id == 0 : # unknown / distractor object continue try : ( position , orientation_q , extents , nocs_transform , ) = self . _get_pose_and_scale ( color_path , mask_id , gt_id , meta_row ) except nocs_utils . PoseEstimationError : print ( \"Insufficient data for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue except ObjectError : print ( \"Insufficient object mesh for pose estimation. \" f \"Skipping { color_path } : { mask_id } .\" , end = \" \\033 [K \\n \" , ) continue obj_path = self . _get_obj_path ( meta_row ) sample_info = { \"color_path\" : color_path , \"depth_path\" : depth_path , \"mask_path\" : mask_path , \"mask_id\" : mask_id , \"category_id\" : category_id , \"obj_path\" : obj_path , \"nocs_transform\" : nocs_transform , \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"nocs_scale\" : torch . linalg . norm ( extents ), \"max_extent\" : torch . max ( extents ), } out_file = os . path . join ( self . _preprocess_path , f \" { image_id : 08 } _ { counter } .pkl\" ) pickle . dump ( sample_info , open ( out_file , \"wb\" )) counter += 1 gt_id += 1 def _get_color_files ( self ) -> list : \"\"\"Return list of paths of color images of the selected split.\"\"\" if self . _split == \"camera_train\" : glob_pattern = os . path . join ( self . _root_dir_path , \"train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"camera_val\" : glob_pattern = os . path . join ( self . _root_dir_path , \"val\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_train\" : glob_pattern = os . path . join ( self . _root_dir_path , \"real_train\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) elif self . _split == \"real_test\" : glob_pattern = os . path . join ( self . _root_dir_path , \"real_test\" , \"**\" , \"*_color.png\" ) return sorted ( glob ( glob_pattern , recursive = True )) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _get_sample_files ( self , category_str : Optional [ str ] = None ) -> list : \"\"\"Return sorted list of sample file paths. Sample files are generated by NOCSDataset._preprocess_dataset. Args: category_str: If not None, only instances of the provided category will be returned. Returns: List of sample_data files. \"\"\" glob_pattern = os . path . join ( self . _preprocess_path , \"*.pkl\" ) sample_files = glob ( glob_pattern ) sample_files . sort () if category_str is None : return sample_files if category_str not in NOCSDataset . category_str_to_id : raise ValueError ( f \"Unsupported category_str { category_str } .\" ) category_json_path = os . path . join ( self . _preprocess_path , \"categories.json\" ) with open ( category_json_path , \"r\" ) as f : category_str_to_filenames = json . load ( f ) filtered_sample_files = [ os . path . join ( self . _preprocess_path , fn ) for fn in category_str_to_filenames [ category_str ] ] return filtered_sample_files def _get_split_camera ( self ) -> None : \"\"\"Return camera information for selected split.\"\"\" # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py if self . _split in [ \"real_train\" , \"real_test\" ]: return camera_utils . Camera ( width = 640 , height = 480 , fx = 591.0125 , fy = 590.16775 , cx = 322.525 , cy = 244.11084 , pixel_center = 0.0 , ) elif self . _split in [ \"camera_train\" , \"camera_val\" ]: return camera_utils . Camera ( width = 640 , height = 480 , fx = 577.5 , fy = 577.5 , cx = 319.5 , cy = 239.5 , pixel_center = 0.0 , ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) def _sample_from_sample_data ( self , sample_data : dict ) -> dict : \"\"\"Create dictionary containing a single sample.\"\"\" color = torch . from_numpy ( np . asarray ( Image . open ( sample_data [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( sample_data [ \"depth_path\" ]) instances_mask = self . _load_mask ( sample_data [ \"mask_path\" ]) instance_mask = instances_mask == sample_data [ \"mask_id\" ] pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( sample_data [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( sample_data [ \"orientation_q\" ], sample_data [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( sample_data , extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : sample_data [ \"color_path\" ], \"obj_path\" : sample_data [ \"obj_path\" ], \"category_id\" : sample_data [ \"category_id\" ], \"category_str\" : NOCSDataset . category_id_to_str [ sample_data [ \"category_id\" ]], } return sample def _depth_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to depth file from color filepath.\"\"\" if self . _split in [ \"real_train\" , \"real_test\" ]: depth_path = color_path . replace ( \"color\" , \"depth\" ) elif self . _split in [ \"camera_train\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/train/\" , \"/camera_full_depths/train/\" ) elif self . _split in [ \"camera_val\" ]: depth_path = color_path . replace ( \"color\" , \"composed\" ) depth_path = depth_path . replace ( \"/val/\" , \"/camera_full_depths/val/\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return depth_path def _mask_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to mask file from color filepath.\"\"\" mask_path = color_path . replace ( \"color\" , \"mask\" ) return mask_path def _meta_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return path to meta file from color filepath.\"\"\" meta_path = color_path . replace ( \"color.png\" , \"meta.txt\" ) return meta_path def _nocs_map_path_from_color_path ( self , color_path : str ) -> str : \"\"\"Return NOCS map filepath from color filepath.\"\"\" nocs_map_path = color_path . replace ( \"color.png\" , \"coord.png\" ) return nocs_map_path def _get_pose_and_scale ( self , color_path : str , mask_id : int , gt_id : int , meta_row : pd . Series ) -> tuple : \"\"\"Return position, orientation, scale and NOCS transform. All of those follow OpenCV (x right, y down, z forward) convention. Args: color_path: Path to the color file. mask_id: Instance id in the instances mask. gt_id: Ground truth id. This is 0-indexed id of valid instances in meta file. meta_row: Matching row of meta file. Contains necessary information about mesh. Returns: position (torch.Tensor): Position of object center in camera frame. Shape (3,). quaternion (torch.Tensor): Orientation of object in camera frame. Scalar-last quaternion, shape (4,). extents (torch.Tensor): Bounding box side lengths. nocs_transformation (torch.Tensor): Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera. \"\"\" gts_path = self . _get_gts_path ( color_path ) obj_path = self . _get_obj_path ( meta_row ) if self . _split == \"real_test\" : # only use gt for real test data, since there are errors in camera val gts_data = pickle . load ( open ( gts_path , \"rb\" )) nocs_transform = gts_data [ \"gt_RTs\" ][ gt_id ] position = nocs_transform [ 0 : 3 , 3 ] rot_scale = nocs_transform [ 0 : 3 , 0 : 3 ] nocs_scales = np . sqrt ( np . sum ( rot_scale ** 2 , axis = 0 )) rotation_matrix = rot_scale / nocs_scales [:, None ] nocs_scale = nocs_scales [ 0 ] else : # camera_train, camera_val, real_train # use ground truth NOCS mask to perform alignment ( position , rotation_matrix , nocs_scale , nocs_transform , ) = self . _estimate_object ( color_path , mask_id ) orientation_q = Rotation . from_matrix ( rotation_matrix ) . as_quat () mesh_extents = self . _get_mesh_extents_from_obj ( obj_path ) if \"camera\" in self . _split : # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1 # get metric extents by scaling with the diagonal extents = nocs_scale * mesh_extents elif \"real\" in self . _split : # REAL object meshes are not normalized extents = mesh_extents else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) position = torch . Tensor ( position ) orientation_q = torch . Tensor ( orientation_q ) extents = torch . Tensor ( extents ) nocs_transform = torch . Tensor ( nocs_transform ) return position , orientation_q , extents , nocs_transform def _get_gts_path ( self , color_path : str ) -> Optional [ str ]: \"\"\"Return path to gts file from color filepath. Return None if split does not have ground truth information. \"\"\" if self . _split == \"real_test\" : gts_dir_path = os . path . join ( self . _root_dir_path , \"gts\" , \"real_test\" ) elif self . _split == \"camera_val\" : gts_dir_path = os . path . join ( self . _root_dir_path , \"gts\" , \"val\" ) else : return None path = os . path . normpath ( color_path ) split_path = path . split ( os . sep ) number = path [ - 14 : - 10 ] gts_filename = f \"results_ { split_path [ - 3 ] } _ { split_path [ - 2 ] } _ { number } .pkl\" gts_path = os . path . join ( gts_dir_path , gts_filename ) return gts_path def _get_obj_path ( self , meta_row : pd . Series ) -> str : \"\"\"Return path to object file from meta data row.\"\"\" if \"camera\" in self . _split : # ShapeNet mesh synset_id = meta_row . iloc [ 2 ] object_id = meta_row . iloc [ 3 ] obj_path = os . path . join ( self . _root_dir_path , \"obj_models\" , self . _split . replace ( \"camera_\" , \"\" ), synset_id , object_id , \"model.obj\" , ) elif \"real_test\" in self . _split : # Fixed REAL test meshes object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir_path , \"fixed_real_test_obj_models\" , object_id + \".obj\" , ) elif \"real_train\" in self . _split : # REAL train mesh (not complete) object_id = meta_row . iloc [ 2 ] obj_path = os . path . join ( self . _root_dir_path , \"obj_models\" , self . _split , object_id + \".obj\" ) else : raise ValueError ( f \"Specified split { self . _split } is not supported.\" ) return obj_path def _get_mesh_extents_from_obj ( self , obj_path : str ) -> torch . Tensor : \"\"\"Return maximum extent of bounding box from obj filepath. Note that this is normalized extent (with diagonal == 1) in the case of CAMERA dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset. \"\"\" mesh = o3d . io . read_triangle_mesh ( obj_path ) vertices = np . asarray ( mesh . vertices ) if len ( vertices ) == 0 : raise ObjectError () extents = np . max ( vertices , axis = 0 ) - np . min ( vertices , axis = 0 ) return torch . Tensor ( extents ) def _load_mask ( self , mask_path : str ) -> torch . Tensor : \"\"\"Load mask from mask filepath.\"\"\" mask_img = np . asarray ( Image . open ( mask_path ), dtype = np . uint8 ) if mask_img . ndim == 3 and mask_img . shape [ 2 ] == 4 : # CAMERA masks are RGBA instances_mask = mask_img [:, :, 0 ] # use first channel only else : # REAL masks are grayscale instances_mask = mask_img return torch . from_numpy ( instances_mask ) def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _load_nocs_map ( self , nocs_map_path : str ) -> torch . Tensor : \"\"\"Load NOCS map from NOCS map filepath. Returns: NOCS map where each channel corresponds to one dimension in NOCS. Coordinates are normalized to [0,1], shape (H,W,3). \"\"\" nocs_map = torch . from_numpy ( np . asarray ( Image . open ( nocs_map_path ), dtype = np . float32 ) / 255 ) # z-coordinate has to be flipped # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501 nocs_map [:, :, 2 ] = 1 - nocs_map [:, :, 2 ] return nocs_map [:, :, : 3 ] def _estimate_object ( self , color_path : str , mask_id : int ) -> tuple : \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\" position = rotation_matrix = scale = out_transform = None depth_path = self . _depth_path_from_color_path ( color_path ) depth = self . _load_depth ( depth_path ) mask_path = self . _mask_path_from_color_path ( color_path ) instances_mask = self . _load_mask ( mask_path ) instance_mask = instances_mask == mask_id nocs_map_path = self . _nocs_map_path_from_color_path ( color_path ) nocs_map = self . _load_nocs_map ( nocs_map_path ) valid_instance_mask = instance_mask * depth != 0 nocs_map [ ~ valid_instance_mask ] = 0 centered_nocs_points = nocs_map [ valid_instance_mask ] - 0.5 measured_points = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = valid_instance_mask , convention = \"opencv\" ) # require at least 30 point correspondences to prevent outliers if len ( measured_points ) < 30 : raise nocs_utils . PoseEstimationError () # skip object if it cointains errorneous depth if torch . max ( depth [ valid_instance_mask ]) > 32.0 : print ( \"Erroneous depth detected.\" , end = \" \\033 [K \\n \" ) raise nocs_utils . PoseEstimationError () ( position , rotation_matrix , scale , out_transform , ) = nocs_utils . estimate_similarity_transform ( centered_nocs_points , measured_points , verbose = False ) if position is None : raise nocs_utils . PoseEstimationError () return position , rotation_matrix , scale , out_transform def _get_scale ( self , sample_data : dict , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return sample_data [ \"nocs_scale\" ] elif self . _scale_convention == \"max\" : return sample_data [ \"max_extent\" ] elif self . _scale_convention == \"half_max\" : return 0.5 * sample_data [ \"max_extent\" ] elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convnetion } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"NOCSDataset"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.Config","text":"Bases: TypedDict Configuration dictionary for NOCSDataset. ATTRIBUTE DESCRIPTION root_dir See NOCSDataset docstring. TYPE: str split The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. Currently only \"quaternion\" supported. TYPE: str remap_y_axis If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in cpas_toolbox/datasets/nocs_dataset.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for NOCSDataset. Attributes: root_dir: See NOCSDataset docstring. split: The dataset split. The following strings are supported: \"camera_train\": 275000 images, synthetic objects + real background \"camera_val\": 25000 images, synthetic objects + real background \"real_train\": 4300 images in 7 scenes, real \"real_test\": 2750 images in 6 scenes, real mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ]","title":"Config"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__init__","text":"__init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. TYPE: Config Source code in cpas_toolbox/datasets/nocs_dataset.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys. \"\"\" config = yoco . load_config ( config , current_dict = NOCSDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _split = config [ \"split\" ] self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _camera = self . _get_split_camera () self . _preprocess_path = os . path . join ( self . _root_dir_path , \"cpas_toolbox\" , self . _split ) if not os . path . isdir ( self . _preprocess_path ): self . _preprocess_dataset () self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _sample_files = self . _get_sample_files ( config [ \"category_str\" ]) self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ]","title":"__init__()"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__len__","text":"__len__ () -> int Return number of sample in dataset. Source code in cpas_toolbox/datasets/nocs_dataset.py 285 286 287 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _sample_files )","title":"__len__()"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__getitem__","text":"__getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" Source code in cpas_toolbox/datasets/nocs_dataset.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"\"\" sample_file = self . _sample_files [ idx ] sample_data = pickle . load ( open ( sample_file , \"rb\" )) sample = self . _sample_from_sample_data ( sample_data ) return sample","title":"__getitem__()"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.load_mesh","text":"load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in cpas_toolbox/datasets/nocs_dataset.py 933 934 935 936 937 938 939 940 941 942 943 944 945 946 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"load_mesh()"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.ObjectError","text":"Bases: Exception Error if something with the mesh is wrong. Source code in cpas_toolbox/datasets/nocs_dataset.py 949 950 951 952 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"ObjectError"},{"location":"api_reference/datasets/nocs_utils/","text":"cpas_toolbox.datasets.nocs_utils Module for utility function related to NOCS dataset. This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets. Aligning code by Srinath Sridhar https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py PoseEstimationError Bases: Exception Error if pose estimation encountered an error. Source code in cpas_toolbox/datasets/nocs_utils.py 246 247 248 249 class PoseEstimationError ( Exception ): \"\"\"Error if pose estimation encountered an error.\"\"\" pass estimate_similarity_transform estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. PARAMETER DESCRIPTION source Source points that will be transformed, shape (N,3). TYPE: np . ndarray target Target points to which source will be aligned to, shape (N,3). TYPE: np . ndarray verbose If true additional information will be printed. TYPE: bool DEFAULT: False RETURNS DESCRIPTION position Translation to translate source to target, shape (3,). TYPE: np . ndarray rotation_matrix Rotation to rotate source to target, shape (3,3). TYPE: np . ndarray scale Scaling factor along each axis, to scale source to target. TYPE: float transform Homogeneous transformation matrix, shape (4,4). TYPE: np . ndarray Source code in cpas_toolbox/datasets/nocs_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple : \"\"\"Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. Args: source: Source points that will be transformed, shape (N,3). target: Target points to which source will be aligned to, shape (N,3). verbose: If true additional information will be printed. Returns: position (np.ndarray): Translation to translate source to target, shape (3,). rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3). scale (float): Scaling factor along each axis, to scale source to target. transform (np.ndarray): Homogeneous transformation matrix, shape (4,4). \"\"\" if len ( source ) < 5 or len ( target ) < 5 : print ( \"Pose estimation failed. Not enough point correspondences: \" , len ( source )) return None , None , None , None # make points homogeneous source_hom = np . transpose ( np . hstack ([ source , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N target_hom = np . transpose ( np . hstack ([ target , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N # Auto-parameter selection based on source-target heuristics target_norm = np . mean ( np . linalg . norm ( target , axis = 1 )) # mean distance from origin source_norm = np . mean ( np . linalg . norm ( source , axis = 1 )) ratio_ts = target_norm / source_norm ratio_st = source_norm / target_norm pass_t = ratio_st if ( ratio_st > ratio_ts ) else ratio_ts pass_t *= 0.01 # tighter bound stop_t = pass_t / 100 n_iter = 100 if verbose : print ( \"Pass threshold: \" , pass_t ) print ( \"Stop threshold: \" , stop_t ) print ( \"Number of iterations: \" , n_iter ) source_inliers_hom , target_inliers_hom , best_inlier_ratio = _get_ransac_inliers ( source_hom , target_hom , max_iterations = n_iter , pass_threshold = pass_t , stop_threshold = stop_t , ) if best_inlier_ratio < 0.1 : print ( \"Pose estimation failed. Small inlier ratio: \" , best_inlier_ratio ) return None , None , None , None scales , rotation_matrix , position , out_transform = _estimate_similarity_umeyama ( source_inliers_hom , target_inliers_hom ) scale = scales [ 0 ] if verbose : print ( \"BestInlierRatio:\" , best_inlier_ratio ) print ( \"Rotation: \\n \" , rotation_matrix ) print ( \"Position: \\n \" , position ) print ( \"Scales:\" , scales ) return position , rotation_matrix , scale , out_transform","title":"nocs_utils.py"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils","text":"Module for utility function related to NOCS dataset. This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets. Aligning code by Srinath Sridhar https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py","title":"nocs_utils"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.PoseEstimationError","text":"Bases: Exception Error if pose estimation encountered an error. Source code in cpas_toolbox/datasets/nocs_utils.py 246 247 248 249 class PoseEstimationError ( Exception ): \"\"\"Error if pose estimation encountered an error.\"\"\" pass","title":"PoseEstimationError"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.estimate_similarity_transform","text":"estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. PARAMETER DESCRIPTION source Source points that will be transformed, shape (N,3). TYPE: np . ndarray target Target points to which source will be aligned to, shape (N,3). TYPE: np . ndarray verbose If true additional information will be printed. TYPE: bool DEFAULT: False RETURNS DESCRIPTION position Translation to translate source to target, shape (3,). TYPE: np . ndarray rotation_matrix Rotation to rotate source to target, shape (3,3). TYPE: np . ndarray scale Scaling factor along each axis, to scale source to target. TYPE: float transform Homogeneous transformation matrix, shape (4,4). TYPE: np . ndarray Source code in cpas_toolbox/datasets/nocs_utils.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def estimate_similarity_transform ( source : np . ndarray , target : np . ndarray , verbose : bool = False ) -> tuple : \"\"\"Estimate similarity transform from source to target from point correspondences. Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation. A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points. Note that the returned values fulfill the following equations transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side. Args: source: Source points that will be transformed, shape (N,3). target: Target points to which source will be aligned to, shape (N,3). verbose: If true additional information will be printed. Returns: position (np.ndarray): Translation to translate source to target, shape (3,). rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3). scale (float): Scaling factor along each axis, to scale source to target. transform (np.ndarray): Homogeneous transformation matrix, shape (4,4). \"\"\" if len ( source ) < 5 or len ( target ) < 5 : print ( \"Pose estimation failed. Not enough point correspondences: \" , len ( source )) return None , None , None , None # make points homogeneous source_hom = np . transpose ( np . hstack ([ source , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N target_hom = np . transpose ( np . hstack ([ target , np . ones ([ source . shape [ 0 ], 1 ])])) # 4,N # Auto-parameter selection based on source-target heuristics target_norm = np . mean ( np . linalg . norm ( target , axis = 1 )) # mean distance from origin source_norm = np . mean ( np . linalg . norm ( source , axis = 1 )) ratio_ts = target_norm / source_norm ratio_st = source_norm / target_norm pass_t = ratio_st if ( ratio_st > ratio_ts ) else ratio_ts pass_t *= 0.01 # tighter bound stop_t = pass_t / 100 n_iter = 100 if verbose : print ( \"Pass threshold: \" , pass_t ) print ( \"Stop threshold: \" , stop_t ) print ( \"Number of iterations: \" , n_iter ) source_inliers_hom , target_inliers_hom , best_inlier_ratio = _get_ransac_inliers ( source_hom , target_hom , max_iterations = n_iter , pass_threshold = pass_t , stop_threshold = stop_t , ) if best_inlier_ratio < 0.1 : print ( \"Pose estimation failed. Small inlier ratio: \" , best_inlier_ratio ) return None , None , None , None scales , rotation_matrix , position , out_transform = _estimate_similarity_umeyama ( source_inliers_hom , target_inliers_hom ) scale = scales [ 0 ] if verbose : print ( \"BestInlierRatio:\" , best_inlier_ratio ) print ( \"Rotation: \\n \" , rotation_matrix ) print ( \"Position: \\n \" , position ) print ( \"Scales:\" , scales ) return position , rotation_matrix , scale , out_transform","title":"estimate_similarity_transform()"},{"location":"api_reference/datasets/redwood_dataset/","text":"cpas_toolbox.datasets.redwood_dataset Module providing dataset class for annotated Redwood dataset. AnnotatedRedwoodDataset Bases: torch . utils . data . Dataset Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are of repo. Expected directory format {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json Source code in cpas_toolbox/datasets/redwood_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class AnnotatedRedwoodDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are of repo. Expected directory format: {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json \"\"\" num_categories = 3 category_id_to_str = { 0 : \"bottle\" , 1 : \"bowl\" , 2 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"ann_dir\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _ann_dir_path = utils . resolve_path ( config [ \"ann_dir\" ]) self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] self . _load_annotations () self . _camera = camera_utils . Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) def _check_dirs ( self ) -> None : if os . path . exists ( self . _root_dir_path ) and os . path . exists ( self . _ann_dir_path ): pass else : print ( \"REDWOOD75 dataset not found, do you want to download it into the \" \"following directories:\" ) print ( \" \" , self . _root_dir_path ) print ( \" \" , self . _ann_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_dataset () break elif decision == \"n\" : print ( \"Dataset not found. Aborting.\" ) exit ( 0 ) def _download_dataset ( self ) -> None : # Download anns if not os . path . exists ( self . _ann_dir_path ): zip_path = os . path . join ( self . _ann_dir_path , \"redwood75.zip\" ) os . makedirs ( self . _ann_dir_path , exist_ok = True ) url = ( \"https://drive.google.com/u/0/uc?id=1PMvIblsXWDxEJykVwhUk_QEjy4_bmDU\" \"-&export=download\" ) utils . download ( url , zip_path ) z = zipfile . ZipFile ( zip_path ) z . extractall ( os . path . join ( self . _ann_dir_path , \"..\" )) z . close () os . remove ( zip_path ) ann_json = os . path . join ( self . _ann_dir_path , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) baseurl = \"https://s3.us-west-1.wasabisys.com/redwood-3dscan/rgbd/\" for seq_id in anns_dict . keys (): download_dir_path = os . path . join ( self . _root_dir_path , anns_dict [ seq_id ][ \"category\" ] ) os . makedirs ( download_dir_path , exist_ok = True ) zip_path = os . path . join ( download_dir_path , f \" { seq_id } .zip\" ) os . makedirs ( os . path . dirname ( zip_path ), exist_ok = True ) utils . download ( baseurl + f \" { seq_id } .zip\" , zip_path ) z = zipfile . ZipFile ( zip_path ) seq_dir_path = os . path . join ( download_dir_path , \"rgbd\" , seq_id ) os . makedirs ( seq_dir_path , exist_ok = True ) z . extractall ( seq_dir_path ) z . close () os . remove ( zip_path ) def _load_annotations ( self ) -> None : \"\"\"Load annotations into memory.\"\"\" ann_json = os . path . join ( self . _ann_dir_path , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) self . _raw_samples = [] for seq_id , seq_anns in anns_dict . items (): for pose_ann in seq_anns [ \"pose_anns\" ]: self . _raw_samples . append ( self . _create_raw_sample ( seq_id , seq_anns , pose_ann ) ) def _create_raw_sample ( self , seq_id : str , sequence_dict : dict , annotation_dict : dict ) -> dict : \"\"\"Create raw sample from information in annotations file.\"\"\" position = torch . tensor ( annotation_dict [ \"position\" ]) orientation_q = torch . tensor ( annotation_dict [ \"orientation\" ]) rgb_filename = annotation_dict [ \"rgb_file\" ] depth_filename = annotation_dict [ \"depth_file\" ] mesh_filename = sequence_dict [ \"mesh\" ] mesh_path = os . path . join ( self . _ann_dir_path , mesh_filename ) category_str = sequence_dict [ \"category\" ] color_path = os . path . join ( self . _root_dir_path , category_str , \"rgbd\" , seq_id , \"rgb\" , rgb_filename ) depth_path = os . path . join ( self . _root_dir_path , category_str , \"rgbd\" , seq_id , \"depth\" , depth_filename ) extents = torch . tensor ( sequence_dict [ \"scale\" ]) * 2 return { \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"color_path\" : color_path , \"depth_path\" : depth_path , \"mesh_path\" : mesh_path , \"category_str\" : category_str , } def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample def _compute_mask ( self , depth : torch . Tensor , raw_sample : dict ) -> torch . Tensor : posed_mesh = o3d . io . read_triangle_mesh ( raw_sample [ \"mesh_path\" ]) R = Rotation . from_quat ( raw_sample [ \"orientation_q\" ]) . as_matrix () posed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) posed_mesh . translate ( raw_sample [ \"position\" ]) posed_mesh . compute_vertex_normals () gt_depth = torch . from_numpy ( _draw_depth_geometry ( posed_mesh , self . _camera )) mask = gt_depth != 0 # exclude occluded parts from mask mask [( depth != 0 ) * ( depth < gt_depth - 0.01 )] = 0 return mask def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _get_scale ( self , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return torch . linalg . norm ( extents ) elif self . _scale_convention == \"max\" : return extents . max () elif self . _scale_convention == \"half_max\" : return 0.5 * extents . max () elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convention } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh Config Bases: TypedDict Configuration dictionary for annoated Redwood dataset. ATTRIBUTE DESCRIPTION root_dir See AnnotatedRedwoodDataset docstring. TYPE: str ann_dir See AnnotatedRedwoodDataset docstring. TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. Currently only \"quaternion\" supported. TYPE: str remap_y_axis If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in cpas_toolbox/datasets/redwood_dataset.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] __init__ __init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. TYPE: Config Source code in cpas_toolbox/datasets/redwood_dataset.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _ann_dir_path = utils . resolve_path ( config [ \"ann_dir\" ]) self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] self . _load_annotations () self . _camera = camera_utils . Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) __len__ __len__ () -> int Return number of sample in dataset. Source code in cpas_toolbox/datasets/redwood_dataset.py 240 241 242 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) __getitem__ __getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" Source code in cpas_toolbox/datasets/redwood_dataset.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample load_mesh load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in cpas_toolbox/datasets/redwood_dataset.py 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh ObjectError Bases: Exception Error if something with the mesh is wrong. Source code in cpas_toolbox/datasets/redwood_dataset.py 459 460 461 462 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"redwood_dataset.py"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset","text":"Module providing dataset class for annotated Redwood dataset.","title":"redwood_dataset"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset","text":"Bases: torch . utils . data . Dataset Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are of repo. Expected directory format {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json Source code in cpas_toolbox/datasets/redwood_dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class AnnotatedRedwoodDataset ( torch . utils . data . Dataset ): \"\"\"Dataset class for annotated Redwood dataset. Data can be found here: http://redwood-data.org/3dscan/index.html Annotations are of repo. Expected directory format: {root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json \"\"\" num_categories = 3 category_id_to_str = { 0 : \"bottle\" , 1 : \"bowl\" , 2 : \"mug\" , } category_str_to_id = { v : k for k , v in category_id_to_str . items ()} class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ] default_config : Config = { \"root_dir\" : None , \"ann_dir\" : None , \"mask_pointcloud\" : False , \"normalize_pointcloud\" : False , \"camera_convention\" : \"opengl\" , \"scale_convention\" : \"half_max\" , \"orientation_repr\" : \"quaternion\" , \"orientation_grid_resolution\" : None , \"category_str\" : None , \"remap_y_axis\" : None , \"remap_x_axis\" : None , } def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _ann_dir_path = utils . resolve_path ( config [ \"ann_dir\" ]) self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] self . _load_annotations () self . _camera = camera_utils . Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 ) def _check_dirs ( self ) -> None : if os . path . exists ( self . _root_dir_path ) and os . path . exists ( self . _ann_dir_path ): pass else : print ( \"REDWOOD75 dataset not found, do you want to download it into the \" \"following directories:\" ) print ( \" \" , self . _root_dir_path ) print ( \" \" , self . _ann_dir_path ) while True : decision = input ( \"(Y/n) \" ) . lower () if decision == \"\" or decision == \"y\" : self . _download_dataset () break elif decision == \"n\" : print ( \"Dataset not found. Aborting.\" ) exit ( 0 ) def _download_dataset ( self ) -> None : # Download anns if not os . path . exists ( self . _ann_dir_path ): zip_path = os . path . join ( self . _ann_dir_path , \"redwood75.zip\" ) os . makedirs ( self . _ann_dir_path , exist_ok = True ) url = ( \"https://drive.google.com/u/0/uc?id=1PMvIblsXWDxEJykVwhUk_QEjy4_bmDU\" \"-&export=download\" ) utils . download ( url , zip_path ) z = zipfile . ZipFile ( zip_path ) z . extractall ( os . path . join ( self . _ann_dir_path , \"..\" )) z . close () os . remove ( zip_path ) ann_json = os . path . join ( self . _ann_dir_path , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) baseurl = \"https://s3.us-west-1.wasabisys.com/redwood-3dscan/rgbd/\" for seq_id in anns_dict . keys (): download_dir_path = os . path . join ( self . _root_dir_path , anns_dict [ seq_id ][ \"category\" ] ) os . makedirs ( download_dir_path , exist_ok = True ) zip_path = os . path . join ( download_dir_path , f \" { seq_id } .zip\" ) os . makedirs ( os . path . dirname ( zip_path ), exist_ok = True ) utils . download ( baseurl + f \" { seq_id } .zip\" , zip_path ) z = zipfile . ZipFile ( zip_path ) seq_dir_path = os . path . join ( download_dir_path , \"rgbd\" , seq_id ) os . makedirs ( seq_dir_path , exist_ok = True ) z . extractall ( seq_dir_path ) z . close () os . remove ( zip_path ) def _load_annotations ( self ) -> None : \"\"\"Load annotations into memory.\"\"\" ann_json = os . path . join ( self . _ann_dir_path , \"annotations.json\" ) with open ( ann_json , \"r\" ) as f : anns_dict = json . load ( f ) self . _raw_samples = [] for seq_id , seq_anns in anns_dict . items (): for pose_ann in seq_anns [ \"pose_anns\" ]: self . _raw_samples . append ( self . _create_raw_sample ( seq_id , seq_anns , pose_ann ) ) def _create_raw_sample ( self , seq_id : str , sequence_dict : dict , annotation_dict : dict ) -> dict : \"\"\"Create raw sample from information in annotations file.\"\"\" position = torch . tensor ( annotation_dict [ \"position\" ]) orientation_q = torch . tensor ( annotation_dict [ \"orientation\" ]) rgb_filename = annotation_dict [ \"rgb_file\" ] depth_filename = annotation_dict [ \"depth_file\" ] mesh_filename = sequence_dict [ \"mesh\" ] mesh_path = os . path . join ( self . _ann_dir_path , mesh_filename ) category_str = sequence_dict [ \"category\" ] color_path = os . path . join ( self . _root_dir_path , category_str , \"rgbd\" , seq_id , \"rgb\" , rgb_filename ) depth_path = os . path . join ( self . _root_dir_path , category_str , \"rgbd\" , seq_id , \"depth\" , depth_filename ) extents = torch . tensor ( sequence_dict [ \"scale\" ]) * 2 return { \"position\" : position , \"orientation_q\" : orientation_q , \"extents\" : extents , \"color_path\" : color_path , \"depth_path\" : depth_path , \"mesh_path\" : mesh_path , \"category_str\" : category_str , } def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples ) def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample def _compute_mask ( self , depth : torch . Tensor , raw_sample : dict ) -> torch . Tensor : posed_mesh = o3d . io . read_triangle_mesh ( raw_sample [ \"mesh_path\" ]) R = Rotation . from_quat ( raw_sample [ \"orientation_q\" ]) . as_matrix () posed_mesh . rotate ( R , center = np . array ([ 0 , 0 , 0 ])) posed_mesh . translate ( raw_sample [ \"position\" ]) posed_mesh . compute_vertex_normals () gt_depth = torch . from_numpy ( _draw_depth_geometry ( posed_mesh , self . _camera )) mask = gt_depth != 0 # exclude occluded parts from mask mask [( depth != 0 ) * ( depth < gt_depth - 0.01 )] = 0 return mask def _load_depth ( self , depth_path : str ) -> torch . Tensor : \"\"\"Load depth from depth filepath.\"\"\" depth = torch . from_numpy ( np . asarray ( Image . open ( depth_path ), dtype = np . float32 ) * 0.001 ) return depth def _get_scale ( self , extents : torch . Tensor ) -> float : \"\"\"Return scale from stored sample data and extents.\"\"\" if self . _scale_convention == \"diagonal\" : return torch . linalg . norm ( extents ) elif self . _scale_convention == \"max\" : return extents . max () elif self . _scale_convention == \"half_max\" : return 0.5 * extents . max () elif self . _scale_convention == \"full\" : return extents else : raise ValueError ( f \"Specified scale convention { self . _scale_convention } not supported.\" ) def _change_axis_convention ( self , orientation_q : torch . Tensor , extents : torch . Tensor ) -> tuple : \"\"\"Adjust up-axis for orientation and extents. Returns: Tuple of position, orienation_q and extents, with specified up-axis. \"\"\" if self . _remap_y_axis is None and self . _remap_x_axis is None : return orientation_q , extents elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () remapped_extents = torch . abs ( torch . Tensor ( rotation_o2n ) @ extents ) # quaternion so far: original -> camera # we want a quaternion: new -> camera rotation_n2o = rotation_o2n . T quaternion_n2o = torch . from_numpy ( Rotation . from_matrix ( rotation_n2o ) . as_quat ()) remapped_orientation_q = quaternion_utils . quaternion_multiply ( orientation_q , quaternion_n2o ) # new -> original -> camera return remapped_orientation_q , remapped_extents def _get_o2n_object_rotation_matrix ( self ) -> np . ndarray : \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\" rotation_o2n = np . zeros (( 3 , 3 )) # original to new object convention if self . _remap_y_axis == \"x\" : rotation_o2n [ 0 , 1 ] = 1 elif self . _remap_y_axis == \"-x\" : rotation_o2n [ 0 , 1 ] = - 1 elif self . _remap_y_axis == \"y\" : rotation_o2n [ 1 , 1 ] = 1 elif self . _remap_y_axis == \"-y\" : rotation_o2n [ 1 , 1 ] = - 1 elif self . _remap_y_axis == \"z\" : rotation_o2n [ 2 , 1 ] = 1 elif self . _remap_y_axis == \"-z\" : rotation_o2n [ 2 , 1 ] = - 1 else : raise ValueError ( \"Unsupported remap_y_axis {self.remap_y} \" ) if self . _remap_x_axis == \"x\" : rotation_o2n [ 0 , 0 ] = 1 elif self . _remap_x_axis == \"-x\" : rotation_o2n [ 0 , 0 ] = - 1 elif self . _remap_x_axis == \"y\" : rotation_o2n [ 1 , 0 ] = 1 elif self . _remap_x_axis == \"-y\" : rotation_o2n [ 1 , 0 ] = - 1 elif self . _remap_x_axis == \"z\" : rotation_o2n [ 2 , 0 ] = 1 elif self . _remap_x_axis == \"-z\" : rotation_o2n [ 2 , 0 ] = - 1 else : raise ValueError ( \"Unsupported remap_x_axis {self.remap_y} \" ) # infer last column rotation_o2n [:, 2 ] = 1 - np . abs ( np . sum ( rotation_o2n , 1 )) # rows must sum to +-1 rotation_o2n [:, 2 ] *= np . linalg . det ( rotation_o2n ) # make special orthogonal if np . linalg . det ( rotation_o2n ) != 1.0 : # check if special orthogonal raise ValueError ( \"Unsupported combination of remap_{y,x}_axis. det != 1\" ) return rotation_o2n def _quat_to_orientation_repr ( self , quaternion : torch . Tensor ) -> torch . Tensor : \"\"\"Convert quaternion to selected orientation representation. Args: quaternion: The quaternion to convert, scalar-last, shape (4,). Returns: The same orientation as represented by the quaternion in the chosen orientation representation. \"\"\" if self . _orientation_repr == \"quaternion\" : return quaternion elif self . _orientation_repr == \"discretized\" : index = self . _orientation_grid . quat_to_index ( quaternion . numpy ()) return torch . tensor ( index , dtype = torch . long , ) else : raise NotImplementedError ( f \"Orientation representation { self . _orientation_repr } is not supported.\" ) def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"AnnotatedRedwoodDataset"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.Config","text":"Bases: TypedDict Configuration dictionary for annoated Redwood dataset. ATTRIBUTE DESCRIPTION root_dir See AnnotatedRedwoodDataset docstring. TYPE: str ann_dir See AnnotatedRedwoodDataset docstring. TYPE: str mask_pointcloud Whether the returned pointcloud will be masked. TYPE: bool normalize_pointcloud Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. TYPE: bool scale_convention Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). TYPE: str camera_convention Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. TYPE: str orientation_repr Which orientation representation is used. Currently only \"quaternion\" supported. TYPE: str remap_y_axis If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] remap_x_axis If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" TYPE: Optional [ str ] category_str If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. TYPE: Optional [ str ] Source code in cpas_toolbox/datasets/redwood_dataset.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Config ( TypedDict , total = False ): \"\"\"Configuration dictionary for annoated Redwood dataset. Attributes: root_dir: See AnnotatedRedwoodDataset docstring. ann_dir: See AnnotatedRedwoodDataset docstring. mask_pointcloud: Whether the returned pointcloud will be masked. normalize_pointcloud: Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin. scale_convention: Which scale is returned. The following strings are supported: \"diagonal\": Length of bounding box' diagonal. This is what NOCS uses. \"max\": Maximum side length of bounding box. \"half_max\": Half maximum side length of bounding box. \"full\": Bounding box side lengths. Shape (3,). camera_convention: Which camera convention is used for position and orientation. One of: \"opengl\": x right, y up, z back \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion. orientation_repr: Which orientation representation is used. Currently only \"quaternion\" supported. remap_y_axis: If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" remap_x_axis: If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\" category_str: If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings. \"\"\" root_dir : str ann_dir : str split : str mask_pointcloud : bool normalize_pointcloud : bool scale_convention : str camera_convention : str orientation_repr : str orientation_grid_resolution : int remap_y_axis : Optional [ str ] remap_x_axis : Optional [ str ] category_str : Optional [ str ]","title":"Config"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__init__","text":"__init__ ( config : Config ) -> None Initialize the dataset. PARAMETER DESCRIPTION config Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. TYPE: Config Source code in cpas_toolbox/datasets/redwood_dataset.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def __init__ ( self , config : Config , ) -> None : \"\"\"Initialize the dataset. Args: config: Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys. \"\"\" config = yoco . load_config ( config , current_dict = AnnotatedRedwoodDataset . default_config ) self . _root_dir_path = utils . resolve_path ( config [ \"root_dir\" ]) self . _ann_dir_path = utils . resolve_path ( config [ \"ann_dir\" ]) self . _check_dirs () self . _camera_convention = config [ \"camera_convention\" ] self . _mask_pointcloud = config [ \"mask_pointcloud\" ] self . _normalize_pointcloud = config [ \"normalize_pointcloud\" ] self . _scale_convention = config [ \"scale_convention\" ] self . _remap_y_axis = config [ \"remap_y_axis\" ] self . _remap_x_axis = config [ \"remap_x_axis\" ] self . _orientation_repr = config [ \"orientation_repr\" ] self . _load_annotations () self . _camera = camera_utils . Camera ( width = 640 , height = 480 , fx = 525 , fy = 525 , cx = 319.5 , cy = 239.5 )","title":"__init__()"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__len__","text":"__len__ () -> int Return number of sample in dataset. Source code in cpas_toolbox/datasets/redwood_dataset.py 240 241 242 def __len__ ( self ) -> int : \"\"\"Return number of sample in dataset.\"\"\" return len ( self . _raw_samples )","title":"__len__()"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__getitem__","text":"__getitem__ ( idx : int ) -> dict Return a sample of the dataset. PARAMETER DESCRIPTION idx Index of the instance. TYPE: int RETURNS DESCRIPTION dict Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" Source code in cpas_toolbox/datasets/redwood_dataset.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __getitem__ ( self , idx : int ) -> dict : \"\"\"Return a sample of the dataset. Args: idx: Index of the instance. Returns: Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\" \"\"\" raw_sample = self . _raw_samples [ idx ] color = torch . from_numpy ( np . asarray ( Image . open ( raw_sample [ \"color_path\" ]), dtype = np . float32 ) / 255 ) depth = self . _load_depth ( raw_sample [ \"depth_path\" ]) instance_mask = self . _compute_mask ( depth , raw_sample ) pointcloud_mask = instance_mask if self . _mask_pointcloud else None pointcloud = pointset_utils . depth_to_pointcloud ( depth , self . _camera , mask = pointcloud_mask , convention = self . _camera_convention , ) # adjust camera convention for position, orientation and scale position = pointset_utils . change_position_camera_convention ( raw_sample [ \"position\" ], \"opencv\" , self . _camera_convention ) # orientation / scale orientation_q , extents = self . _change_axis_convention ( raw_sample [ \"orientation_q\" ], raw_sample [ \"extents\" ] ) orientation_q = pointset_utils . change_orientation_camera_convention ( orientation_q , \"opencv\" , self . _camera_convention ) orientation = self . _quat_to_orientation_repr ( orientation_q ) scale = self . _get_scale ( extents ) # normalize pointcloud & position if self . _normalize_pointcloud : pointcloud , centroid = pointset_utils . normalize_points ( pointcloud ) position = position - centroid category_str = raw_sample [ \"category_str\" ] sample = { \"color\" : color , \"depth\" : depth , \"pointset\" : pointcloud , \"mask\" : instance_mask , \"position\" : position , \"orientation\" : orientation , \"quaternion\" : orientation_q , \"scale\" : scale , \"color_path\" : raw_sample [ \"color_path\" ], \"obj_path\" : raw_sample [ \"mesh_path\" ], \"category_id\" : self . category_str_to_id [ category_str ], \"category_str\" : category_str , } return sample","title":"__getitem__()"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.load_mesh","text":"load_mesh ( object_path : str ) -> o3d . geometry . TriangleMesh Load an object mesh and adjust its object frame convention. Source code in cpas_toolbox/datasets/redwood_dataset.py 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def load_mesh ( self , object_path : str ) -> o3d . geometry . TriangleMesh : \"\"\"Load an object mesh and adjust its object frame convention.\"\"\" mesh = o3d . io . read_triangle_mesh ( object_path ) if self . _remap_y_axis is None and self . _remap_x_axis is None : return mesh elif self . _remap_y_axis is None or self . _remap_x_axis is None : raise ValueError ( \"Either both or none of remap_{y,x}_axis have to be None.\" ) rotation_o2n = self . _get_o2n_object_rotation_matrix () mesh . rotate ( rotation_o2n , center = np . array ([ 0.0 , 0.0 , 0.0 ])[:, None ], ) return mesh","title":"load_mesh()"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.ObjectError","text":"Bases: Exception Error if something with the mesh is wrong. Source code in cpas_toolbox/datasets/redwood_dataset.py 459 460 461 462 class ObjectError ( Exception ): \"\"\"Error if something with the mesh is wrong.\"\"\" pass","title":"ObjectError"}]}