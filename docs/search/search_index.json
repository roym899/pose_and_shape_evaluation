{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>CPAS Toolbox is a package for evaluation of categorical pose and shape estimation methods. It contains metrics, and wrappers for datasets and methods.</p>"},{"location":"#installation","title":"Installation","text":"<p>Run <pre><code>pip install cpas_toolbox\n</code></pre> to install the latest release of the toolbox. There is no need to download any additional weights or datasets. Upon first usage the evaluation script will ask to download the weights if they are not available at the expected path.</p>"},{"location":"#evaluation-of-baseline-methods","title":"Evaluation of baseline methods","text":"<p>To reproduce the REAL275 benchmark run: <pre><code>python -m cpas_toolbox.evaluate --config real275.yaml all_baselines.yaml --out_dir ./results/\n</code></pre> To reproduce the REDWOOD75 benchmark run: <pre><code>python -m cpas_toolbox.evaluate --config redwood75.yaml all_baselines.yaml --out_dir ./results/\n</code></pre></p> <p>We can overwrite settings of the configuration via the command-line. For example,  <pre><code>python -m cpas_toolbox.evaluate --config redwood75.yaml all_baselines.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True\n</code></pre> enables interactive visualization of ground truth and predictions. Alternatively, you could specify <code>--store_visualization True</code> to save the visualization of every prediction in the results directory.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this library useful in your research, consider citing our publication: <pre><code>@article{bruns2022evaluation,\n  title={On the Evaluation of {RGB-D}-based Categorical Pose and Shape Estimation},\n  author={Bruns, Leonard and Jensfelt, Patric},\n  journal={arXiv preprint arXiv:2202.10346},\n  year={2022}\n}\n</code></pre></p>"},{"location":"data_preparation/","title":"Data preparation","text":"<p>On first usage of a dataset the script will download and preprocess the datasets automatically. This is the recommended way to use the package as it ensures an unmodified dataset.</p> <p>If you already downloaded a dataset and want to use symlinks instead of storing them again to save storage space, you can follow the manual instructions below.</p> <p>The default directories can be found in the configuration file for the respective dataset (REAL275, REDWOOD75).</p>"},{"location":"data_preparation/#real275","title":"REAL275","text":"<p>For download links check the NOCS repository.</p> <p>The expected directory structure for REAL275 evaluation is as follows: <pre><code>    {root_dir}/real_test/...\n    {root_dir}/gts/...\n    {root_dir}/obj_models/...\n</code></pre> An additional directory <code>{root_dir}/cpas_toolbox/</code> will be created to store preprocessed files. By default <code>{root_dir}</code> will be <code>data/nocs/</code> (i.e., relative to the current working directory, when executing the evaluation script), but it can be modified.</p>"},{"location":"data_preparation/#redwood75","title":"REDWOOD75","text":"<p>To download the raw data check the redwood-data website. You can download the REDWOOD75 annotations here. Only the Redwood sequences ids contained in this file are required for evaluation.</p> <p>The expected directory structure for REDWOOD75 evaluation is as follows: <pre><code>    {root_dir}/bottle/rgbd/00049/depth/*.png\n    {root_dir}/bottle/rgbd/...\n    {root_dir}/bowl/...\n    {root_dir}/mug/...\n    {ann_dir}/obj_models/...\n</code></pre> By default <code>{root_dir}</code> will be <code>data/redwood/</code> (i.e., relative to the current working directory, when executing the evaluation script) <code>{ann_dir}</code> will be <code>data/redwood75/</code>, but those can be modified.</p>"},{"location":"tutorial_evaluation/","title":"Tutorial: Evaluate new method","text":"<p>In this tutorial we will walk through how to use the toolbox to evaluate a new method for pose and shape estimation. We assume the toolbox has already been installed successfully and the standard commands work.</p> <p>To evaluate a new method, we need to implement the interface between the method and the toolbox by defining a class inheriting from CPASMethod. The derived class needs to define at least the inference function, which is called by the evaluation script. The __init__ typically also has to be defined as it provides the dataset's camera parameters which are typically required for inference.</p> <p>Typically three steps have to implemented: </p> <ol> <li>Converting the input to the expected input format</li> <li>Call the inference code of the new method</li> <li>Convert the output of the method to the expected output format for the toolbox</li> </ol> <pre><code>from cpas_toolbox.cpas_method import CPASMethod, PredictionDict\nimport torch\nclass MyCPASMethod(CPASMethod):\ndef __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize categorical pose and shape estimation method.\n        Args:\n            config: Method configuration dictionary.\n            camera: Camera used for the input image.\n        \"\"\"\n# save / convert camera as needed by inference\npass\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"Run a method to predict pose and shape of an object.\n        Args:\n            color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n            depth_image: The depth image, shape (H, W), meters, float.\n            instance_mask: Mask of object of interest. (H, W), bool.\n            category_str: The category of the object.\n        \"\"\"\n# Step 1\n# convert color_image, depth_image, instance_mask, category_str\n# to whatever convention your inference code expects\n# Step 2\n# run your inference code\n# Step 3\n# convert output of your method to prediction dictionary\npass\n</code></pre>"},{"location":"tutorial_evaluation/#step-1-converting-the-input","title":"Step 1: Converting the input","text":"<p>Most methods follow different conventions for the color and depth image.</p> <p>Common pitfalls: </p> <ul> <li>OpenCV uses BGR instead of RGB, use <code>color_image = color_image[:,:,::-1]</code> to convert</li> <li>range of color image might be expected to be 0-255 instead of 0-1</li> <li>depth_image might be expected to contain millimeter instead of meter</li> <li>often a batch dimension has to be added</li> <li>data has to be moved to same device as model (all parameters will be on CPU by default)</li> </ul>"},{"location":"tutorial_evaluation/#step-2-calling-the-method","title":"Step 2: Calling the method","text":"<p>This step is method specific. Ideally this is a single call to a method's inference function.</p>"},{"location":"tutorial_evaluation/#step-3-converting-the-output","title":"Step 3: Converting the output","text":"<p>The expected output is a dictionary as specified by PredictionDict.</p> <p>Position and orientation should be in OpenCV convention, which refers to x-axis right, y-axis down, z-axis forward (see this page for visualization).</p> <p>Common pitfalls / things to make sure are right:</p> <ul> <li>some methods predict in OpenGL camera convention (x-axis right, y-axis up, z-axis backward), both position and orientation has to be adjusted in this case (see, e.g., sdfest.py)</li> <li>some methods use different canonical object orientations; our toolbox normalizes all datasets to follow ShapeNet convention (see below), changing the convention requires adjusting the orientation, extent, and reconstruction (see, e.g., spd.py)</li> </ul> <p></p>"},{"location":"tutorial_evaluation/#create-config-file-and-run-the-evaluation","title":"Create config file and run the evaluation","text":"<p>Once the class has been created and saved as a Python module (<code>mycpasmethod.py</code>) we can run the evaluation for any dataset. To do so, we must create the following config file (<code>mycpasmethod.yaml</code>): <pre><code>methods:\nmycpasmethod:\n# name of method (used in results files)\nname: MyMethodName\n# fully-specified name of class\nmethod_type: mycpasmethod.MyCPASMethod\n# this dictionary will be passed to __init__ of MyCPASMethod\nconfig_dict:\nx: 1  </code></pre> To run the evaluation on REAL275 we can now use <pre><code>python -m cpas_toolbox.evaluate --config real275.yaml mycpasmethod.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True\n</code></pre> <code>real275.yaml</code> can simply be replaced with <code>redwood75.yaml</code> to run the evaluation on REDWOOD75. The visualization of the prediction can be used to debug mismatched conventions and should be deactivated to evaluate the whole dataset.</p>"},{"location":"api_reference/camera_utils/","title":"camera_utils.py","text":""},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils","title":"cpas_toolbox.camera_utils","text":"<p>This module provides a pinhole camera class.</p>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera","title":"Camera","text":"<p>Pinhole camera parameters.</p> <p>This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision.</p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>class Camera:\n\"\"\"Pinhole camera parameters.\n    This class allows conversion between different pixel conventions, i.e., pixel\n    center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in\n    computer vision.\n    \"\"\"\ndef __init__(\nself,\nwidth: int,\nheight: int,\nfx: float,\nfy: float,\ncx: float,\ncy: float,\ns: float = 0.0,\npixel_center: float = 0.0,\n) -&gt; None:\n\"\"\"Initialize camera parameters.\n        Note that the principal point is only fully defined in combination with\n        pixel_center.\n        The pixel_center defines the relation between continuous image plane\n        coordinates and discrete pixel coordinates.\n        A discrete image coordinate (x, y) will correspond to the continuous\n        image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n        will be either 0 or 0.5. During calibration it depends on the convention\n        the point features used to compute the calibration matrix.\n        Note that if pixel_center == 0, the corresponding continuous coordinate\n        interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n        to convert from continuous coordinate to the corresponding discrete coordinate.\n        For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n        pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n        coordinate to the corresponding discrete coordinate.\n        Args:\n            width: Number of pixels in horizontal direction.\n            height: Number of pixels in vertical direction.\n            fx: Horizontal focal length.\n            fy: Vertical focal length.\n            cx: Principal point x-coordinate.\n            cy: Principal point y-coordinate.\n            s: Skew.\n            pixel_center: The center offset for the provided principal point.\n        \"\"\"\n# focal length\nself.fx = fx\nself.fy = fy\n# principal point\nself.cx = cx\nself.cy = cy\nself.pixel_center = pixel_center\n# skew\nself.s = s\n# image dimensions\nself.width = width\nself.height = height\ndef get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n\"\"\"Convert camera to Open3D pinhole camera parameters.\n        Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n        values are in front of camera). Open3D expects camera with pixel_center = 0\n        and does not support skew.\n        Returns:\n            The pinhole camera parameters.\n        \"\"\"\nfx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\nparams = o3d.camera.PinholeCameraParameters()\nparams.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\nparams.extrinsic = np.array(\n[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n)\nreturn params\ndef get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n\"\"\"Convert camera to general camera parameters.\n        Args:\n            pixel_center:\n                At which ratio of a square the pixel center should be for the resulting\n                parameters. Typically 0 or 0.5. See class documentation for more info.\n        Returns:\n            - fx, fy: The horizontal and vertical focal length\n            - cx, cy:\n                The position of the principal point in continuous image plane\n                coordinates considering the provided pixel center and the pixel center\n                specified during the construction.\n            - s: The skew.\n        \"\"\"\ncx_corrected = self.cx - self.pixel_center + pixel_center\ncy_corrected = self.cy - self.pixel_center + pixel_center\nreturn self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.__init__","title":"__init__","text":"<pre><code>__init__(\nwidth: int,\nheight: int,\nfx: float,\nfy: float,\ncx: float,\ncy: float,\ns: float = 0.0,\npixel_center: float = 0.0,\n) -&gt; None\n</code></pre> <p>Initialize camera parameters.</p> <p>Note that the principal point is only fully defined in combination with pixel_center.</p> <p>The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates.</p> <p>A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix.</p> <p>Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate.</p> <p>For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate.</p> PARAMETER DESCRIPTION <code>width</code> <p>Number of pixels in horizontal direction.</p> <p> TYPE: <code>int</code> </p> <code>height</code> <p>Number of pixels in vertical direction.</p> <p> TYPE: <code>int</code> </p> <code>fx</code> <p>Horizontal focal length.</p> <p> TYPE: <code>float</code> </p> <code>fy</code> <p>Vertical focal length.</p> <p> TYPE: <code>float</code> </p> <code>cx</code> <p>Principal point x-coordinate.</p> <p> TYPE: <code>float</code> </p> <code>cy</code> <p>Principal point y-coordinate.</p> <p> TYPE: <code>float</code> </p> <code>s</code> <p>Skew.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>pixel_center</code> <p>The center offset for the provided principal point.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def __init__(\nself,\nwidth: int,\nheight: int,\nfx: float,\nfy: float,\ncx: float,\ncy: float,\ns: float = 0.0,\npixel_center: float = 0.0,\n) -&gt; None:\n\"\"\"Initialize camera parameters.\n    Note that the principal point is only fully defined in combination with\n    pixel_center.\n    The pixel_center defines the relation between continuous image plane\n    coordinates and discrete pixel coordinates.\n    A discrete image coordinate (x, y) will correspond to the continuous\n    image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n    will be either 0 or 0.5. During calibration it depends on the convention\n    the point features used to compute the calibration matrix.\n    Note that if pixel_center == 0, the corresponding continuous coordinate\n    interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n    to convert from continuous coordinate to the corresponding discrete coordinate.\n    For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n    pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n    coordinate to the corresponding discrete coordinate.\n    Args:\n        width: Number of pixels in horizontal direction.\n        height: Number of pixels in vertical direction.\n        fx: Horizontal focal length.\n        fy: Vertical focal length.\n        cx: Principal point x-coordinate.\n        cy: Principal point y-coordinate.\n        s: Skew.\n        pixel_center: The center offset for the provided principal point.\n    \"\"\"\n# focal length\nself.fx = fx\nself.fy = fy\n# principal point\nself.cx = cx\nself.cy = cy\nself.pixel_center = pixel_center\n# skew\nself.s = s\n# image dimensions\nself.width = width\nself.height = height\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_o3d_pinhole_camera_parameters","title":"get_o3d_pinhole_camera_parameters","text":"<pre><code>get_o3d_pinhole_camera_parameters() -&gt; o3d.camera.PinholeCameraParameters()\n</code></pre> <p>Convert camera to Open3D pinhole camera parameters.</p> <p>Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew.</p> RETURNS DESCRIPTION <code>o3d.camera.PinholeCameraParameters()</code> <p>The pinhole camera parameters.</p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n\"\"\"Convert camera to Open3D pinhole camera parameters.\n    Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n    values are in front of camera). Open3D expects camera with pixel_center = 0\n    and does not support skew.\n    Returns:\n        The pinhole camera parameters.\n    \"\"\"\nfx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\nparams = o3d.camera.PinholeCameraParameters()\nparams.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\nparams.extrinsic = np.array(\n[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n)\nreturn params\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_pinhole_camera_parameters","title":"get_pinhole_camera_parameters","text":"<pre><code>get_pinhole_camera_parameters(pixel_center: float) -&gt; Tuple\n</code></pre> <p>Convert camera to general camera parameters.</p> PARAMETER DESCRIPTION <code>pixel_center</code> <p>At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <ul> <li>fx, fy: The horizontal and vertical focal length</li> </ul> <code>Tuple</code> <ul> <li>cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction.</li> </ul> <code>Tuple</code> <ul> <li>s: The skew.</li> </ul> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n\"\"\"Convert camera to general camera parameters.\n    Args:\n        pixel_center:\n            At which ratio of a square the pixel center should be for the resulting\n            parameters. Typically 0 or 0.5. See class documentation for more info.\n    Returns:\n        - fx, fy: The horizontal and vertical focal length\n        - cx, cy:\n            The position of the principal point in continuous image plane\n            coordinates considering the provided pixel center and the pixel center\n            specified during the construction.\n        - s: The skew.\n    \"\"\"\ncx_corrected = self.cx - self.pixel_center + pixel_center\ncy_corrected = self.cy - self.pixel_center + pixel_center\nreturn self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"api_reference/cpas_method/","title":"cpas_method.py","text":""},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method","title":"cpas_toolbox.cpas_method","text":"<p>This module defines the interface for categorical pose and shape estimation methods.</p> <p>This module defines two classes: PredictionDict and CPASMethod. PredictionDict defines the prediction produced by a CPASMethod. CPASMethod defines the interface used to evaluate categorical pose and shape estimation methods.</p>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.PredictionDict","title":"PredictionDict","text":"<p>         Bases: <code>TypedDict</code></p> <p>Pose and shape prediction.</p> ATTRIBUTE DESCRIPTION <code>position</code> <p>Position of object center in camera frame. OpenCV convention. Shape (3,).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>orientation</code> <p>Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>extents</code> <p>Bounding box side lengths, shape (3,).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>reconstructed_pointcloud</code> <p>Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction.</p> <p> TYPE: <code>Optional[torch.Tensor]</code> </p> <code>reconstructed_mesh</code> <p>Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction.</p> <p> TYPE: <code>Optional[o3d.geometry.TriangleMesh]</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>class PredictionDict(TypedDict):\n\"\"\"Pose and shape prediction.\n    Attributes:\n        position:\n            Position of object center in camera frame. OpenCV convention. Shape (3,).\n        orientation:\n            Orientation of object in camera frame. OpenCV convention.\n            Scalar-last quaternion, shape (4,).\n        extents:\n            Bounding box side lengths, shape (3,).\n        reconstructed_pointcloud:\n            Metrically-scaled reconstructed pointcloud in object frame.\n            None if method does not perform reconstruction.\n        reconstructed_mesh:\n            Metrically-scaled reconstructed mesh in object frame.\n            None if method does not perform reconstruction.\n    \"\"\"\nposition: torch.Tensor\norientation: torch.Tensor\nextents: torch.Tensor\nreconstructed_pointcloud: Optional[torch.Tensor]\nreconstructed_mesh: Optional[o3d.geometry.TriangleMesh]\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod","title":"CPASMethod","text":"<p>         Bases: <code>ABC</code></p> <p>Interface class for categorical pose and shape estimation methods.</p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>class CPASMethod(ABC):\n\"\"\"Interface class for categorical pose and shape estimation methods.\"\"\"\ndef __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize categorical pose and shape estimation method.\n        Args:\n            config: Method configuration dictionary.\n            camera: Camera used for the input image.\n        \"\"\"\npass\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"Run a method to predict pose and shape of an object.\n        Args:\n            color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n            depth_image: The depth image, shape (H, W), meters, float.\n            instance_mask: Mask of object of interest. (H, W), bool.\n            category_str: The category of the object.\n        \"\"\"\npass\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod.__init__","title":"__init__","text":"<pre><code>__init__(config: dict, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize categorical pose and shape estimation method.</p> PARAMETER DESCRIPTION <code>config</code> <p>Method configuration dictionary.</p> <p> TYPE: <code>dict</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>def __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize categorical pose and shape estimation method.\n    Args:\n        config: Method configuration dictionary.\n        camera: Camera used for the input image.\n    \"\"\"\npass\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>Run a method to predict pose and shape of an object.</p> PARAMETER DESCRIPTION <code>color_image</code> <p>The color image, shape (H, W, 3), RGB, 0-1, float.</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>depth_image</code> <p>The depth image, shape (H, W), meters, float.</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>instance_mask</code> <p>Mask of object of interest. (H, W), bool.</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>category_str</code> <p>The category of the object.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"Run a method to predict pose and shape of an object.\n    Args:\n        color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n        depth_image: The depth image, shape (H, W), meters, float.\n        instance_mask: Mask of object of interest. (H, W), bool.\n        category_str: The category of the object.\n    \"\"\"\npass\n</code></pre>"},{"location":"api_reference/evaluate/","title":"evaluate.py","text":""},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate","title":"cpas_toolbox.evaluate","text":"<p>Script to run pose and shape evaluation for different datasets and methods.</p>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator","title":"Evaluator","text":"<p>Class to evaluate various pose and shape estimation algorithms.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>class Evaluator:\n\"\"\"Class to evaluate various pose and shape estimation algorithms.\"\"\"\n# ShapeNetV2 convention for all objects and datasets assumed\n# for simplicity assume all cans, bowls and bottles to be rotation symmetric\nSYMMETRY_AXIS_DICT = {\n\"mug\": None,\n\"laptop\": None,\n\"camera\": None,\n\"can\": 1,\n\"bowl\": 1,\n\"bottle\": 1,\n}\ndef __init__(self, config: dict) -&gt; None:\n\"\"\"Initialize model wrappers and evaluator.\"\"\"\nself._parse_config(config)\ndef _parse_config(self, config: dict) -&gt; None:\n\"\"\"Read config and initialize method wrappers.\"\"\"\nself._init_dataset(config[\"dataset_config\"])\nself._visualize_input = config[\"visualize_input\"]\nself._visualize_prediction = config[\"visualize_prediction\"]\nself._visualize_gt = config[\"visualize_gt\"]\nself._fast_eval = config[\"fast_eval\"]\nself._store_visualization = config[\"store_visualization\"]\nself._run_name = (\nf\"{self._dataset_name}_eval_{config['run_name']}_\"\nf\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n)\nself._out_dir_path = config[\"out_dir\"]\nself._metrics = config[\"metrics\"]\nself._num_gt_points = config[\"num_gt_points\"]\nself._vis_camera_json = config[\"vis_camera_json\"]\nself._render_options_json = config[\"render_options_json\"]\nself._cam = camera_utils.Camera(**config[\"camera\"])\nself._init_wrappers(config[\"methods\"])\nself._config = config\ndef _init_dataset(self, dataset_config: dict) -&gt; None:\n\"\"\"Initialize reading of dataset.\n        This includes sanity checks whether the provided path is correct.\n        \"\"\"\nself._dataset_name = dataset_config[\"name\"]\nprint(f\"Initializing {self._dataset_name} dataset...\")\ndataset_type = utils.str_to_object(dataset_config[\"type\"])\nself._dataset = dataset_type(config=dataset_config[\"config_dict\"])\n# Faster but probably only worth it if whole evaluation supports batches\n# self._dataloader = DataLoader(self._dataset, 1, num_workers=8)\nif len(self._dataset) == 0:\nprint(f\"No images found for dataset {self._dataset_name}\")\nexit()\nprint(f\"{len(self._dataset)} samples found for dataset {self._dataset_name}.\")\ndef _init_wrappers(self, method_configs: dict) -&gt; None:\n\"\"\"Initialize method wrappers.\"\"\"\nself._wrappers = {}\nfor method_dict in method_configs.values():\nmethod_name = method_dict[\"name\"]\nprint(f\"Initializing {method_name}...\")\nmethod_type = utils.str_to_object(method_dict[\"method_type\"])\nif method_type is None:\nprint(f\"Could not find class {method_dict['method_type']}\")\ncontinue\nself._wrappers[method_name] = method_type(\nconfig=method_dict[\"config_dict\"], camera=self._cam\n)\ndef _eval_method(self, method_name: str, method_wrapper: CPASMethod) -&gt; None:\n\"\"\"Run and evaluate method on all samples.\"\"\"\nprint(f\"Run {method_name}...\")\nself._init_metrics()\nindices = list(range(len(self._dataset)))\nrandom.seed(0)\nrandom.shuffle(indices)\nfor i in tqdm(indices):\nif self._fast_eval and i % 10 != 0:\ncontinue\nsample = self._dataset[i]\nif self._visualize_input:\n_, ((ax1, ax2), (ax3, _)) = plt.subplots(2, 2)\nax1.imshow(sample[\"color\"].numpy())\nax2.imshow(sample[\"depth\"].numpy())\nax3.imshow(sample[\"mask\"].numpy())\nplt.show()\nt_start = time.time()\nprediction = method_wrapper.inference(\ncolor_image=sample[\"color\"],\ndepth_image=sample[\"depth\"],\ninstance_mask=sample[\"mask\"],\ncategory_str=sample[\"category_str\"],\n)\ninference_time = time.time() - t_start\nself._runtime_data[\"total\"] += inference_time\nself._runtime_data[\"count\"] += 1\nif self._visualize_gt:\nvisualize_estimation(\ncolor_image=sample[\"color\"],\ndepth_image=sample[\"depth\"],\nlocal_cv_position=sample[\"position\"],\nlocal_cv_orientation_q=sample[\"quaternion\"],\nreconstructed_mesh=self._dataset.load_mesh(sample[\"obj_path\"]),\nextents=sample[\"scale\"],\ncamera=self._cam,\nvis_camera_json=self._vis_camera_json,\nrender_options_json=self._render_options_json,\n)  # GT estimate\nif self._visualize_prediction:\nvisualize_estimation(\ncolor_image=sample[\"color\"],\ndepth_image=sample[\"depth\"],\nlocal_cv_position=prediction[\"position\"],\nlocal_cv_orientation_q=prediction[\"orientation\"],\nextents=prediction[\"extents\"],\nreconstructed_points=prediction[\"reconstructed_pointcloud\"],\nreconstructed_mesh=prediction[\"reconstructed_mesh\"],\ncamera=self._cam,\nvis_camera_json=self._vis_camera_json,\nrender_options_json=self._render_options_json,\n)\nif self._store_visualization:\nvis_dir_path = os.path.join(\nself._out_dir_path, self._run_name, \"visualization\"\n)\nos.makedirs(vis_dir_path, exist_ok=True)\nvis_file_path = os.path.join(vis_dir_path, f\"{i:06}_{method_name}.jpg\")\nvisualize_estimation(\ncolor_image=sample[\"color\"],\ndepth_image=sample[\"depth\"],\nlocal_cv_position=prediction[\"position\"],\nlocal_cv_orientation_q=prediction[\"orientation\"],\nextents=prediction[\"extents\"],\nreconstructed_points=prediction[\"reconstructed_pointcloud\"],\nreconstructed_mesh=prediction[\"reconstructed_mesh\"],\ncamera=self._cam,\nvis_camera_json=self._vis_camera_json,\nrender_options_json=self._render_options_json,\nvis_file_path=vis_file_path,\n)\nself._eval_prediction(prediction, sample)\nself._finalize_metrics(method_name)\ndef _eval_prediction(self, prediction: PredictionDict, sample: dict) -&gt; None:\n\"\"\"Evaluate all metrics for a prediction.\"\"\"\n# correctness metric\nfor metric_name in self._metrics.keys():\nself._eval_metric(metric_name, prediction, sample)\ndef _init_metrics(self) -&gt; None:\n\"\"\"Initialize metrics.\"\"\"\nself._metric_data = {}\nself._runtime_data = {\n\"total\": 0.0,\n\"count\": 0.0,\n}\nfor metric_name, metric_config_dict in self._metrics.items():\nself._metric_data[metric_name] = self._init_metric_data(metric_config_dict)\ndef _init_metric_data(self, metric_config_dict: dict) -&gt; dict:\n\"\"\"Create data structure necessary to compute a metric.\"\"\"\nmetric_data = {}\nif \"position_thresholds\" in metric_config_dict:\npts = metric_config_dict[\"position_thresholds\"]\ndts = metric_config_dict[\"deg_thresholds\"]\nits = metric_config_dict[\"iou_thresholds\"]\nfts = metric_config_dict[\"f_thresholds\"]\nmetric_data[\"correct_counters\"] = np.zeros(\n(\nlen(pts),\nlen(dts),\nlen(its),\nlen(fts),\nself._dataset.num_categories + 1,\n)\n)\nmetric_data[\"total_counters\"] = np.zeros(self._dataset.num_categories + 1)\nelif \"pointwise_f\" in metric_config_dict:\nmetric_data[\"means\"] = np.zeros(self._dataset.num_categories + 1)\nmetric_data[\"m2s\"] = np.zeros(self._dataset.num_categories + 1)\nmetric_data[\"counts\"] = np.zeros(self._dataset.num_categories + 1)\nelse:\nraise NotImplementedError(\"Unsupported metric configuration.\")\nreturn metric_data\ndef _eval_metric(\nself, metric_name: str, prediction: PredictionDict, sample: dict\n) -&gt; None:\n\"\"\"Evaluate and update single metric for a single prediction.\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\nmetric_config_dict = self._metrics[metric_name]\nif \"position_thresholds\" in metric_config_dict:  # correctness metrics\nself._eval_correctness_metric(metric_name, prediction, sample)\nelif \"pointwise_f\" in metric_config_dict:  # pointwise reconstruction metrics\nself._eval_pointwise_metric(metric_name, prediction, sample)\nelse:\nraise NotImplementedError(\nf\"Unsupported metric configuration with name {metric_name}.\"\n)\ndef _eval_correctness_metric(\nself, metric_name: str, prediction: PredictionDict, sample: dict\n) -&gt; None:\n\"\"\"Evaluate and update single correctness metric for a single prediction.\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\nmetric_dict = self._metrics[metric_name]\ncorrect_counters = self._metric_data[metric_name][\"correct_counters\"]\ntotal_counters = self._metric_data[metric_name][\"total_counters\"]\ncategory_id = sample[\"category_id\"]\ntotal_counters[category_id] += 1\ntotal_counters[-1] += 1\ngt_points, pred_points = self._get_points(sample, prediction, True)\nfor pi, p in enumerate(metric_dict[\"position_thresholds\"]):\nfor di, d in enumerate(metric_dict[\"deg_thresholds\"]):\nfor ii, i in enumerate(metric_dict[\"iou_thresholds\"]):\nfor fi, f in enumerate(metric_dict[\"f_thresholds\"]):\ncorrect = metrics.correct_thresh(\nposition_gt=sample[\"position\"].cpu().numpy(),\nposition_prediction=prediction[\"position\"].cpu().numpy(),\norientation_gt=Rotation.from_quat(sample[\"quaternion\"]),\norientation_prediction=Rotation.from_quat(\nprediction[\"orientation\"]\n),\nextent_gt=sample[\"scale\"].cpu().numpy(),\nextent_prediction=prediction[\"extents\"].cpu().numpy(),\npoints_gt=gt_points,\npoints_prediction=pred_points,\nposition_threshold=p,\ndegree_threshold=d,\niou_3d_threshold=i,\nfscore_threshold=f,\nrotational_symmetry_axis=self.SYMMETRY_AXIS_DICT[\nsample[\"category_str\"]\n],\n)\ncorrect_counters[pi, di, ii, fi, category_id] += correct\ncorrect_counters[pi, di, ii, fi, -1] += correct  # all\ndef _eval_pointwise_metric(\nself, metric_name: str, prediction: PredictionDict, sample: dict\n) -&gt; None:\n\"\"\"Evaluate and update single pointwise metric for a single prediction.\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\nmetric_config_dict = self._metrics[metric_name]\nmeans = self._metric_data[metric_name][\"means\"]\nm2s = self._metric_data[metric_name][\"m2s\"]\ncounts = self._metric_data[metric_name][\"counts\"]\ncategory_id = sample[\"category_id\"]\npoint_metric = utils.str_to_object(metric_config_dict[\"pointwise_f\"])\ngt_points, pred_points = self._get_points(\nsample, prediction, metric_config_dict[\"posed\"]\n)\nresult = point_metric(\ngt_points.numpy(), pred_points.numpy(), **metric_config_dict[\"kwargs\"]\n)\n# Use Welfords algorithm to update mean and variance\n# for category\ncounts[category_id] += 1\ndelta = result - means[category_id]\nmeans[category_id] += delta / counts[category_id]\ndelta2 = result - means[category_id]\nm2s[category_id] += delta * delta2\n# for all\ncounts[-1] += 1\ndelta = result - means[-1]\nmeans[-1] += delta / counts[-1]\ndelta2 = result - means[-1]\nm2s[-1] += delta * delta2\ndef _get_points(\nself, sample: dict, prediction: PredictionDict, posed: bool\n) -&gt; Tuple[np.ndarray]:\n# load ground truth mesh\ngt_mesh = self._dataset.load_mesh(sample[\"obj_path\"])\ngt_points = torch.from_numpy(\nnp.asarray(gt_mesh.sample_points_uniformly(self._num_gt_points).points)\n)\npred_points = prediction[\"reconstructed_pointcloud\"]\n# transform points if posed\nif posed:\ngt_points = quaternion_utils.quaternion_apply(\nsample[\"quaternion\"], gt_points\n)\ngt_points += sample[\"position\"]\npred_points = quaternion_utils.quaternion_apply(\nprediction[\"orientation\"], pred_points\n)\npred_points += prediction[\"position\"]\nreturn gt_points, pred_points\ndef _finalize_metrics(self, method_name: str) -&gt; None:\n\"\"\"Finalize metrics after all samples have been evaluated.\n        Also writes results to disk and create plot if applicable.\n        \"\"\"\nresults_dir_path = os.path.join(self._out_dir_path, self._run_name)\nos.makedirs(results_dir_path, exist_ok=True)\nyaml_file_path = os.path.join(results_dir_path, \"results.yaml\")\nself._results_dict[method_name] = {}\nself._runtime_results_dict[method_name] = (\nself._runtime_data[\"total\"] / self._runtime_data[\"count\"]\n)\nfor metric_name, metric_dict in self._metrics.items():\nif \"position_thresholds\" in metric_dict:  # correctness metrics\ncorrect_counter = self._metric_data[metric_name][\"correct_counters\"]\ntotal_counter = self._metric_data[metric_name][\"total_counters\"]\ncorrect_percentage = correct_counter / total_counter\nself._results_dict[method_name][\nmetric_name\n] = correct_percentage.tolist()\nself._create_metric_plot(\nmethod_name,\nmetric_name,\nmetric_dict,\ncorrect_percentage,\nresults_dir_path,\n)\nelif \"pointwise_f\" in metric_dict:  # pointwise reconstruction metrics\ncounts = self._metric_data[metric_name][\"counts\"]\nm2s = self._metric_data[metric_name][\"m2s\"]\nmeans = self._metric_data[metric_name][\"means\"]\nvariances = m2s / counts\nstds = np.sqrt(variances)\nself._results_dict[method_name][metric_name] = {\n\"means\": means.tolist(),\n\"variances\": variances.tolist(),\n\"std\": stds.tolist(),\n}\nelse:\nraise NotImplementedError(\nf\"Unsupported metric configuration with name {metric_name}.\"\n)\nresults_dict = {\n**self._config,\n\"results\": self._results_dict,\n\"runtime_results\": self._runtime_results_dict,\n}\nyoco.save_config_to_file(yaml_file_path, results_dict)\nprint(f\"Results saved to: {yaml_file_path}\")\ndef _create_metric_plot(\nself,\nmethod_name: str,\nmetric_name: str,\nmetric_dict: dict,\ncorrect_percentage: np.ndarray,\nout_dir: str,\n) -&gt; None:\n\"\"\"Create metric plot if applicable.\n        Applicable means only one of the thresholds has multiple values.\n        Args:\n            correct_percentage:\n                Array holding the percentage of correct predictions.\n                Shape (NUM_POS_THRESH,NUM_DEG_THRESH,NUM_IOU_THRESH,NUM_CATEGORIES + 1).\n        \"\"\"\naxis = None\nfor i, s in enumerate(correct_percentage.shape[:4]):\nif s != 1 and axis is None:\naxis = i\nelif s != 1:  # multiple axis with != 1 size\nreturn\nif axis is None:\nreturn\naxis_to_threshold_key = {\n0: \"position_thresholds\",\n1: \"deg_thresholds\",\n2: \"iou_thresholds\",\n3: \"f_thresholds\",\n}\nthreshold_key = axis_to_threshold_key[axis]\nx_values = metric_dict[threshold_key]\nfor category_id in range(self._dataset.num_categories + 1):\ny_values = correct_percentage[..., category_id].flatten()\nif category_id in self._dataset.category_id_to_str:\nlabel = self._dataset.category_id_to_str[category_id]\nelse:\nlabel = \"all\"\nplt.plot(x_values, y_values, label=label)\nfigure_file_path = os.path.join(out_dir, f\"{method_name}_{metric_name}\")\nplt.xlabel(threshold_key)\nplt.ylabel(\"Correct\")\nplt.legend()\nplt.grid()\ntikzplotlib.save(figure_file_path + \".tex\")\nplt.savefig(figure_file_path + \".png\")\nplt.close()\ndef run(self) -&gt; None:\n\"\"\"Run the evaluation.\"\"\"\nself._results_dict = {}\nself._runtime_results_dict = {}\nfor method_name, method_wrapper in self._wrappers.items():\nself._eval_method(method_name, method_wrapper)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.__init__","title":"__init__","text":"<pre><code>__init__(config: dict) -&gt; None\n</code></pre> <p>Initialize model wrappers and evaluator.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n\"\"\"Initialize model wrappers and evaluator.\"\"\"\nself._parse_config(config)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the evaluation.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def run(self) -&gt; None:\n\"\"\"Run the evaluation.\"\"\"\nself._results_dict = {}\nself._runtime_results_dict = {}\nfor method_name, method_wrapper in self._wrappers.items():\nself._eval_method(method_name, method_wrapper)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.visualize_estimation","title":"visualize_estimation","text":"<pre><code>visualize_estimation(\ncolor_image: Optional[torch.Tensor] = None,\ndepth_image: Optional[torch.Tensor] = None,\ncamera: Optional[camera_utils.Camera] = None,\nlocal_cv_position: Optional[torch.Tensor] = None,\nlocal_cv_orientation_q: Optional[torch.Tensor] = None,\ninstance_mask: Optional[torch.Tensor] = None,\nextents: Optional[torch.Tensor] = None,\nreconstructed_points: Optional[torch.Tensor] = None,\nreconstructed_mesh: Optional[o3d.geometry.TriangleMesh] = None,\nvis_camera_json: Optional[str] = None,\nrender_options_json: Optional[str] = None,\nvis_file_path: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Visualize prediction and ask for confirmation.</p> PARAMETER DESCRIPTION <code>color_image</code> <p>The unmasked color image. Not visualized if None. Shape (H,W,3), RGB, 0-1, float.</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>depth_image</code> <p>The unmasked depth image visualized as a point set. Not visualized if None. Shape (H,W), float (meters along z).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>camera</code> <p>Camera used to project depth image to point set.</p> <p> TYPE: <code>Optional[camera_utils.Camera]</code> DEFAULT: <code>None</code> </p> <code>local_cv_position</code> <p>The position in the OpenCV camera frame. Not visualized if None. Shape (3,).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>local_cv_orientation_q</code> <p>The orientation in the OpenCV camera frame. Not visualized if None. Scalar last, shape (4,).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>extents</code> <p>Extents of the bounding box. Not visualized if None. Shape (3,).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>instance_mask</code> <p>The instance mask. No masking if None. Shape (H,W).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>reconstructed_points</code> <p>Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3).</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>reconstructed_mesh</code> <p>Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled.</p> <p> TYPE: <code>Optional[o3d.geometry.TriangleMesh]</code> DEFAULT: <code>None</code> </p> <code>vis_camera_json</code> <p>Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>vis_file_path</code> <p>If not None, the image will be rendered off screen and saved at the specified path.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>True if confirmation was positive. False if negative.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def visualize_estimation(\ncolor_image: Optional[torch.Tensor] = None,\ndepth_image: Optional[torch.Tensor] = None,\ncamera: Optional[camera_utils.Camera] = None,\nlocal_cv_position: Optional[torch.Tensor] = None,\nlocal_cv_orientation_q: Optional[torch.Tensor] = None,\ninstance_mask: Optional[torch.Tensor] = None,\nextents: Optional[torch.Tensor] = None,\nreconstructed_points: Optional[torch.Tensor] = None,\nreconstructed_mesh: Optional[o3d.geometry.TriangleMesh] = None,\nvis_camera_json: Optional[str] = None,\nrender_options_json: Optional[str] = None,\nvis_file_path: Optional[str] = None,\n) -&gt; None:\n\"\"\"Visualize prediction and ask for confirmation.\n    Args:\n        color_image:\n            The unmasked color image. Not visualized if None.\n            Shape (H,W,3), RGB, 0-1, float.\n        depth_image:\n            The unmasked depth image visualized as a point set. Not visualized if None.\n            Shape (H,W), float (meters along z).\n        camera: Camera used to project depth image to point set.\n        local_cv_position:\n            The position in the OpenCV camera frame. Not visualized if None. Shape (3,).\n        local_cv_orientation_q:\n            The orientation in the OpenCV camera frame. Not visualized if None.\n            Scalar last, shape (4,).\n        extents: Extents of the bounding box. Not visualized if None. Shape (3,).\n        instance_mask: The instance mask. No masking if None. Shape (H,W).\n        reconstructed_points:\n            Reconstructed points in object coordinate frame. Not visualized if None.\n            The points must already metrically scaled.\n            Shape (M,3).\n        reconstructed_mesh:\n            Reconstructed mesh in object coordinate frame. Not visualized if None.\n            The mesh must already metrically scaled.\n        vis_camera_json:\n            Path to open3d camera options json file that will be applied.\n            Generated by pressing p in desired view.\n            No render options will be applied if None.\n        vis_file_path:\n            If not None, the image will be rendered off screen and saved at the\n            specified path.\n    Returns:\n        True if confirmation was positive. False if negative.\n    \"\"\"\no3d_geometries = []\nif depth_image is not None:\nif instance_mask is not None:\nvalid_depth_mask = (depth_image != 0) * instance_mask\nelse:\nvalid_depth_mask = depth_image != 0\nmasked_pointset = pointset_utils.depth_to_pointcloud(\ndepth_image,\ncamera,\nnormalize=False,\nmask=instance_mask,\nconvention=\"opencv\",\n)\no3d_points = o3d.geometry.PointCloud(\npoints=o3d.utility.Vector3dVector(masked_pointset.cpu().numpy())\n)\nif color_image is not None:\npointset_colors = color_image[valid_depth_mask]\no3d_points.colors = o3d.utility.Vector3dVector(\npointset_colors.cpu().numpy()\n)\no3d_geometries.append(o3d_points)\n# coordinate frame\nif local_cv_position is not None:\nlocal_cv_position = local_cv_position.cpu().double().numpy()  # shape (3,)\nlocal_cv_orientation_q = (\nlocal_cv_orientation_q.cpu().double().numpy()\n)  # shape (4,)\nlocal_cv_orientation_m = Rotation.from_quat(local_cv_orientation_q).as_matrix()\no3d_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\no3d_frame.rotate(\nlocal_cv_orientation_m,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\no3d_frame.translate(local_cv_position[:, None])\no3d_geometries.append(o3d_frame)\no3d_cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\no3d_geometries.append(o3d_cam_frame)\nif extents is not None:\nextents = extents.cpu().double().numpy()\no3d_obb = o3d.geometry.OrientedBoundingBox(\ncenter=local_cv_position[:, None],\nR=local_cv_orientation_m,\nextent=extents[:, None],\n)\no3d_geometries.append(o3d_obb)\nif reconstructed_points is not None and reconstructed_mesh is None:\no3d_rec_points = o3d.geometry.PointCloud(\npoints=o3d.utility.Vector3dVector(reconstructed_points.cpu().numpy())\n)\no3d_rec_points.rotate(\nlocal_cv_orientation_m,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\no3d_rec_points.translate(local_cv_position[:, None])\no3d_geometries.append(o3d_rec_points)\nif reconstructed_mesh is not None:\n# copy the mesh to keep original unmoved\nposed_mesh = o3d.geometry.TriangleMesh(reconstructed_mesh)\nposed_mesh.rotate(\nlocal_cv_orientation_m,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\nposed_mesh.translate(local_cv_position[:, None])\nposed_mesh.compute_vertex_normals()\no3d_geometries.append(posed_mesh)\nvis = o3d.visualization.Visualizer()\nif vis_camera_json is not None:\nvis_camera = o3d.io.read_pinhole_camera_parameters(vis_camera_json)\nwidth = vis_camera.intrinsic.width\nheight = vis_camera.intrinsic.height\nelse:\nwidth = 800\nheight = 600\nvis_camera = None\nvis.create_window(width=width, height=height, visible=(vis_file_path is None))\nfor g in o3d_geometries:\nvis.add_geometry(g)\nif vis_camera is not None:\nview_control = vis.get_view_control()\nview_control.convert_from_pinhole_camera_parameters(vis_camera)\nif render_options_json is not None:\nrender_option = vis.get_render_option()\nrender_option.load_from_json(render_options_json)\nif vis_file_path is not None:\nvis.poll_events()\nvis.update_renderer()\nvis.capture_screen_image(vis_file_path, do_render=True)\nelse:\nvis.run()\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point of the evaluation program.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def main() -&gt; None:\n\"\"\"Entry point of the evaluation program.\"\"\"\nparser = argparse.ArgumentParser(\ndescription=\"Pose and shape estimation evaluation on REAL275 data\"\n)\nparser.add_argument(\"--config\", required=True, nargs=\"+\")\nparser.add_argument(\"--out_dir\", required=True)\nconfig = yoco.load_config_from_args(\nparser,\nsearch_paths=[\n\".\",\n\"~/.cpas_toolbox\",\nos.path.join(os.path.dirname(__file__), \"config\"),\nos.path.dirname(__file__),\n],\n)\nevaluator = Evaluator(config)\nevaluator.run()\n</code></pre>"},{"location":"api_reference/metrics/","title":"metrics.py","text":""},{"location":"api_reference/metrics/#cpas_toolbox.metrics","title":"cpas_toolbox.metrics","text":"<p>Metrics for shape evaluation.</p>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.diameter","title":"diameter","text":"<pre><code>diameter(points: np.ndarray) -&gt; float\n</code></pre> <p>Compute largest Euclidean distance between any two points.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true</p> <p> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> </p> RETURNS DESCRIPTION <code>float</code> <p>Ratio of reconstructed points with closest ground truth point closer than</p> <code>float</code> <p>threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def diameter(points: np.ndarray) -&gt; float:\n\"\"\"Compute largest Euclidean distance between any two points.\n    Args:\n        points_gt: set of true\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\ntry:\nhull = scipy.spatial.ConvexHull(points)\nexcept scipy.spatial.qhull.QhullError:\n# fallback to brute force distance matrix\nreturn np.max(scipy.spatial.distance_matrix(points, points))\n# this is wasteful, if too slow implement rotating caliper method\nreturn np.max(\nscipy.spatial.distance_matrix(points[hull.vertices], points[hull.vertices])\n)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_accuracy","title":"mean_accuracy","text":"<pre><code>mean_accuracy(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute accuracy metric.</p> <p>Accuracy metric is the same as the mean pointwise distance (or asymmetric chamfer distance) from rec to gt.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)</p> <code>float</code> <p>ground truth points.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def mean_accuracy(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute accuracy metric.\n    Accuracy metric is the same as the mean pointwise distance (or asymmetric chamfer\n    distance) from rec to gt.\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)\n        ground truth points.\n    \"\"\"\nkd_tree = scipy.spatial.KDTree(points_gt)\nd, _ = kd_tree.query(points_rec, p=p_norm)\nif normalize:\nreturn np.mean(d) / diameter(points_gt)\nelse:\nreturn np.mean(d)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_completeness","title":"mean_completeness","text":"<pre><code>mean_completeness(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute completeness metric.</p> <p>Completeness metric is the same as the mean pointwise distance (or asymmetric chamfer distance) from gt to rec.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Arithmetic mean of p-norm from ground truth points to closest (in p-norm)</p> <code>float</code> <p>reconstructed points.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def mean_completeness(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute completeness metric.\n    Completeness metric is the same as the mean pointwise distance (or asymmetric\n    chamfer distance) from gt to rec.\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from ground truth points to closest (in p-norm)\n        reconstructed points.\n    \"\"\"\nkd_tree = scipy.spatial.KDTree(points_rec)\nd, _ = kd_tree.query(points_gt, p=p_norm)\nif normalize:\nreturn np.mean(d) / diameter(points_gt)\nelse:\nreturn np.mean(d)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.symmetric_chamfer","title":"symmetric_chamfer","text":"<pre><code>symmetric_chamfer(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute symmetric chamfer distance.</p> <p>There are various slightly different definitions for the chamfer distance.</p> <p>Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two.</p> <p>Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Arithmetic mean of accuracy and completeness metrics using the specified p-norm.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def symmetric_chamfer(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute symmetric chamfer distance.\n    There are various slightly different definitions for the chamfer distance.\n    Note that completeness and accuracy are themselves sometimes referred to as\n    chamfer distances, with symmetric chamfer distance being the combination of the two.\n    Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D\n    Reconstruction in Function Space, Mescheder et al., 2019) refers to using\n    arithmetic mean (note that this is actually differently scaled from L1) when\n    combining accuracy and completeness.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of accuracy and completeness metrics using the specified p-norm.\n    \"\"\"\nreturn (\nmean_completeness(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n+ mean_accuracy(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n) / 2\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.normalized_average_distance","title":"normalized_average_distance","text":"<pre><code>normalized_average_distance(\npoints_gt: np.ndarray, points_rec: np.ndarray, p_norm: int = 2\n) -&gt; float\n</code></pre> <p>Compute the maximum of the directed normalized average distances.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Maximum of normalized mean accuracy and mean completeness metrics using the</p> <code>float</code> <p>specified p-norm.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def normalized_average_distance(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\np_norm: int = 2,\n) -&gt; float:\n\"\"\"Compute the maximum of the directed normalized average distances.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n    Returns:\n        Maximum of normalized mean accuracy and mean completeness metrics using the\n        specified p-norm.\n    \"\"\"\nreturn max(\nmean_completeness(points_gt, points_rec, p_norm=p_norm, normalize=True),\nmean_accuracy(points_gt, points_rec, p_norm=p_norm, normalize=True),\n)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.completeness_thresh","title":"completeness_thresh","text":"<pre><code>completeness_thresh(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute thresholded completion metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Ratio of ground truth points with closest reconstructed point closer than</p> <code>float</code> <p>threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def completeness_thresh(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute thresholded completion metric.\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of ground truth points with closest reconstructed point closer than\n        threshold (in p-norm).\n    \"\"\"\nkd_tree = scipy.spatial.KDTree(points_rec)\nd, _ = kd_tree.query(points_gt, p=p_norm)\nif normalize:\nreturn np.sum(d / diameter(points_gt) &lt; threshold) / points_gt.shape[0]\nelse:\nreturn np.sum(d &lt; threshold) / points_gt.shape[0]\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.accuracy_thresh","title":"accuracy_thresh","text":"<pre><code>accuracy_thresh(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute thresholded accuracy metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Ratio of reconstructed points with closest ground truth point closer than</p> <code>float</code> <p>threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def accuracy_thresh(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute thresholded accuracy metric.\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\nkd_tree = scipy.spatial.KDTree(points_gt)\nd, _ = kd_tree.query(points_rec, p=p_norm)\nif normalize:\nreturn np.sum(d / diameter(points_gt) &lt; threshold) / points_rec.shape[0]\nelse:\nreturn np.sum(d &lt; threshold) / points_rec.shape[0]\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.reconstruction_fscore","title":"reconstruction_fscore","text":"<pre><code>reconstruction_fscore(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute reconstruction fscore.</p> <p>See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019</p> PARAMETER DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>np.ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Harmonic mean of precision (thresholded accuracy) and recall (thresholded</p> <code>float</code> <p>completeness).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def reconstruction_fscore(\npoints_gt: np.ndarray,\npoints_rec: np.ndarray,\nthreshold: float,\np_norm: int = 2,\nnormalize: bool = False,\n) -&gt; float:\n\"\"\"Compute reconstruction fscore.\n    See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Harmonic mean of precision (thresholded accuracy) and recall (thresholded\n        completeness).\n    \"\"\"\nrecall = completeness_thresh(\npoints_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n)\nprecision = accuracy_thresh(\npoints_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n)\nif recall &lt; 1e-7 or precision &lt; 1e-7:\nreturn 0\nreturn 2 / (1 / recall + 1 / precision)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d_sampling","title":"iou_3d_sampling","text":"<pre><code>iou_3d_sampling(\np1: np.ndarray,\nr1: Rotation,\ne1: np.ndarray,\np2: np.ndarray,\nr2: Rotation,\ne2: np.ndarray,\nnum_points: int = 10000,\n) -&gt; float\n</code></pre> <p>Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box.</p> PARAMETER DESCRIPTION <code>p1</code> <p>Center position of first bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>r1</code> <p>Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e1</code> <p>Extents (i.e., side lengths) of first bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>p2</code> <p>Center position of second bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>r2</code> <p>Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e2</code> <p>Extents (i.e., side lengths) of second bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>num_points</code> <p>Number of points to sample in smaller bounding box.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Approximate intersection-over-union for the two oriented bounding boxes.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def iou_3d_sampling(\np1: np.ndarray,\nr1: Rotation,\ne1: np.ndarray,\np2: np.ndarray,\nr2: Rotation,\ne2: np.ndarray,\nnum_points: int = 10000,\n) -&gt; float:\n\"\"\"Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box.\n    Args:\n        p1: Center position of first bounding box, shape (3,).\n        r1: Orientation of first bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e1: Extents (i.e., side lengths) of first bounding box, shape (3,).\n        p2: Center position of second bounding box, shape (3,).\n        r2: Orientation of second bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e2: Extents (i.e., side lengths) of second bounding box, shape (3,).\n        num_points: Number of points to sample in smaller bounding box.\n    Returns:\n        Approximate intersection-over-union for the two oriented bounding boxes.\n    \"\"\"\n# sample smaller volume to estimate intersection\nvol_1 = np.prod(e1)\nvol_2 = np.prod(e2)\nif vol_1 &lt; vol_2:\npoints_1_in_1 = e1 * np.random.rand(num_points, 3) - e1 / 2\npoints_1_in_w = r1.apply(points_1_in_1) + p1\npoints_1_in_2 = r2.inv().apply(points_1_in_w - p2)\nratio_1_in_2 = (\nnp.sum(\nnp.all(points_1_in_2 &lt; e2 / 2, axis=1)\n* np.all(-e2 / 2 &lt; points_1_in_2, axis=1)\n)\n/ num_points\n)\nintersection = ratio_1_in_2 * vol_1\nelse:\npoints_2_in_2 = e2 * np.random.rand(num_points, 3) - e2 / 2\npoints_2_in_w = r2.apply(points_2_in_2) + p2\npoints_2_in_1 = r1.inv().apply(points_2_in_w - p1)\nratio_2_in_1 = (\nnp.sum(\nnp.all(points_2_in_1 &lt; e1 / 2, axis=1)\n* np.all(-e1 / 2 &lt; points_2_in_1, axis=1)\n)\n/ num_points\n)\nintersection = ratio_2_in_1 * vol_2\nunion = vol_1 + vol_2 - intersection\nreturn intersection / union\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d","title":"iou_3d","text":"<pre><code>iou_3d(\np1: np.ndarray,\nr1: Rotation,\ne1: np.ndarray,\np2: np.ndarray,\nr2: Rotation,\ne2: np.ndarray,\n) -&gt; float\n</code></pre> <p>Compute 3D IoU of oriented bounding boxes analytically.</p> <p>Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm.</p> PARAMETER DESCRIPTION <code>p1</code> <p>Center position of first bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>r1</code> <p>Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e1</code> <p>Extents (i.e., side lengths) of first bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>p2</code> <p>Center position of second bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>r2</code> <p>Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e2</code> <p>Extents (i.e., side lengths) of second bounding box, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Accurate intersection-over-union for the two oriented bounding boxes.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def iou_3d(\np1: np.ndarray,\nr1: Rotation,\ne1: np.ndarray,\np2: np.ndarray,\nr2: Rotation,\ne2: np.ndarray,\n) -&gt; float:\n\"\"\"Compute 3D IoU of oriented bounding boxes analytically.\n    Code partly based on https://github.com/google-research-datasets/Objectron/.\n    Implementation uses HalfSpace intersection instead of Sutherland-Hodgman algorithm.\n    Args:\n        p1: Center position of first bounding box, shape (3,).\n        r1: Orientation of first bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e1: Extents (i.e., side lengths) of first bounding box, shape (3,).\n        p2: Center position of second bounding box, shape (3,).\n        r2: Orientation of second bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e2: Extents (i.e., side lengths) of second bounding box, shape (3,).\n    Returns:\n        Accurate intersection-over-union for the two oriented bounding boxes.\n    \"\"\"\n# create halfspaces\nhalfspaces = np.zeros((12, 4))\nhalfspaces[0:3, 0:3] = r1.as_matrix().T\nhalfspaces[0:3, 3] = -halfspaces[0:3, 0:3] @ (r1.apply(e1 / 2) + p1)\nhalfspaces[3:6, 0:3] = -halfspaces[0:3, 0:3]\nhalfspaces[3:6, 3] = -halfspaces[3:6, 0:3] @ (r1.apply(-e1 / 2) + p1)\nhalfspaces[6:9, 0:3] = r2.as_matrix().T\nhalfspaces[6:9, 3] = -halfspaces[6:9, 0:3] @ (r2.apply(e2 / 2) + p2)\nhalfspaces[9:12, 0:3] = -halfspaces[6:9, 0:3]\nhalfspaces[9:12, 3] = -halfspaces[9:12, 0:3] @ (r2.apply(-e2 / 2) + p2)\n# try to find point inside both bounding boxes\ninside_point = _find_inside_point(p1, r1, e1, p2, r2, e2, halfspaces)\nif inside_point is None:\nreturn 0\n# create halfspace intersection and compute IoU\nhs = scipy.spatial.HalfspaceIntersection(halfspaces, inside_point)\nch = scipy.spatial.ConvexHull(hs.intersections)\nintersection = ch.volume\nvol_1 = np.prod(e1)\nvol_2 = np.prod(e2)\nunion = vol_1 + vol_2 - intersection\nreturn intersection / union\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.correct_thresh","title":"correct_thresh","text":"<pre><code>correct_thresh(\nposition_gt: np.ndarray,\nposition_prediction: np.ndarray,\norientation_gt: Rotation,\norientation_prediction: Rotation,\nextent_gt: Optional[np.ndarray] = None,\nextent_prediction: Optional[np.ndarray] = None,\npoints_gt: Optional[np.ndarray] = None,\npoints_prediction: Optional[np.ndarray] = None,\nposition_threshold: Optional[float] = None,\ndegree_threshold: Optional[float] = None,\niou_3d_threshold: Optional[float] = None,\nfscore_threshold: Optional[float] = None,\nnad_threshold: Optional[float] = None,\nrotational_symmetry_axis: Optional[int] = None,\n) -&gt; int\n</code></pre> <p>Classify a pose prediction as correct or incorrect.</p> PARAMETER DESCRIPTION <code>position_gt</code> <p>Ground truth position, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>position_prediction</code> <p>Predicted position, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>position_threshold</code> <p>Position threshold in meters, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>orientation_gt</code> <p>Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>orientation_prediction</code> <p>Predicted orientation. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>extent_gt</code> <p>Bounding box extents, shape (3,). Only used if IoU threshold specified.</p> <p> TYPE: <code>Optional[np.ndarray]</code> DEFAULT: <code>None</code> </p> <code>extent_prediction</code> <p>Bounding box extents, shape (3,). Only used if IoU threshold specified.</p> <p> TYPE: <code>Optional[np.ndarray]</code> DEFAULT: <code>None</code> </p> <code>point_gt</code> <p>Set of true points, shape (N,3).</p> <p> </p> <code>points_rec</code> <p>Set of reconstructed points, shape (M,3).</p> <p> </p> <code>degree_threshold</code> <p>Orientation threshold in degrees, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>iou_3d_threshold</code> <p>3D IoU threshold, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>nad_threshold</code> <p>Normalized average distance thresold, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>rotational_symmetry_axis</code> <p>Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>1 if error is below all provided thresholds.  0 if error is above one provided</p> <code>int</code> <p>threshold.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def correct_thresh(\nposition_gt: np.ndarray,\nposition_prediction: np.ndarray,\norientation_gt: Rotation,\norientation_prediction: Rotation,\nextent_gt: Optional[np.ndarray] = None,\nextent_prediction: Optional[np.ndarray] = None,\npoints_gt: Optional[np.ndarray] = None,\npoints_prediction: Optional[np.ndarray] = None,\nposition_threshold: Optional[float] = None,\ndegree_threshold: Optional[float] = None,\niou_3d_threshold: Optional[float] = None,\nfscore_threshold: Optional[float] = None,\nnad_threshold: Optional[float] = None,\nrotational_symmetry_axis: Optional[int] = None,\n) -&gt; int:\n\"\"\"Classify a pose prediction as correct or incorrect.\n    Args:\n        position_gt: Ground truth position, shape (3,).\n        position_prediction: Predicted position, shape (3,).\n        position_threshold: Position threshold in meters, no threshold if None.\n        orientation_gt:\n            Ground truth orientation.\n            This is the rotation that rotates points from bounding box to camera frame.\n        orientation_prediction:\n            Predicted orientation.\n            This is the rotation that rotates points from bounding box to camera frame.\n        extent_gt:\n            Bounding box extents, shape (3,).\n            Only used if IoU threshold specified.\n        extent_prediction:\n            Bounding box extents, shape (3,).\n            Only used if IoU threshold specified.\n        point_gt: Set of true points, shape (N,3).\n        points_rec: Set of reconstructed points, shape (M,3).\n        degree_threshold: Orientation threshold in degrees, no threshold if None.\n        iou_3d_threshold: 3D IoU threshold, no threshold if None.\n        nad_threshold: Normalized average distance thresold, no threshold if None.\n        rotational_symmetry_axis:\n            Specify axis along which rotation is ignored. If None, no axis is ignored.\n            0 for x-axis, 1 for y-axis, 2 for z-axis.\n    Returns:\n        1 if error is below all provided thresholds.  0 if error is above one provided\n        threshold.\n    \"\"\"\nif position_threshold is not None:\nposition_error = np.linalg.norm(position_gt - position_prediction)\nif position_error &gt; position_threshold:\nreturn 0\nif degree_threshold is not None:\nrad_threshold = degree_threshold * np.pi / 180.0\nif rotational_symmetry_axis is not None:\np = np.array([0.0, 0.0, 0.0])\np[rotational_symmetry_axis] = 1.0\np1 = orientation_gt.apply(p)\np2 = orientation_prediction.apply(p)\nrad_error = np.arccos(p1 @ p2)\nelse:\nrad_error = (orientation_gt * orientation_prediction.inv()).magnitude()\nif rad_error &gt; rad_threshold:\nreturn 0\nif iou_3d_threshold is not None:\nif rotational_symmetry_axis is not None:\nmax_iou = 0\nfor r in np.linspace(0, np.pi, 100):\np = np.array([0.0, 0.0, 0.0])\np[rotational_symmetry_axis] = 1.0\np *= r\nsym_rot = Rotation.from_rotvec(r)\niou = iou_3d(\nposition_gt,\norientation_gt,\nextent_gt,\nposition_prediction,\norientation_prediction * sym_rot,\nextent_prediction,\n)\nmax_iou = max(iou, max_iou)\niou = max_iou\nelse:\niou = iou_3d(\nposition_gt,\norientation_gt,\nextent_gt,\nposition_prediction,\norientation_prediction,\nextent_prediction,\n)\nif iou &lt; iou_3d_threshold:\nreturn 0\nif fscore_threshold is not None:\n# TODO make 0.01 a parameter\nfscore = reconstruction_fscore(points_gt, points_prediction, 0.01)\nif fscore &lt; fscore_threshold:\nreturn 0\nif nad_threshold is not None:\nnad = normalized_average_distance(points_gt, points_prediction)\nif nad &lt; nad_threshold:\nreturn 0\nreturn 1\n</code></pre>"},{"location":"api_reference/pointset_utils/","title":"pointset_utils.py","text":""},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils","title":"cpas_toolbox.pointset_utils","text":"<p>Utility functions to handle pointsets.</p>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.normalize_points","title":"normalize_points","text":"<pre><code>normalize_points(points: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize pointset to have zero mean.</p> <p>Normalization will be performed along second last dimension.</p> PARAMETER DESCRIPTION <code>points</code> <p>The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.</p> <p> TYPE: <code>torch.Tensor</code> </p> Return <p>normalized_points:     The normalized pointset, same shape as points. centroids:     The means of the pointclouds used to normalize points.     Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def normalize_points(points: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Normalize pointset to have zero mean.\n    Normalization will be performed along second last dimension.\n    Args:\n        points:\n            The pointsets which will be normalized,\n            shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.\n    Return:\n        normalized_points:\n            The normalized pointset, same shape as points.\n        centroids:\n            The means of the pointclouds used to normalize points.\n            Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.\n    \"\"\"\ncentroids = torch.mean(points, dim=-2, keepdim=True)\nnormalized_points = points - centroids\nreturn normalized_points, centroids.squeeze()\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.depth_to_pointcloud","title":"depth_to_pointcloud","text":"<pre><code>depth_to_pointcloud(\ndepth_image: torch.Tensor,\ncamera: camera_utils.Camera,\nnormalize: bool = False,\nmask: Optional[torch.Tensor] = None,\nconvention: str = \"opengl\",\n) -&gt; torch.Tensor\n</code></pre> <p>Convert depth image to pointcloud.</p> PARAMETER DESCRIPTION <code>depth_image</code> <p>The depth image to convert to pointcloud, shape (H,W).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>camera</code> <p>The camera used to lift the points.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> <code>normalize</code> <p>Whether to normalize the pointcloud with 0 centroid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mask</code> <p>Only points with mask != 0 will be added to pointcloud. No masking will be performed if None.</p> <p> TYPE: <code>Optional[torch.Tensor]</code> DEFAULT: <code>None</code> </p> <code>convention</code> <p>The camera frame convention to use. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward</p> <p> TYPE: <code>str</code> DEFAULT: <code>'opengl'</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>The pointcloud in the camera frame, in OpenGL convention, shape (N,3).</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def depth_to_pointcloud(\ndepth_image: torch.Tensor,\ncamera: camera_utils.Camera,\nnormalize: bool = False,\nmask: Optional[torch.Tensor] = None,\nconvention: str = \"opengl\",\n) -&gt; torch.Tensor:\n\"\"\"Convert depth image to pointcloud.\n    Args:\n        depth_image: The depth image to convert to pointcloud, shape (H,W).\n        camera: The camera used to lift the points.\n        normalize: Whether to normalize the pointcloud with 0 centroid.\n        mask:\n            Only points with mask != 0 will be added to pointcloud.\n            No masking will be performed if None.\n        convention:\n            The camera frame convention to use. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n    Returns:\n        The pointcloud in the camera frame, in OpenGL convention, shape (N,3).\n    \"\"\"\nfx, fy, cx, cy, _ = camera.get_pinhole_camera_parameters(0.0)\nif mask is None:\nindices = torch.nonzero(depth_image, as_tuple=True)\nelse:\nindices = torch.nonzero(depth_image * mask, as_tuple=True)\ndepth_values = depth_image[indices]\npoints = torch.cat(\n(\nindices[1][:, None].float(),\nindices[0][:, None].float(),\ndepth_values[:, None],\n),\ndim=1,\n)\nif convention == \"opengl\":\nfinal_points = torch.empty_like(points)\nfinal_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\nfinal_points[:, 1] = -(points[:, 1] - cy) * points[:, 2] / fy\nfinal_points[:, 2] = -points[:, 2]\nelif convention == \"opencv\":\nfinal_points = torch.empty_like(points)\nfinal_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\nfinal_points[:, 1] = (points[:, 1] - cy) * points[:, 2] / fy\nfinal_points[:, 2] = points[:, 2]\nelse:\nraise ValueError(f\"Unsupported camera convention {convention}.\")\nif normalize:\nfinal_points, _ = normalize_points(final_points)\nreturn final_points\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_transform_camera_convention","title":"change_transform_camera_convention","text":"<pre><code>change_transform_camera_convention(\nin_transform: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; torch.Tensor\n</code></pre> <p>Change the camera convention for a frame A -&gt; camera frame transform.</p> PARAMETER DESCRIPTION <code>in_transform</code> <p>Transformtion matrix(es) from coordinate frame A to in_convention camera frame.  Shape (...,4,4).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>Transformtion matrix(es) from coordinate frame A to out_convention camera frame.</p> <code>torch.Tensor</code> <p>Same shape as in_transform.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_transform_camera_convention(\nin_transform: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; torch.Tensor:\n\"\"\"Change the camera convention for a frame A -&gt; camera frame transform.\n    Args:\n        in_transform:\n            Transformtion matrix(es) from coordinate frame A to in_convention camera\n            frame.  Shape (...,4,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Transformtion matrix(es) from coordinate frame A to out_convention camera frame.\n        Same shape as in_transform.\n    \"\"\"\n# check whether valild convention was provided\nif in_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"In camera convention {in_convention} not supported.\")\nif out_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"Out camera convention {in_convention} not supported.\")\nif in_convention == out_convention:\nreturn in_transform\nelse:\ngl2cv_transform = torch.diag(\nin_transform.new_tensor([1.0, -1.0, -1.0, 1.0])\n)  # == cv2gl_transform\nreturn gl2cv_transform @ in_transform\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_position_camera_convention","title":"change_position_camera_convention","text":"<pre><code>change_position_camera_convention(\nin_position: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; tuple\n</code></pre> <p>Change the camera convention for a position in a camera frame.</p> PARAMETER DESCRIPTION <code>in_position</code> <p>Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_position. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_position_camera_convention(\nin_position: torch.Tensor,\nin_convention: str,\nout_convention: str,\n) -&gt; tuple:\n\"\"\"Change the camera convention for a position in a camera frame.\n    Args:\n        in_position:\n            Position(s) of coordinate frame A in in_convention camera frame.\n            Shape (...,3).\n        in_convention:\n            Camera convention for the in_position. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).\n    \"\"\"\n# check whether valild convention was provided\nif in_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"In camera convention {in_convention} not supported.\")\nif out_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"Out camera convention {in_convention} not supported.\")\nif in_convention == out_convention:\nreturn in_position\nelse:\ngl2cv = in_position.new_tensor([1.0, -1.0, -1.0])  # == cv2gl\nreturn gl2cv * in_position\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_orientation_camera_convention","title":"change_orientation_camera_convention","text":"<pre><code>change_orientation_camera_convention(\nin_orientation_q: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; tuple\n</code></pre> <p>Change the camera convention for an orientation in a camera frame.</p> <p>Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin).</p> PARAMETER DESCRIPTION <code>in_orientation_q</code> <p>Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Quaternion(s) which transforms from coordinate frame A to in_convention camera</p> <code>tuple</code> <p>frame. Scalar-last convention. Same shape as in_orientation_q.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_orientation_camera_convention(\nin_orientation_q: torch.Tensor,\nin_convention: str,\nout_convention: str,\n) -&gt; tuple:\n\"\"\"Change the camera convention for an orientation in a camera frame.\n    Orientation is represented as a quaternion, that rotates points from a\n    coordinate frame A to a camera frame (if those frames had the same origin).\n    Args:\n        in_orientation_q:\n            Quaternion(s) which transforms from coordinate frame A to in_convention\n            camera frame. Scalar-last convention. Shape (...,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n    Returns:\n        Quaternion(s) which transforms from coordinate frame A to in_convention camera\n        frame. Scalar-last convention. Same shape as in_orientation_q.\n    \"\"\"\n# check whether valild convention was provided\nif in_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"In camera convention {in_convention} not supported.\")\nif out_convention not in [\"opengl\", \"opencv\"]:\nraise ValueError(f\"Out camera convention {in_convention} not supported.\")\nif in_convention == out_convention:\nreturn in_orientation_q\nelse:\n# rotate 180deg around x direction\ngl2cv_q = in_orientation_q.new_tensor([1.0, 0, 0, 0])  # == cv2gl\nreturn quaternion_utils.quaternion_multiply(gl2cv_q, in_orientation_q)\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.visualize_pointset","title":"visualize_pointset","text":"<pre><code>visualize_pointset(pointset: torch.Tensor, max_points: int = 1000) -&gt; None\n</code></pre> <p>Visualize pointset as 3D scatter plot.</p> PARAMETER DESCRIPTION <code>pointset</code> <p>The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>max_points</code> <p>Maximum number of points. If N&gt;max_points only a random subset will be shown.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def visualize_pointset(pointset: torch.Tensor, max_points: int = 1000) -&gt; None:\n\"\"\"Visualize pointset as 3D scatter plot.\n    Args:\n        pointset:\n            The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.\n        max_points:\n            Maximum number of points.\n            If N&gt;max_points only a random subset will be shown.\n    \"\"\"\npointset_np = pointset.cpu().detach().numpy()\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nif len(pointset_np) &gt; max_points:\nindices = np.random.choice(len(pointset_np), replace=False, size=max_points)\npointset_np = pointset_np[indices]\nif pointset_np.shape[1] == 6:\ncolors = pointset_np[:, 3:]\nelse:\ncolors = None\nax.scatter(pointset_np[:, 0], pointset_np[:, 1], pointset_np[:, 2], c=colors)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\")\nax.set_box_aspect(pointset_np.max(axis=0) - pointset_np.min(axis=0))\nplt.show()\n</code></pre>"},{"location":"api_reference/quaternion_utils/","title":"quaternion_utils.py","text":""},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils","title":"cpas_toolbox.quaternion_utils","text":"<p>Functions to handle transformations with quaternions.</p> <p>Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar &gt; 0. https://github.com/facebookresearch/pytorch3d</p>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_multiply","title":"quaternion_multiply","text":"<pre><code>quaternion_multiply(\nquaternions_1: torch.Tensor, quaternions_2: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Multiply two quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> PARAMETER DESCRIPTION <code>quaternions_1</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>quaternions_2</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>torch.Tensor</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>Composition of passed quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_multiply(\nquaternions_1: torch.Tensor, quaternions_2: torch.Tensor\n) -&gt; torch.Tensor:\n\"\"\"Multiply two quaternions representing rotations.\n    Normal broadcasting rules apply.\n    Args:\n        quaternions_1:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        quaternions_2:\n            normalized quaternions of shape (..., 4), scalar-last convention\n    Returns:\n        Composition of passed quaternions.\n    \"\"\"\nax, ay, az, aw = torch.unbind(quaternions_1, -1)\nbx, by, bz, bw = torch.unbind(quaternions_2, -1)\nox = aw * bx + ax * bw + ay * bz - az * by\noy = aw * by - ax * bz + ay * bw + az * bx\noz = aw * bz + ax * by - ay * bx + az * bw\now = aw * bw - ax * bx - ay * by - az * bz\nreturn torch.stack((ox, oy, oz, ow), -1)\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_apply","title":"quaternion_apply","text":"<pre><code>quaternion_apply(\nquaternions: torch.Tensor, points: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Rotate points by quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> PARAMETER DESCRIPTION <code>quaternions</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>points</code> <p>points of shape (..., 3)</p> <p> TYPE: <code>torch.Tensor</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>Points rotated by the rotations representing quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_apply(quaternions: torch.Tensor, points: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Rotate points by quaternions representing rotations.\n    Normal broadcasting rules apply.\n    Args:\n        quaternions:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        points:\n            points of shape (..., 3)\n    Returns:\n        Points rotated by the rotations representing quaternions.\n    \"\"\"\npoints_as_quaternions = points.new_zeros(points.shape[:-1] + (4,))\npoints_as_quaternions[..., :-1] = points\nreturn quaternion_multiply(\nquaternion_multiply(quaternions, points_as_quaternions),\nquaternion_invert(quaternions),\n)[..., :-1]\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_invert","title":"quaternion_invert","text":"<pre><code>quaternion_invert(quaternions: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Invert quaternions representing orientations.</p> PARAMETER DESCRIPTION <code>quaternions</code> <p>The quaternions to invert, shape (..., 4), scalar-last convention.</p> <p> TYPE: <code>torch.Tensor</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>Inverted quaternions, same shape as quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_invert(quaternions: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Invert quaternions representing orientations.\n    Args:\n        quaternions:\n            The quaternions to invert, shape (..., 4), scalar-last convention.\n    Returns:\n        Inverted quaternions, same shape as quaternions.\n    \"\"\"\nreturn quaternions * quaternions.new_tensor([-1, -1, -1, 1])\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.geodesic_distance","title":"geodesic_distance","text":"<pre><code>geodesic_distance(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute geodesic distances between quaternions.</p> PARAMETER DESCRIPTION <code>q1</code> <p>First set of quaterions, shape (N,4).</p> <p> TYPE: <code>torch.Tensor</code> </p> <code>q2</code> <p>Second set of quaternions, shape (N,4).</p> <p> TYPE: <code>torch.Tensor</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> <p>Mean distance between the quaternions, scalar.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def geodesic_distance(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute geodesic distances between quaternions.\n    Args:\n        q1: First set of quaterions, shape (N,4).\n        q2: Second set of quaternions, shape (N,4).\n    Returns:\n        Mean distance between the quaternions, scalar.\n    \"\"\"\nabs_q1q2 = torch.clip(torch.abs(torch.sum(q1 * q2, dim=1)), 0, 1)\ngeodesic_distances = 2 * torch.acos(abs_q1q2)\nreturn geodesic_distances\n</code></pre>"},{"location":"api_reference/utils/","title":"utils.py","text":""},{"location":"api_reference/utils/#cpas_toolbox.utils","title":"cpas_toolbox.utils","text":"<p>This module provides miscellaneous utility functions.</p>"},{"location":"api_reference/utils/#cpas_toolbox.utils.str_to_object","title":"str_to_object","text":"<pre><code>str_to_object(name: str) -&gt; Any\n</code></pre> <p>Try to find object with a given name.</p> <p>First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the object to resolve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The object which the provided name refers to. None if no object was found.</p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def str_to_object(name: str) -&gt; Any:\n\"\"\"Try to find object with a given name.\n    First scope of calling function is checked for the name, then current environment\n    (in which case name has to be a fully qualified name). In the second case, the\n    object is imported if found.\n    Args:\n        name: Name of the object to resolve.\n    Returns:\n        The object which the provided name refers to. None if no object was found.\n    \"\"\"\n# check callers local variables\ncaller_locals = inspect.currentframe().f_back.f_locals\nif name in caller_locals:\nreturn caller_locals[name]\n# check callers global variables (i.e., imported modules etc.)\ncaller_globals = inspect.currentframe().f_back.f_globals\nif name in caller_globals:\nreturn caller_globals[name]\n# check environment\nreturn locate(name)\n</code></pre>"},{"location":"api_reference/utils/#cpas_toolbox.utils.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(path: str, search_paths: Optional[List[str]] = None) -&gt; str\n</code></pre> <p>Resolves a path to a full absolute path based on search_paths.</p> <p>This function considers paths of 5 different cases     /... -&gt; absolute path, nothing todo     ~/... -&gt; home dir, expand user     ./... -&gt; relative to current directory     ../... -&gt; relative to current parent directory     ... -&gt; relative to search paths</p> <p>Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists.</p> <p>Returns original path, if file does not exist.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to resolve.</p> <p> TYPE: <code>str</code> </p> <code>search_paths</code> <p>List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def resolve_path(path: str, search_paths: Optional[List[str]] = None) -&gt; str:\n\"\"\"Resolves a path to a full absolute path based on search_paths.\n    This function considers paths of 5 different cases\n        /... -&gt; absolute path, nothing todo\n        ~/... -&gt; home dir, expand user\n        ./... -&gt; relative to current directory\n        ../... -&gt; relative to current parent directory\n        ... -&gt; relative to search paths\n    Current directory is not implicitly included in search paths and has to be added\n    with \".\" if desired. Search paths are handled in first to last order and considered\n    correct if file or directory exists.\n    Returns original path, if file does not exist.\n    Args:\n        path: The path to resolve.\n        search_paths:\n            List of search paths to prepend relative paths which are not explicitly\n            relative to current directory.\n            If None, no search paths are assumed.\n    \"\"\"\nif search_paths is None:\nsearch_paths = []\nif os.path.isabs(path):\nreturn path\nparts = path.split(os.sep)\nif parts[0] in [\".\", \"..\"]:\nreturn os.path.abspath(path)\nelif parts[0] == \"~\":\nreturn os.path.expanduser(path)\nfor search_path in search_paths:\nresolved_path = os.path.expanduser(os.path.join(search_path, path))\nif os.path.exists(resolved_path):\nreturn os.path.abspath(resolved_path)\nreturn path\n</code></pre>"},{"location":"api_reference/utils/#cpas_toolbox.utils.download","title":"download","text":"<pre><code>download(url: str, download_path: str) -&gt; str\n</code></pre> <p>Download file from URL to a specified path.</p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def download(url: str, download_path: str) -&gt; str:\n\"\"\"Download file from URL to a specified path.\"\"\"\nblock_size = 100\nif \"drive.google.com\" in url:\ngdown.download(url, download_path)\nelse:\n# adapted from https://stackoverflow.com/a/37573701\nresponse = requests.get(url, stream=True)\ntotal_size_in_bytes = int(response.headers.get(\"content-length\", 0))\nprogress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\nwith open(download_path, \"wb\") as file:\nfor data in response.iter_content(block_size):\nprogress_bar.update(len(data))\nfile.write(data)\nprogress_bar.close()\nif total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\nprint(\"ERROR, download failed\")\nexit()\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/","title":"asmnet.py","text":""},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet","title":"cpas_toolbox.cpas_methods.asmnet","text":"<p>This module defines ASMNet interface.</p> <p>Method is described in ASM-Net: Category-level Pose and Shape, Akizuki, 2021</p> <p>Implementation based on https://github.com/sakizuki/asm-net</p>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet","title":"ASMNet","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for ASMNet.</p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>class ASMNet(CPASMethod):\n\"\"\"Wrapper class for ASMNet.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for ASMNet.\n        Attributes:\n            model: Path to model.\n            device: Device string for the model.\n            models_dir:\n                Path to directory containing model parameters.\n                Must contain the following directory structure:\n                    {models_dir}/{category_0}/model.pth\n                    ...\n            asm_params_dir:\n                Path to direactory containing ASM parameters.\n                Must contain the following directory structure:\n                    {asm_params_directory}/{category_0}/train/info.npz\n                    ...\n            weights_url:\n                URL to download model and ASM params from if they do not exist yet.\n            categories:\n                List of categories. Each category requires corresponding directory with\n                model.pth and info.npz. See models_dir and asm_params_dir.\n            num_points: Number of input points.\n            deformation_dimension: Number of deformation parameters.\n            use_mean_shape:\n                Whether the mean shape (0) or the predicted shape deformation should\n                be used.\n            use_icp: Whether to use ICP to refine the pose.\n        \"\"\"\nmodels_dir: str\nasm_params_dir: str\nweights_url: str\ndevice: str\ncategories: List[str]\nnum_points: int\ndeformation_dimension: int\nuse_mean_shape: bool\nuse_icp: bool\ndefault_config: Config = {\n\"model_params_dir\": None,\n\"asm_params_dir\": None,\n\"weights_url\": None,\n\"device\": \"cuda\",\n\"categories\": [],\n\"num_points\": 800,\n\"deformation_dimension\": 3,\n\"use_mean_shape\": False,\n\"use_icp\": True,\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load ASMNet model.\n        Args:\n            config: ASMNet configuration. See ASMNet.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=ASMNet.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._weights_dir_path = utils.resolve_path(config[\"models_dir\"])\nself._asm_params_dir_path = utils.resolve_path(config[\"asm_params_dir\"])\nself._weights_url = config[\"weights_url\"]\nself._check_paths()\nsynset_names = [\"placeholder\"] + config[\"categories\"]  # first will be ignored\nself._asmds = asmnet.cr6d_utils.load_asmds(\nself._asm_params_dir_path, synset_names\n)\nself._models = asmnet.cr6d_utils.load_models_release(\nself._weights_dir_path,\nsynset_names,\nconfig[\"deformation_dimension\"],\nconfig[\"num_points\"],\nself._device,\n)\nself._num_points = config[\"num_points\"]\nself._use_mean_shape = config[\"use_mean_shape\"]\nself._use_icp = config[\"use_icp\"]\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._weights_dir_path) or not os.path.exists(\nself._asm_params_dir_path\n):\nprint(\"ASM-Net model weights not found, do you want to download to \")\nprint(\"  \", self._weights_dir_path)\nprint(\"  \", self._asm_params_dir_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"ASM-Net model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\ndownload_dir_path = tempfile.mkdtemp()\nzip_file_path = os.path.join(download_dir_path, \"asmnetweights.zip\")\nutils.download(\nself._weights_url,\nzip_file_path,\n)\nzip_file = zipfile.ZipFile(zip_file_path)\nzip_file.extractall(download_dir_path)\nzip_file.close()\nos.remove(zip_file_path)\nif not os.path.exists(self._asm_params_dir_path):\nos.makedirs(self._asm_params_dir_path, exist_ok=True)\nsource_dir_path = os.path.join(download_dir_path, \"params\", \"asm_params\")\nfile_names = os.listdir(source_dir_path)\nfor fn in file_names:\nshutil.move(\nos.path.join(source_dir_path, fn), self._asm_params_dir_path\n)\nif not os.path.exists(self._weights_dir_path):\nos.makedirs(self._weights_dir_path, exist_ok=True)\nsource_dir_path = os.path.join(download_dir_path, \"params\", \"weights\")\nfile_names = os.listdir(source_dir_path)\nfor fn in file_names:\nshutil.move(os.path.join(source_dir_path, fn), self._weights_dir_path)\nshutil.rmtree(os.path.join(download_dir_path, \"params\"))\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\n        Based on asmnet.ASM_Net.test_net_nocs2019_release\n        \"\"\"\n# torch -&gt; numpy\ncolor_image = np.uint8(\n(color_image * 255).numpy()\n)  # (H, W, 3), uint8, 0-255, RGB\ndepth_image = np.uint16((depth_image * 1000).numpy())  # (H, W), uint16, mm\ninstance_mask = instance_mask.numpy()\n# Noise reduction + pointcloud generation\nmasked_depth = depth_image * instance_mask\nmasked_depth = asmnet.common3Dfunc.image_statistical_outlier_removal(\nmasked_depth, factor=2.0\n)\npcd_obj = asmnet.cr6d_utils.get_pcd_from_rgbd(\ncolor_image.copy(),\nmasked_depth.copy(),\nself._camera.get_o3d_pinhole_camera_parameters().intrinsic,\n)\n[pcd_obj, _] = pcd_obj.remove_statistical_outlier(100, 2.0)\npcd_in = copy.deepcopy(pcd_obj)\npcd_c, offset = asmnet.common3Dfunc.centering(pcd_in)\npcd_n, scale = asmnet.common3Dfunc.size_normalization(pcd_c)\n# o3d -&gt; torch\nnp_pcd = np.array(pcd_n.points)\nnp_input = asmnet.cr6d_utils.random_sample(np_pcd, self._num_points)\nnp_input = np_input.astype(np.float32)\ninput_points = torch.from_numpy(np_input)\n# prepare input shape\ninput_points = input_points.unsqueeze(0).transpose(2, 1).to(self._device)\n# evaluate model\nwith torch.no_grad():\ndparam_pred, q_pred = self._models[category_str](input_points)\ndparam_pred = dparam_pred.cpu().numpy().squeeze()\npred_rot = asmnet.cr6d_utils.quaternion2rotationPT(q_pred)\npred_rot = pred_rot.cpu().numpy().squeeze()\npred_dp_param = dparam_pred[:-1]  # deformation params\npred_scaling_param = dparam_pred[-1]  # scale\n# get shape prediction\npcd_pred = None\nif self._use_mean_shape:\npcd_pred = self._asmds[category_str].deformation([0])\nelse:\npcd_pred = self._asmds[category_str].deformation(pred_dp_param)\npcd_pred = pcd_pred.remove_statistical_outlier(20, 1.0)[0]\npcd_pred.scale(pred_scaling_param, (0.0, 0.0, 0.0))\nmetric_pcd = copy.deepcopy(pcd_pred)\nmetric_pcd.scale(scale, (0.0, 0.0, 0.0))  # undo scale normalization\n# ICP\npcd_pred_posed = copy.deepcopy(metric_pcd)\npcd_pred_posed.rotate(pred_rot)  # rotate metric reconstruction\npcd_pred_posed.translate(offset)  # move to center of cropped pcd\npred_rt = np.identity(4)\npred_rt[:3, :3] = pred_rot\nif self._use_icp:\npcd_pred_posed_ds = pcd_pred_posed.voxel_down_sample(0.005)\nif len(pcd_pred_posed_ds.points) &gt; 3:\n# remove hidden points\npcd_pred_posed_visible = asmnet.common3Dfunc.applyHPR(\npcd_pred_posed_ds\n)\npcd_in = pcd_in.voxel_down_sample(0.005)\nreg_result = o3d.pipelines.registration.registration_icp(\npcd_pred_posed_visible, pcd_in, max_correspondence_distance=0.02\n)\npcd_pred_posed = copy.deepcopy(pcd_pred_posed_ds).transform(\nreg_result.transformation\n)\npred_rt = np.dot(reg_result.transformation, pred_rt)\nelse:\nprint(\n\"ASM-Net Warning: Couldn't perform ICP, too few points after\"\n\"voxel down sampling\"\n)\n# center position\nmaxb = pcd_pred_posed.get_max_bound()  # bbox max\nminb = pcd_pred_posed.get_min_bound()  # bbox min\ncenter = (maxb - minb) / 2 + minb  # bbox center\npred_rt[:3, 3] = center.copy()\nposition = torch.Tensor(pred_rt[:3, 3])\norientation_q = torch.Tensor(\nRotation.from_matrix(pred_rt[:3, :3]).as_quat()\n)\nreconstructed_points = torch.from_numpy(np.asarray(metric_pcd.points))\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for ASMNet.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> <code>models_dir</code> <p>Path to directory containing model parameters. Must contain the following directory structure:     {models_dir}/{category_0}/model.pth     ...</p> <p> TYPE: <code>str</code> </p> <code>asm_params_dir</code> <p>Path to direactory containing ASM parameters. Must contain the following directory structure:     {asm_params_directory}/{category_0}/train/info.npz     ...</p> <p> TYPE: <code>str</code> </p> <code>weights_url</code> <p>URL to download model and ASM params from if they do not exist yet.</p> <p> TYPE: <code>str</code> </p> <code>categories</code> <p>List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir.</p> <p> TYPE: <code>List[str]</code> </p> <code>num_points</code> <p>Number of input points.</p> <p> TYPE: <code>int</code> </p> <code>deformation_dimension</code> <p>Number of deformation parameters.</p> <p> TYPE: <code>int</code> </p> <code>use_mean_shape</code> <p>Whether the mean shape (0) or the predicted shape deformation should be used.</p> <p> TYPE: <code>bool</code> </p> <code>use_icp</code> <p>Whether to use ICP to refine the pose.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for ASMNet.\n    Attributes:\n        model: Path to model.\n        device: Device string for the model.\n        models_dir:\n            Path to directory containing model parameters.\n            Must contain the following directory structure:\n                {models_dir}/{category_0}/model.pth\n                ...\n        asm_params_dir:\n            Path to direactory containing ASM parameters.\n            Must contain the following directory structure:\n                {asm_params_directory}/{category_0}/train/info.npz\n                ...\n        weights_url:\n            URL to download model and ASM params from if they do not exist yet.\n        categories:\n            List of categories. Each category requires corresponding directory with\n            model.pth and info.npz. See models_dir and asm_params_dir.\n        num_points: Number of input points.\n        deformation_dimension: Number of deformation parameters.\n        use_mean_shape:\n            Whether the mean shape (0) or the predicted shape deformation should\n            be used.\n        use_icp: Whether to use ICP to refine the pose.\n    \"\"\"\nmodels_dir: str\nasm_params_dir: str\nweights_url: str\ndevice: str\ncategories: List[str]\nnum_points: int\ndeformation_dimension: int\nuse_mean_shape: bool\nuse_icp: bool\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load ASMNet model.</p> PARAMETER DESCRIPTION <code>config</code> <p>ASMNet configuration. See ASMNet.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load ASMNet model.\n    Args:\n        config: ASMNet configuration. See ASMNet.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=ASMNet.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> <p>Based on asmnet.ASM_Net.test_net_nocs2019_release</p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\n    Based on asmnet.ASM_Net.test_net_nocs2019_release\n    \"\"\"\n# torch -&gt; numpy\ncolor_image = np.uint8(\n(color_image * 255).numpy()\n)  # (H, W, 3), uint8, 0-255, RGB\ndepth_image = np.uint16((depth_image * 1000).numpy())  # (H, W), uint16, mm\ninstance_mask = instance_mask.numpy()\n# Noise reduction + pointcloud generation\nmasked_depth = depth_image * instance_mask\nmasked_depth = asmnet.common3Dfunc.image_statistical_outlier_removal(\nmasked_depth, factor=2.0\n)\npcd_obj = asmnet.cr6d_utils.get_pcd_from_rgbd(\ncolor_image.copy(),\nmasked_depth.copy(),\nself._camera.get_o3d_pinhole_camera_parameters().intrinsic,\n)\n[pcd_obj, _] = pcd_obj.remove_statistical_outlier(100, 2.0)\npcd_in = copy.deepcopy(pcd_obj)\npcd_c, offset = asmnet.common3Dfunc.centering(pcd_in)\npcd_n, scale = asmnet.common3Dfunc.size_normalization(pcd_c)\n# o3d -&gt; torch\nnp_pcd = np.array(pcd_n.points)\nnp_input = asmnet.cr6d_utils.random_sample(np_pcd, self._num_points)\nnp_input = np_input.astype(np.float32)\ninput_points = torch.from_numpy(np_input)\n# prepare input shape\ninput_points = input_points.unsqueeze(0).transpose(2, 1).to(self._device)\n# evaluate model\nwith torch.no_grad():\ndparam_pred, q_pred = self._models[category_str](input_points)\ndparam_pred = dparam_pred.cpu().numpy().squeeze()\npred_rot = asmnet.cr6d_utils.quaternion2rotationPT(q_pred)\npred_rot = pred_rot.cpu().numpy().squeeze()\npred_dp_param = dparam_pred[:-1]  # deformation params\npred_scaling_param = dparam_pred[-1]  # scale\n# get shape prediction\npcd_pred = None\nif self._use_mean_shape:\npcd_pred = self._asmds[category_str].deformation([0])\nelse:\npcd_pred = self._asmds[category_str].deformation(pred_dp_param)\npcd_pred = pcd_pred.remove_statistical_outlier(20, 1.0)[0]\npcd_pred.scale(pred_scaling_param, (0.0, 0.0, 0.0))\nmetric_pcd = copy.deepcopy(pcd_pred)\nmetric_pcd.scale(scale, (0.0, 0.0, 0.0))  # undo scale normalization\n# ICP\npcd_pred_posed = copy.deepcopy(metric_pcd)\npcd_pred_posed.rotate(pred_rot)  # rotate metric reconstruction\npcd_pred_posed.translate(offset)  # move to center of cropped pcd\npred_rt = np.identity(4)\npred_rt[:3, :3] = pred_rot\nif self._use_icp:\npcd_pred_posed_ds = pcd_pred_posed.voxel_down_sample(0.005)\nif len(pcd_pred_posed_ds.points) &gt; 3:\n# remove hidden points\npcd_pred_posed_visible = asmnet.common3Dfunc.applyHPR(\npcd_pred_posed_ds\n)\npcd_in = pcd_in.voxel_down_sample(0.005)\nreg_result = o3d.pipelines.registration.registration_icp(\npcd_pred_posed_visible, pcd_in, max_correspondence_distance=0.02\n)\npcd_pred_posed = copy.deepcopy(pcd_pred_posed_ds).transform(\nreg_result.transformation\n)\npred_rt = np.dot(reg_result.transformation, pred_rt)\nelse:\nprint(\n\"ASM-Net Warning: Couldn't perform ICP, too few points after\"\n\"voxel down sampling\"\n)\n# center position\nmaxb = pcd_pred_posed.get_max_bound()  # bbox max\nminb = pcd_pred_posed.get_min_bound()  # bbox min\ncenter = (maxb - minb) / 2 + minb  # bbox center\npred_rt[:3, 3] = center.copy()\nposition = torch.Tensor(pred_rt[:3, 3])\norientation_q = torch.Tensor(\nRotation.from_matrix(pred_rt[:3, :3]).as_quat()\n)\nreconstructed_points = torch.from_numpy(np.asarray(metric_pcd.points))\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/","title":"cass.py","text":""},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass","title":"cpas_toolbox.cpas_methods.cass","text":"<p>This module defines CASS interface.</p> <p>Method is described in Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation, Chen, 2020</p> <p>Implementation based on https://github.com/densechen/CASS</p>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS","title":"CASS","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for CASS.</p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>class CASS(CPASMethod):\n\"\"\"Wrapper class for CASS.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for CASS.\n        Attributes:\n            model: Path to model.\n            device: Device string for the model.\n        \"\"\"\nmodel: str\ndefault_config: Config = {\n\"model\": None,\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load CASS model.\n        Args:\n            config: CASS configuration. See CASS.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=CASS.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_path = utils.resolve_path(config[\"model\"])\nself._check_paths()\nself._cass = cass.CASS(\nnum_points=config[\"num_points\"], num_obj=config[\"num_objects\"]\n)\nself._num_points = config[\"num_points\"]\nself._cass.load_state_dict(\ntorch.load(self._model_path, map_location=self._device)\n)\nself._cass.to(self._device)\nself._cass.eval()\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_path):\nprint(\"CASS model weights not found, do you want to download to \")\nprint(\"  \", self._model_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"CASS model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_path):\nos.makedirs(os.path.dirname(self._model_path), exist_ok=True)\nutils.download(\n\"https://drive.google.com/u/0/uc?id=14K1a-Ft-YO9dUREEXxmWqF2ruUP4p7BZ&amp;\"\n\"export=download\",\nself._model_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\n        Based on cass.tools.eval.\n        \"\"\"\n# get bounding box\nvalid_mask = (depth_image != 0) * instance_mask\nrmin, rmax, cmin, cmax = cass.get_bbox(valid_mask.numpy())\nbb_mask = torch.zeros_like(depth_image)\nbb_mask[rmin:rmax, cmin:cmax] = 1.0\n# prepare image crop\ncolor_input = torch.flip(color_image, (2,)).permute([2, 0, 1])  # RGB -&gt; BGR\ncolor_input = color_input[:, rmin:rmax, cmin:cmax]  # bb crop\ncolor_input = color_input.unsqueeze(0)  # add batch dim\ncolor_input = TF.normalize(\ncolor_input, mean=[0.51, 0.47, 0.44], std=[0.29, 0.27, 0.28]\n)\n# prepare points (fixed number of points, randomly picked)\npoint_indices = valid_mask.nonzero()\nif len(point_indices) &gt; self._num_points:\nsubset = np.random.choice(\nlen(point_indices), replace=False, size=self._num_points\n)\npoint_indices = point_indices[subset]\ndepth_mask = torch.zeros_like(depth_image)\ndepth_mask[point_indices[:, 0], point_indices[:, 1]] = 1.0\ncropped_depth_mask = depth_mask[rmin:rmax, cmin:cmax]\npoint_indices_input = cropped_depth_mask.flatten().nonzero()[:, 0]\n# prepare pointcloud\npoints = pointset_utils.depth_to_pointcloud(\ndepth_image,\nself._camera,\nnormalize=False,\nmask=depth_mask,\nconvention=\"opencv\",\n)\nif len(points) &lt; self._num_points:\nwrap_indices = np.pad(\nnp.arange(len(points)), (0, self._num_points - len(points)), mode=\"wrap\"\n)\npoints = points[wrap_indices]\npoint_indices_input = point_indices_input[wrap_indices]\n# x, y inverted for some reason...\npoints[:, 0] *= -1\npoints[:, 1] *= -1\npoints = points.unsqueeze(0)\npoint_indices_input = point_indices_input.unsqueeze(0)\n# move inputs to device\ncolor_input = color_input.to(self._device)\npoints = points.to(self._device)\npoint_indices_input = point_indices_input.to(self._device)\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\n# CASS model uses 0-indexed categories, same order as NOCSDataset\ncategory_index = torch.tensor([category_id], device=self._device)\n# Call CASS network\nfolding_encode = self._cass.foldingnet.encode(\ncolor_input, points, point_indices_input\n)\nposenet_encode = self._cass.estimator.encode(\ncolor_input, points, point_indices_input\n)\npred_r, pred_t, pred_c = self._cass.estimator.pose(\ntorch.cat([posenet_encode, folding_encode], dim=1), category_index\n)\nreconstructed_points = self._cass.foldingnet.recon(folding_encode)[0]\n# Postprocess outputs\nreconstructed_points = reconstructed_points.view(-1, 3).cpu()\npred_c = pred_c.view(1, self._num_points)\n_, max_index = torch.max(pred_c, 1)\npred_t = pred_t.view(self._num_points, 1, 3)\norientation_q = pred_r[0][max_index[0]].view(-1).cpu()\npoints = points.view(self._num_points, 1, 3)\nposition = (points + pred_t)[max_index[0]].view(-1).cpu()\n# output is scalar-first -&gt; scalar-last\norientation_q = torch.tensor([*orientation_q[1:], orientation_q[0]])\n# Flip x and y axis of position and orientation (undo flipping of points)\n# (x-left, y-up, z-forward) convention -&gt; OpenCV convention\nposition[0] *= -1\nposition[1] *= -1\ncam_fix = torch.tensor([0.0, 0.0, 1.0, 0.0])\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(cam_fix, orientation_q)\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\n# TODO refinement code from cass.tools.eval? (not mentioned in paper??)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\n# pointset_utils.visualize_pointset(reconstructed_points)\nreturn {\n\"position\": position.detach(),\n\"orientation\": orientation_q.detach(),\n\"extents\": extents.detach(),\n\"reconstructed_pointcloud\": reconstructed_points.detach(),\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for CASS.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for CASS.\n    Attributes:\n        model: Path to model.\n        device: Device string for the model.\n    \"\"\"\nmodel: str\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load CASS model.</p> PARAMETER DESCRIPTION <code>config</code> <p>CASS configuration. See CASS.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load CASS model.\n    Args:\n        config: CASS configuration. See CASS.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=CASS.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> <p>Based on cass.tools.eval.</p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\n    Based on cass.tools.eval.\n    \"\"\"\n# get bounding box\nvalid_mask = (depth_image != 0) * instance_mask\nrmin, rmax, cmin, cmax = cass.get_bbox(valid_mask.numpy())\nbb_mask = torch.zeros_like(depth_image)\nbb_mask[rmin:rmax, cmin:cmax] = 1.0\n# prepare image crop\ncolor_input = torch.flip(color_image, (2,)).permute([2, 0, 1])  # RGB -&gt; BGR\ncolor_input = color_input[:, rmin:rmax, cmin:cmax]  # bb crop\ncolor_input = color_input.unsqueeze(0)  # add batch dim\ncolor_input = TF.normalize(\ncolor_input, mean=[0.51, 0.47, 0.44], std=[0.29, 0.27, 0.28]\n)\n# prepare points (fixed number of points, randomly picked)\npoint_indices = valid_mask.nonzero()\nif len(point_indices) &gt; self._num_points:\nsubset = np.random.choice(\nlen(point_indices), replace=False, size=self._num_points\n)\npoint_indices = point_indices[subset]\ndepth_mask = torch.zeros_like(depth_image)\ndepth_mask[point_indices[:, 0], point_indices[:, 1]] = 1.0\ncropped_depth_mask = depth_mask[rmin:rmax, cmin:cmax]\npoint_indices_input = cropped_depth_mask.flatten().nonzero()[:, 0]\n# prepare pointcloud\npoints = pointset_utils.depth_to_pointcloud(\ndepth_image,\nself._camera,\nnormalize=False,\nmask=depth_mask,\nconvention=\"opencv\",\n)\nif len(points) &lt; self._num_points:\nwrap_indices = np.pad(\nnp.arange(len(points)), (0, self._num_points - len(points)), mode=\"wrap\"\n)\npoints = points[wrap_indices]\npoint_indices_input = point_indices_input[wrap_indices]\n# x, y inverted for some reason...\npoints[:, 0] *= -1\npoints[:, 1] *= -1\npoints = points.unsqueeze(0)\npoint_indices_input = point_indices_input.unsqueeze(0)\n# move inputs to device\ncolor_input = color_input.to(self._device)\npoints = points.to(self._device)\npoint_indices_input = point_indices_input.to(self._device)\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\n# CASS model uses 0-indexed categories, same order as NOCSDataset\ncategory_index = torch.tensor([category_id], device=self._device)\n# Call CASS network\nfolding_encode = self._cass.foldingnet.encode(\ncolor_input, points, point_indices_input\n)\nposenet_encode = self._cass.estimator.encode(\ncolor_input, points, point_indices_input\n)\npred_r, pred_t, pred_c = self._cass.estimator.pose(\ntorch.cat([posenet_encode, folding_encode], dim=1), category_index\n)\nreconstructed_points = self._cass.foldingnet.recon(folding_encode)[0]\n# Postprocess outputs\nreconstructed_points = reconstructed_points.view(-1, 3).cpu()\npred_c = pred_c.view(1, self._num_points)\n_, max_index = torch.max(pred_c, 1)\npred_t = pred_t.view(self._num_points, 1, 3)\norientation_q = pred_r[0][max_index[0]].view(-1).cpu()\npoints = points.view(self._num_points, 1, 3)\nposition = (points + pred_t)[max_index[0]].view(-1).cpu()\n# output is scalar-first -&gt; scalar-last\norientation_q = torch.tensor([*orientation_q[1:], orientation_q[0]])\n# Flip x and y axis of position and orientation (undo flipping of points)\n# (x-left, y-up, z-forward) convention -&gt; OpenCV convention\nposition[0] *= -1\nposition[1] *= -1\ncam_fix = torch.tensor([0.0, 0.0, 1.0, 0.0])\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(cam_fix, orientation_q)\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\n# TODO refinement code from cass.tools.eval? (not mentioned in paper??)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\n# pointset_utils.visualize_pointset(reconstructed_points)\nreturn {\n\"position\": position.detach(),\n\"orientation\": orientation_q.detach(),\n\"extents\": extents.detach(),\n\"reconstructed_pointcloud\": reconstructed_points.detach(),\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/","title":"crnet.py","text":""},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet","title":"cpas_toolbox.cpas_methods.crnet","text":"<p>This module defines CR-Net interface.</p> <p>Method is described in Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks, Wang, 2021.</p> <p>Implementation based on https://github.com/JeremyWANGJZ/Category-6D-Pose</p>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet","title":"CRNet","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for CRNet.</p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>class CRNet(CPASMethod):\n\"\"\"Wrapper class for CRNet.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for CRNet.\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            device: Device string for the model.\n        \"\"\"\nmodel: str\nnum_categories: int\ndefault_config: Config = {\n\"model\": None,\n\"num_categories\": None,\n\"num_shape_points\": None,\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load CRNet model.\n        Args:\n            config: CRNet configuration. See CRNet.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=CRNet.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_path = utils.resolve_path(config[\"model\"])\nself._model_url = config[\"model_url\"]\nself._mean_shape_path = utils.resolve_path(config[\"mean_shape\"])\nself._mean_shape_url = config[\"mean_shape_url\"]\nself._check_paths()\nself._crnet = crnet.DeformNet(\nconfig[\"num_categories\"], config[\"num_shape_points\"]\n)\nself._crnet.cuda()\nself._crnet = torch.nn.DataParallel(self._crnet, device_ids=[self._device])\nself._crnet.load_state_dict(\ntorch.load(self._model_path, map_location=self._device)\n)\nself._crnet.eval()\nself._mean_shape_pointsets = np.load(self._mean_shape_path)\nself._num_input_points = config[\"num_input_points\"]\nself._image_size = config[\"image_size\"]\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_path) or not os.path.exists(\nself._mean_shape_path\n):\nprint(\"CRNet model weights not found, do you want to download to \")\nprint(\"  \", self._model_path)\nprint(\"  \", self._mean_shape_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"CRNet model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_path):\nos.makedirs(os.path.dirname(self._model_path), exist_ok=True)\nutils.download(\nself._model_url,\nself._model_path,\n)\nif not os.path.exists(self._mean_shape_path):\nos.makedirs(os.path.dirname(self._mean_shape_path), exist_ok=True)\nutils.download(\nself._mean_shape_url,\nself._mean_shape_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n        Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n        \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = crnet.get_bbox([y1, x1, y2, x2])\nvalid_mask = (depth_image != 0) * instance_mask\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\n# if len(choose) &lt; 32:\n#     f_sRT[i] = np.identity(4, dtype=float)\n#     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n#     continue\n# else:\n#     valid_inst.append(i)\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# Move inputs to device and convert to right shape\ncolor_input = color_input.unsqueeze(0).to(self._device)\npoints = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.cuda.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call CRNet\nassign_matrix, deltas = self._crnet(\npoints,\ncolor_input,\npoint_indices,\ncategory_id,\nmean_shape_pointset,\n)\n# Postprocess output\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = crnet.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for CRNet.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>int</code> </p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for CRNet.\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        device: Device string for the model.\n    \"\"\"\nmodel: str\nnum_categories: int\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load CRNet model.</p> PARAMETER DESCRIPTION <code>config</code> <p>CRNet configuration. See CRNet.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load CRNet model.\n    Args:\n        config: CRNet configuration. See CRNet.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=CRNet.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n    Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n    \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = crnet.get_bbox([y1, x1, y2, x2])\nvalid_mask = (depth_image != 0) * instance_mask\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\n# if len(choose) &lt; 32:\n#     f_sRT[i] = np.identity(4, dtype=float)\n#     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n#     continue\n# else:\n#     valid_inst.append(i)\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# Move inputs to device and convert to right shape\ncolor_input = color_input.unsqueeze(0).to(self._device)\npoints = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.cuda.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call CRNet\nassign_matrix, deltas = self._crnet(\npoints,\ncolor_input,\npoint_indices,\ncategory_id,\nmean_shape_pointset,\n)\n# Postprocess output\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = crnet.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/","title":"dpdn.py","text":""},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn","title":"cpas_toolbox.cpas_methods.dpdn","text":"<p>This module defines DPDN interface.</p> <p>Method is described in Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks, Lin, 2022.</p> <p>Implementation based on https://github.com/JiehongLin/Self-DPDN.</p>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN","title":"DPDN","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for DPDN.</p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>class DPDN(CPASMethod):\n\"\"\"Wrapper class for DPDN.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for DPDN.\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            device: Device string for the model.\n        \"\"\"\nmodel: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nimage_size: int\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\nresnet_dir: str\ndevice: str\ndefault_config: Config = {\n\"model\": None,\n\"num_categories\": None,\n\"num_shape_points\": None,\n\"num_input_points\": None,\n\"image_size\": None,\n\"model\": None,\n\"model_url\": None,\n\"mean_shape\": None,\n\"mean_shape_url\": None,\n\"resnet_dir\": None,\n\"device\": \"cuda\",\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load DPDN model.\n        Args:\n            config: DPDN configuration. See DPDN.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=DPDN.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_file_path = utils.resolve_path(config[\"model\"])\nself._model_url = config[\"model_url\"]\nself._mean_shape_file_path = utils.resolve_path(config[\"mean_shape\"])\nself._mean_shape_url = config[\"mean_shape_url\"]\nself._check_paths()\nself._resnet_dir_path = utils.resolve_path(config[\"resnet_dir\"])\nself._dpdn = dpdn.Net(\nconfig[\"num_categories\"], config[\"num_shape_points\"], self._resnet_dir_path\n)\nself._dpdn = self._dpdn.to(self._device)\ncheckpoint = torch.load(self._model_file_path, map_location=self._device)\nif \"model\" in checkpoint:\nstate_dict = checkpoint[\"model\"]\nelif \"state_dict\":\nstate_dict = checkpoint[\"state_dict\"]\nelse:\nstate_dict = checkpoint\nself._dpdn.load_state_dict(state_dict)\nself._dpdn.eval()\nself._mean_shape_pointsets = np.load(self._mean_shape_file_path)\nself._num_input_points = config[\"num_input_points\"]\nself._image_size = config[\"image_size\"]\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_file_path) or not os.path.exists(\nself._mean_shape_file_path\n):\nprint(\"DPDN model weights not found, do you want to download to \")\nprint(\"  \", self._model_file_path)\nprint(\"  \", self._mean_shape_file_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"DPDN model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_file_path):\ndl_dir_path = tempfile.mkdtemp()\nprint(dl_dir_path)\nzip_file_path = os.path.join(dl_dir_path, \"temp\")\nos.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\nutils.download(\nself._model_url,\nzip_file_path,\n)\nz = zipfile.ZipFile(zip_file_path)\nz.extract(\"log/supervised/epoch_30.pth\", dl_dir_path)\nz.close()\nos.remove(zip_file_path)\nshutil.move(\nos.path.join(dl_dir_path, \"log\", \"supervised\", \"epoch_30.pth\"),\nself._model_file_path,\n)\nshutil.rmtree(dl_dir_path)\nif not os.path.exists(self._mean_shape_file_path):\nos.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\nutils.download(\nself._mean_shape_url,\nself._mean_shape_file_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n        Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py\n        \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\ninput_dict = {}\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = dpdn.get_bbox([y1, x1, y2, x2])\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\ninput_dict[\"rgb\"] = color_input.unsqueeze(0).to(self._device)\n# Prepare point indices\nmask = (depth_image != 0) * instance_mask\ncropped_mask = mask[rmin:rmax, cmin:cmax]\ncropped_mask_indices = cropped_mask.numpy().flatten().nonzero()[0]\nif len(cropped_mask_indices) &lt;= self._num_input_points:\nindices = np.random.choice(\nlen(cropped_mask_indices), self._num_input_points\n)\nelse:\nindices = np.random.choice(\nlen(cropped_mask_indices), self._num_input_points, replace=False\n)\nchosen_cropped_indices = cropped_mask_indices[indices]\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = chosen_cropped_indices % crop_w\nrow_idx = chosen_cropped_indices // crop_w\nfinal_cropped_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\ninput_dict[\"choose\"] = (\ntorch.LongTensor(final_cropped_indices).unsqueeze(0).to(self._device)\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\ndepth_image_np = depth_image.numpy()\ndepth_image_np = dpdn.fill_missing(depth_image_np * 1000.0, 1000.0, 1) / 1000.0\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\npts2 = depth_image_np.copy()\npts0 = (xmap - cx) * pts2 / fx\npts1 = (ymap - cy) * pts2 / fy\npts_map = np.stack([pts0, pts1, pts2])\npts_map = np.transpose(pts_map, (1, 2, 0)).astype(np.float32)\ncropped_pts_map = pts_map[rmin:rmax, cmin:cmax, :]\ninput_points = cropped_pts_map.reshape((-1, 3))[chosen_cropped_indices, :]\ninput_dict[\"pts\"] = (\ntorch.FloatTensor(input_points).unsqueeze(0).to(self._device)\n)\n# Prepare prior\ninput_dict[\"prior\"] = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Prepare category id\ninput_dict[\"category_label\"] = torch.cuda.LongTensor([category_id]).to(\nself._device\n)\n# Call DPDN\noutputs = self._dpdn(input_dict)\n# Convert outputs to expected format\nposition = outputs[\"pred_translation\"][0].detach().cpu()\norientation_mat = outputs[\"pred_rotation\"][0].detach().cpu()\norientation = Rotation.from_matrix(orientation_mat.detach().numpy())\norientation_q = torch.FloatTensor(orientation.as_quat())\nextents = outputs[\"pred_size\"][0].detach().cpu()\nreconstructed_points = outputs[\"pred_qv\"][0].detach().cpu()\nscale = torch.linalg.norm(extents)\nreconstructed_points *= scale\n# Recenter for mug category\n# TODO not really sure if this is correct, but seems to give best results\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents = torch.FloatTensor([extents[2], extents[1], extents[0]])\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for DPDN.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for DPDN.\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        device: Device string for the model.\n    \"\"\"\nmodel: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nimage_size: int\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\nresnet_dir: str\ndevice: str\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load DPDN model.</p> PARAMETER DESCRIPTION <code>config</code> <p>DPDN configuration. See DPDN.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load DPDN model.\n    Args:\n        config: DPDN configuration. See DPDN.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=DPDN.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py</p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n    Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py\n    \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\ninput_dict = {}\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = dpdn.get_bbox([y1, x1, y2, x2])\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\ninput_dict[\"rgb\"] = color_input.unsqueeze(0).to(self._device)\n# Prepare point indices\nmask = (depth_image != 0) * instance_mask\ncropped_mask = mask[rmin:rmax, cmin:cmax]\ncropped_mask_indices = cropped_mask.numpy().flatten().nonzero()[0]\nif len(cropped_mask_indices) &lt;= self._num_input_points:\nindices = np.random.choice(\nlen(cropped_mask_indices), self._num_input_points\n)\nelse:\nindices = np.random.choice(\nlen(cropped_mask_indices), self._num_input_points, replace=False\n)\nchosen_cropped_indices = cropped_mask_indices[indices]\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = chosen_cropped_indices % crop_w\nrow_idx = chosen_cropped_indices // crop_w\nfinal_cropped_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\ninput_dict[\"choose\"] = (\ntorch.LongTensor(final_cropped_indices).unsqueeze(0).to(self._device)\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\ndepth_image_np = depth_image.numpy()\ndepth_image_np = dpdn.fill_missing(depth_image_np * 1000.0, 1000.0, 1) / 1000.0\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\npts2 = depth_image_np.copy()\npts0 = (xmap - cx) * pts2 / fx\npts1 = (ymap - cy) * pts2 / fy\npts_map = np.stack([pts0, pts1, pts2])\npts_map = np.transpose(pts_map, (1, 2, 0)).astype(np.float32)\ncropped_pts_map = pts_map[rmin:rmax, cmin:cmax, :]\ninput_points = cropped_pts_map.reshape((-1, 3))[chosen_cropped_indices, :]\ninput_dict[\"pts\"] = (\ntorch.FloatTensor(input_points).unsqueeze(0).to(self._device)\n)\n# Prepare prior\ninput_dict[\"prior\"] = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Prepare category id\ninput_dict[\"category_label\"] = torch.cuda.LongTensor([category_id]).to(\nself._device\n)\n# Call DPDN\noutputs = self._dpdn(input_dict)\n# Convert outputs to expected format\nposition = outputs[\"pred_translation\"][0].detach().cpu()\norientation_mat = outputs[\"pred_rotation\"][0].detach().cpu()\norientation = Rotation.from_matrix(orientation_mat.detach().numpy())\norientation_q = torch.FloatTensor(orientation.as_quat())\nextents = outputs[\"pred_size\"][0].detach().cpu()\nreconstructed_points = outputs[\"pred_qv\"][0].detach().cpu()\nscale = torch.linalg.norm(extents)\nreconstructed_points *= scale\n# Recenter for mug category\n# TODO not really sure if this is correct, but seems to give best results\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents = torch.FloatTensor([extents[2], extents[1], extents[0]])\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/","title":"icaps.py","text":""},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps","title":"cpas_toolbox.cpas_methods.icaps","text":"<p>This module defines iCaps interface.</p> <p>Method is described in iCaps Iterative Category-Level Object Pose and Shape Estimation, Deng, 2022.</p> <p>Implementation based on https://github.com/aerogjy/iCaps</p>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps","title":"ICaps","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for iCaps.</p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>class ICaps(CPASMethod):\n\"\"\"Wrapper class for iCaps.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for iCaps.\n        Attributes:\n            pf_config_dir:\n                Particle filter configuration directory for iCaps. Must contain one yml\n                file for each supported category_str.\n            deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints.\n            latentnet_checkpoint_dir: Directory containing LatentNet checkpoints.\n            aae_checkpoint_dir: Directory containing auto-encoder checkpoints.\n            checkpoints_url:\n                URL to download checkpoints from if checkpoint directories are empty or\n                do not exist yet (assumed to be tar file).\n            categories:\n                List of category strings. Each category requires corresponding\n                directories in each checkpoint dir.\n            device:\n                The device to use.\n        \"\"\"\npf_config_dir: str\ndeepsdf_checkpoint_dir: str\nlatentnet_checkpoint_dir: str\naae_checkpoint_dir: str\ncheckpoints_url: str\ncategories: List[str]\ndevice: str\ndefault_config: Config = {\n\"pf_config_dir\": None,\n\"deepsdf_checkpoint_dir\": None,\n\"latentnet_checkpoint_dir\": None,\n\"aae_checkpoint_dir\": None,\n\"checkpoints_url\": None,\n\"categories\": [\"bottle\", \"bowl\", \"camera\", \"can\", \"laptop\", \"mug\"],\n\"device\": \"cuda\",\n}\n# This is to replace reliance on ground-truth mesh in iCaps\n# Numbers computed from mean shapes used in SDF\n# see: https://github.com/aerogjy/iCaps/issues/1\ncategory_str_to_ratio = {\n\"bottle\": 2.149,\n\"bowl\": 2.7,\n\"camera\": 2.328,\n\"can\": 2.327,\n\"laptop\": 2.076,\n\"mug\": 2.199,\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load ICaps models.\n        Args:\n            config: iCaps configuration. See ICaps.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=ICaps.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._num_points = config[\"num_points\"]\nself._category_strs = config[\"categories\"]\nself._checkpoints_url = config[\"checkpoints_url\"]\nself._pose_rbpfs = {}\npf_cfg_dir_path = utils.resolve_path(\nconfig[\"pf_config_dir\"],\nsearch_paths=[\n\".\",\n\"~/.cpas_toolbox\",\nos.path.join(os.path.dirname(__file__), \"config\"),\nos.path.dirname(__file__),\n],\n)\nself._deepsdf_ckp_dir_path = utils.resolve_path(\nconfig[\"deepsdf_checkpoint_dir\"]\n)\nself._latentnet_ckp_dir_path = utils.resolve_path(\nconfig[\"latentnet_checkpoint_dir\"]\n)\nself._aae_ckp_dir_path = utils.resolve_path(config[\"aae_checkpoint_dir\"])\nself._check_paths()\nfor category_str in self._category_strs:\nfull_ckpt_dir_path = os.path.join(self._aae_ckp_dir_path, category_str)\ntrain_cfg_file = os.path.join(full_ckpt_dir_path, \"config.yml\")\nicaps.icaps_config.cfg_from_file(train_cfg_file)\ntest_cfg_file = os.path.join(pf_cfg_dir_path, category_str + \".yml\")\nicaps.icaps_config.cfg_from_file(test_cfg_file)\nobj_list = icaps.icaps_config.cfg.TEST.OBJECTS\ncfg_list = []\ncfg_list.append(copy.deepcopy(icaps.icaps_config.cfg))\nself._pose_rbpfs[category_str] = icaps.PoseRBPF(\nobj_list,\ncfg_list,\nfull_ckpt_dir_path,\nself._deepsdf_ckp_dir_path,\nself._latentnet_ckp_dir_path,\ndevice=self._device,\n)\nself._pose_rbpfs[category_str].set_target_obj(\nicaps.icaps_config.cfg.TEST.OBJECTS[0]\n)\nself._pose_rbpfs[category_str].set_ratio(\nself.category_str_to_ratio[category_str]\n)\ndef _check_paths(self) -&gt; None:\npath_exists = (\nos.path.exists(p)\nfor p in [\nself._aae_ckp_dir_path,\nself._deepsdf_ckp_dir_path,\nself._latentnet_ckp_dir_path,\n]\n)\nif not all(path_exists):\nprint(\"iCaps model weights not found, do you want to download to \")\nprint(\"  \", self._aae_ckp_dir_path)\nprint(\"  \", self._deepsdf_ckp_dir_path)\nprint(\"  \", self._latentnet_ckp_dir_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"iCaps model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\ndl_dir_path = tempfile.mkdtemp()\ntar_file_path = os.path.join(dl_dir_path, \"temp\")\nprint(self._checkpoints_url, tar_file_path)\nutils.download(\nself._checkpoints_url,\ntar_file_path,\n)\ntar_file = tarfile.open(tar_file_path)\nprint(\"Extracting weights... (this might take a while)\")\ntar_file.extractall(dl_dir_path)\nif not os.path.exists(self._latentnet_ckp_dir_path):\nsrc_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"latentnet_ckpts\")\nshutil.move(src_dir_path, self._latentnet_ckp_dir_path)\nif not os.path.exists(self._deepsdf_ckp_dir_path):\nsrc_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"deepsdf_ckpts\")\nshutil.move(src_dir_path, self._deepsdf_ckp_dir_path)\nif not os.path.exists(self._aae_ckp_dir_path):\nsrc_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"aae_ckpts\")\nshutil.move(src_dir_path, self._aae_ckp_dir_path)\n# normalize names\nfor category_str in self._category_strs:\nfor aae_category_dir in os.listdir(self._aae_ckp_dir_path):\nif category_str in aae_category_dir:\nos.rename(\naae_category_dir,\nos.path.join(self._aae_ckp_dir_path, category_str),\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See MethodWrapper.inference.\n        Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n        \"\"\"\n# prepare data as expected by iCaps functions (same as nocs_real_dataset)\ncolor_image = color_image * 255  # see icaps.datasets.nocs_real_dataset l71\ndepth_image = depth_image.unsqueeze(2)  # (...)nocs_real_dataset l79\ninstance_mask = instance_mask.float()  # (...)nocs_real_dataset l100\nintrinsics = torch.eye(3)\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nintrinsics[0, 0] = fx\nintrinsics[1, 1] = fy\nintrinsics[0, 2] = cx\nintrinsics[1, 2] = cy\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nbbox = [y1, y2, x1, x2]\n# from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\npose_rbpf = self._pose_rbpfs[category_str]\npose_rbpf.reset()  # like init but without loading models\nself._pose_rbpfs[category_str].set_target_obj(category_str)\npose_rbpf.data_intrinsics = intrinsics.numpy()\npose_rbpf.intrinsics = intrinsics.numpy()\npose_rbpf.target_obj_cfg.PF.FU = pose_rbpf.intrinsics[0, 0]\npose_rbpf.target_obj_cfg.PF.FV = pose_rbpf.intrinsics[1, 1]\npose_rbpf.target_obj_cfg.PF.U0 = pose_rbpf.intrinsics[0, 2]\npose_rbpf.target_obj_cfg.PF.V0 = pose_rbpf.intrinsics[1, 2]\npose_rbpf.data_with_est_center = False\npose_rbpf.data_with_gt = False  # should this be False now?\npose_rbpf.mask_raw = instance_mask[:, :].cpu().numpy()\npose_rbpf.mask = scipy.ndimage.binary_erosion(\npose_rbpf.mask_raw, iterations=2\n).astype(pose_rbpf.mask_raw.dtype)\npose_rbpf.prior_uv[0] = (bbox[2] + bbox[3]) / 2\npose_rbpf.prior_uv[1] = (bbox[0] + bbox[1]) / 2\n# what is this ??\nif pose_rbpf.aae_full.angle_diff.shape[0] != 0:\npose_rbpf.aae_full.angle_diff = np.array([])\nif pose_rbpf.target_obj_cfg.PF.USE_DEPTH:\ndepth_data = depth_image\nelse:\ndepth_data = None\ntry:\npose_rbpf.initialize_poserbpf(\ncolor_image,\npose_rbpf.data_intrinsics,\npose_rbpf.prior_uv[:2],\npose_rbpf.target_obj_cfg.PF.N_INIT,\nscale_prior=pose_rbpf.target_obj_cfg.PF.SCALE_PRIOR,\ndepth=depth_data,\n)\npose_rbpf.process_poserbpf(\ncolor_image,\nintrinsics.unsqueeze(0),\ndepth=depth_data,\nrun_deep_sdf=False,\n)\n# 3 * 50 iters by default\npose_rbpf.refine_pose_and_shape(depth_data, intrinsics.unsqueeze(0))\nposition_cv = torch.tensor(pose_rbpf.rbpf.trans_bar)\norientation_q = torch.Tensor(\nRotation.from_matrix(pose_rbpf.rbpf.rot_bar).as_quat()\n)\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\norientation_cv = orientation_q\nextents = torch.tensor([0.5, 0.5, 0.5])\nmesh_dir_path = tempfile.mkdtemp()\nmesh_file_path = os.path.join(mesh_dir_path, \"mesh.ply\")\npoint_set = pose_rbpf.evaluator.latent_vec_to_points(\npose_rbpf.latent_vec_refine,\nN=64,\nnum_points=self._num_points,\nsilent=True,\nfname=mesh_file_path,\n)\nif point_set is None:\npoint_set = torch.tensor([[0.0, 0.0, 0.0]])  # failed / no isosurface\nreconstructed_mesh = None\nelse:\nscale = pose_rbpf.size_est / pose_rbpf.ratio\npoint_set *= scale\nreconstructed_mesh = o3d.io.read_triangle_mesh(mesh_file_path)\nreconstructed_mesh.scale(scale.item(), np.array([0, 0, 0]))\nreconstructed_points = torch.tensor(point_set)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position_cv.detach().cpu(),\n\"orientation\": orientation_cv.detach().cpu(),\n\"extents\": extents.detach().cpu(),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": reconstructed_mesh,\n}\nexcept:\nprint(\"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\")\nreturn {\n\"position\": torch.tensor([0.0, 0.0, 0.0]),\n\"orientation\": torch.tensor([0.0, 0.0, 0.0, 1.0]),\n\"extents\": torch.tensor([0.5, 0.5, 0.5]),\n\"reconstructed_pointcloud\": torch.tensor([[0.0, 0.0, 0.0]]),\n\"reconstructed_mesh\": None,  # TODO if time\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for iCaps.</p> ATTRIBUTE DESCRIPTION <code>pf_config_dir</code> <p>Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str.</p> <p> TYPE: <code>str</code> </p> <code>deepsdf_checkpoint_dir</code> <p>Directory containing DeepSDF checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>latentnet_checkpoint_dir</code> <p>Directory containing LatentNet checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>aae_checkpoint_dir</code> <p>Directory containing auto-encoder checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>checkpoints_url</code> <p>URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file).</p> <p> TYPE: <code>str</code> </p> <code>categories</code> <p>List of category strings. Each category requires corresponding directories in each checkpoint dir.</p> <p> TYPE: <code>List[str]</code> </p> <code>device</code> <p>The device to use.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for iCaps.\n    Attributes:\n        pf_config_dir:\n            Particle filter configuration directory for iCaps. Must contain one yml\n            file for each supported category_str.\n        deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints.\n        latentnet_checkpoint_dir: Directory containing LatentNet checkpoints.\n        aae_checkpoint_dir: Directory containing auto-encoder checkpoints.\n        checkpoints_url:\n            URL to download checkpoints from if checkpoint directories are empty or\n            do not exist yet (assumed to be tar file).\n        categories:\n            List of category strings. Each category requires corresponding\n            directories in each checkpoint dir.\n        device:\n            The device to use.\n    \"\"\"\npf_config_dir: str\ndeepsdf_checkpoint_dir: str\nlatentnet_checkpoint_dir: str\naae_checkpoint_dir: str\ncheckpoints_url: str\ncategories: List[str]\ndevice: str\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load ICaps models.</p> PARAMETER DESCRIPTION <code>config</code> <p>iCaps configuration. See ICaps.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load ICaps models.\n    Args:\n        config: iCaps configuration. See ICaps.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=ICaps.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See MethodWrapper.inference.</p> <p>Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset</p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See MethodWrapper.inference.\n    Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n    \"\"\"\n# prepare data as expected by iCaps functions (same as nocs_real_dataset)\ncolor_image = color_image * 255  # see icaps.datasets.nocs_real_dataset l71\ndepth_image = depth_image.unsqueeze(2)  # (...)nocs_real_dataset l79\ninstance_mask = instance_mask.float()  # (...)nocs_real_dataset l100\nintrinsics = torch.eye(3)\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nintrinsics[0, 0] = fx\nintrinsics[1, 1] = fy\nintrinsics[0, 2] = cx\nintrinsics[1, 2] = cy\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nbbox = [y1, y2, x1, x2]\n# from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\npose_rbpf = self._pose_rbpfs[category_str]\npose_rbpf.reset()  # like init but without loading models\nself._pose_rbpfs[category_str].set_target_obj(category_str)\npose_rbpf.data_intrinsics = intrinsics.numpy()\npose_rbpf.intrinsics = intrinsics.numpy()\npose_rbpf.target_obj_cfg.PF.FU = pose_rbpf.intrinsics[0, 0]\npose_rbpf.target_obj_cfg.PF.FV = pose_rbpf.intrinsics[1, 1]\npose_rbpf.target_obj_cfg.PF.U0 = pose_rbpf.intrinsics[0, 2]\npose_rbpf.target_obj_cfg.PF.V0 = pose_rbpf.intrinsics[1, 2]\npose_rbpf.data_with_est_center = False\npose_rbpf.data_with_gt = False  # should this be False now?\npose_rbpf.mask_raw = instance_mask[:, :].cpu().numpy()\npose_rbpf.mask = scipy.ndimage.binary_erosion(\npose_rbpf.mask_raw, iterations=2\n).astype(pose_rbpf.mask_raw.dtype)\npose_rbpf.prior_uv[0] = (bbox[2] + bbox[3]) / 2\npose_rbpf.prior_uv[1] = (bbox[0] + bbox[1]) / 2\n# what is this ??\nif pose_rbpf.aae_full.angle_diff.shape[0] != 0:\npose_rbpf.aae_full.angle_diff = np.array([])\nif pose_rbpf.target_obj_cfg.PF.USE_DEPTH:\ndepth_data = depth_image\nelse:\ndepth_data = None\ntry:\npose_rbpf.initialize_poserbpf(\ncolor_image,\npose_rbpf.data_intrinsics,\npose_rbpf.prior_uv[:2],\npose_rbpf.target_obj_cfg.PF.N_INIT,\nscale_prior=pose_rbpf.target_obj_cfg.PF.SCALE_PRIOR,\ndepth=depth_data,\n)\npose_rbpf.process_poserbpf(\ncolor_image,\nintrinsics.unsqueeze(0),\ndepth=depth_data,\nrun_deep_sdf=False,\n)\n# 3 * 50 iters by default\npose_rbpf.refine_pose_and_shape(depth_data, intrinsics.unsqueeze(0))\nposition_cv = torch.tensor(pose_rbpf.rbpf.trans_bar)\norientation_q = torch.Tensor(\nRotation.from_matrix(pose_rbpf.rbpf.rot_bar).as_quat()\n)\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor(\n[0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n)  # CASS object to ShapeNet object\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\norientation_cv = orientation_q\nextents = torch.tensor([0.5, 0.5, 0.5])\nmesh_dir_path = tempfile.mkdtemp()\nmesh_file_path = os.path.join(mesh_dir_path, \"mesh.ply\")\npoint_set = pose_rbpf.evaluator.latent_vec_to_points(\npose_rbpf.latent_vec_refine,\nN=64,\nnum_points=self._num_points,\nsilent=True,\nfname=mesh_file_path,\n)\nif point_set is None:\npoint_set = torch.tensor([[0.0, 0.0, 0.0]])  # failed / no isosurface\nreconstructed_mesh = None\nelse:\nscale = pose_rbpf.size_est / pose_rbpf.ratio\npoint_set *= scale\nreconstructed_mesh = o3d.io.read_triangle_mesh(mesh_file_path)\nreconstructed_mesh.scale(scale.item(), np.array([0, 0, 0]))\nreconstructed_points = torch.tensor(point_set)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position_cv.detach().cpu(),\n\"orientation\": orientation_cv.detach().cpu(),\n\"extents\": extents.detach().cpu(),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": reconstructed_mesh,\n}\nexcept:\nprint(\"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\")\nreturn {\n\"position\": torch.tensor([0.0, 0.0, 0.0]),\n\"orientation\": torch.tensor([0.0, 0.0, 0.0, 1.0]),\n\"extents\": torch.tensor([0.5, 0.5, 0.5]),\n\"reconstructed_pointcloud\": torch.tensor([[0.0, 0.0, 0.0]]),\n\"reconstructed_mesh\": None,  # TODO if time\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/","title":"rbppose.py","text":""},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose","title":"cpas_toolbox.cpas_methods.rbppose","text":"<p>This module defines RBPPose interface.</p> <p>Method is described in RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation, Zhang, 2022</p> <p>Implementation based on https://github.com/lolrudy/RBP_Pose.</p>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose","title":"RBPPose","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for RBPPose.</p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>class RBPPose(CPASMethod):\n\"\"\"Wrapper class for RBPPose.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for RBPPose.\n        Attributes:\n            model: File path for model weights.\n            model_url: URL to download model weights if file is not found.\n            mean_shape: File path for mean shape file.\n            mean_shape_url: URL to download mean shape file if it is not found.\n            device: Device string for the model.\n        \"\"\"\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\ndevice: str\ndefault_config: Config = {\n\"model\": None,\n\"model_url\": None,\n\"mean_shape\": None,\n\"mean_shape_url\": None,\n\"device\": \"cuda\",\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load RBPPose model.\n        Args:\n            config: RBPPose configuration. See RBPPose.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=RBPPose.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_file_path = utils.resolve_path(config[\"model\"])\nself._model_url = config[\"model_url\"]\nself._mean_shape_file_path = utils.resolve_path(config[\"mean_shape\"])\nself._mean_shape_url = config[\"mean_shape_url\"]\n# Check if files are available and download if not\nself._check_paths()\n# Initialize model\nself._net = rbppose.SSPN(...)\nself._net = self._net.to(self._device)\nstate_dict = torch.load(self._model_file_path, map_location=self._device)\ncleaned_state_dict = copy.copy(state_dict)\nfor key in state_dict.keys():\nif \"face_recon\" in key:\ncleaned_state_dict.pop(key)\nelif \"pcl_encoder_prior\" in key:\ncleaned_state_dict.pop(key)\ncurrent_model_dict = self._net.state_dict()\ncurrent_model_dict.update(cleaned_state_dict)\nself._net.load_state_dict(current_model_dict)\nself._net.eval()\nself._mean_shape_pointsets = np.load(self._mean_shape_file_path)\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_file_path) or not os.path.exists(\nself._mean_shape_file_path\n):\nprint(\"RBPPose model weights not found, do you want to download to \")\nprint(\"  \", self._model_file_path)\nprint(\"  \", self._mean_shape_file_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"RBPPose model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_file_path):\nos.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\nutils.download(\nself._model_url,\nself._model_file_path,\n)\nif not os.path.exists(self._mean_shape_file_path):\nos.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\nutils.download(\nself._mean_shape_url,\nself._mean_shape_file_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n        Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py\n        \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\n# Handle camera information\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(0)\nwidth = self._camera.width\nheight = self._camera.height\ncamera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\ncamera_matrix = torch.FloatTensor(camera_matrix).unsqueeze(0).to(self._device)\n# Prepare RGB crop (not used by default config)\nrgb_cv = color_image.numpy()[:, :, ::-1]  # BGR\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = rbppose.get_bbox([y1, x1, y2, x2])\ncx = 0.5 * (cmin + cmax)\ncy = 0.5 * (rmin + rmax)\nbbox_center = np.array([cx, cy])  # (w/2, h/2)\nscale = min(max(cmax - cmin, rmax - rmin), max(height, width))\nrgb_crop = rbppose.crop_resize_by_warp_affine(\nrgb_cv,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n).transpose(2, 0, 1)\nrgb_crop = torch.FloatTensor(rgb_crop).unsqueeze(0).to(self._device)\n# Prepare depth crop (expected in mm)\ndepth_cv = depth_image.numpy() * 1000\ndepth_crop = rbppose.crop_resize_by_warp_affine(\ndepth_cv,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n)\ndepth_crop = torch.FloatTensor(depth_crop)[None, None].to(self._device)\n# Prepare category\ncategory_input = torch.LongTensor([category_id]).to(self._device)\n# Prepare ROI Mask\nmask_np = instance_mask.float().numpy()\nroi_mask = rbppose.crop_resize_by_warp_affine(\nmask_np,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n)\nroi_mask = torch.FloatTensor(roi_mask)[None, None].to(self._device)\n# Prepare mean shape (size?)\nmean_shape = rbppose.get_mean_shape(category_str) / 1000.0\nmean_shape = torch.FloatTensor(mean_shape).unsqueeze(0).to(self._device)\n# Prepare shape prior\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\nshape_prior = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Prepare 2D coordinates\ncoord_2d = rbppose.get_2d_coord_np(width, height).transpose(1, 2, 0)\nroi_coord_2d = rbppose.crop_resize_by_warp_affine(\ncoord_2d,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n).transpose(2, 0, 1)\nroi_coord_2d = torch.FloatTensor(roi_coord_2d).unsqueeze(0).to(self._device)\noutput_dict = self._net(\nrgb=rgb_crop,\ndepth=depth_crop,\nobj_id=category_input,\ncamK=camera_matrix,\ndef_mask=roi_mask,\nmean_shape=mean_shape,\nshape_prior=shape_prior,\ngt_2D=roi_coord_2d,\n)\np_green_R_vec = output_dict[\"p_green_R\"].detach().cpu()\np_red_R_vec = output_dict[\"p_red_R\"].detach().cpu()\np_T = output_dict[\"Pred_T\"].detach().cpu()\nf_green_R = output_dict[\"f_green_R\"].detach().cpu()\nf_red_R = output_dict[\"f_red_R\"].detach().cpu()\nsym = torch.FloatTensor(rbppose.get_sym_info(category_str)).unsqueeze(0)\npred_RT = rbppose.generate_RT(\n[p_green_R_vec, p_red_R_vec],\n[f_green_R, f_red_R],\np_T,\nmode=\"vec\",\nsym=sym,\n)[0]\nposition = output_dict[\"Pred_T\"][0].detach().cpu()\norientation_mat = pred_RT[:3, :3].detach().cpu()\norientation = Rotation.from_matrix(orientation_mat.numpy())\norientation_q = torch.FloatTensor(orientation.as_quat())\nextents = output_dict[\"Pred_s\"][0].detach().cpu()\nscale = torch.linalg.norm(extents)\nreconstructed_points = output_dict[\"recon_model\"][0].detach().cpu()\nreconstructed_points *= scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents = torch.FloatTensor([extents[2], extents[1], extents[0]])\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for RBPPose.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>File path for model weights.</p> <p> TYPE: <code>str</code> </p> <code>model_url</code> <p>URL to download model weights if file is not found.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape</code> <p>File path for mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_url</code> <p>URL to download mean shape file if it is not found.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for RBPPose.\n    Attributes:\n        model: File path for model weights.\n        model_url: URL to download model weights if file is not found.\n        mean_shape: File path for mean shape file.\n        mean_shape_url: URL to download mean shape file if it is not found.\n        device: Device string for the model.\n    \"\"\"\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\ndevice: str\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load RBPPose model.</p> PARAMETER DESCRIPTION <code>config</code> <p>RBPPose configuration. See RBPPose.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load RBPPose model.\n    Args:\n        config: RBPPose configuration. See RBPPose.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=RBPPose.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n    Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py\n    \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\n# Handle camera information\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(0)\nwidth = self._camera.width\nheight = self._camera.height\ncamera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\ncamera_matrix = torch.FloatTensor(camera_matrix).unsqueeze(0).to(self._device)\n# Prepare RGB crop (not used by default config)\nrgb_cv = color_image.numpy()[:, :, ::-1]  # BGR\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = rbppose.get_bbox([y1, x1, y2, x2])\ncx = 0.5 * (cmin + cmax)\ncy = 0.5 * (rmin + rmax)\nbbox_center = np.array([cx, cy])  # (w/2, h/2)\nscale = min(max(cmax - cmin, rmax - rmin), max(height, width))\nrgb_crop = rbppose.crop_resize_by_warp_affine(\nrgb_cv,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n).transpose(2, 0, 1)\nrgb_crop = torch.FloatTensor(rgb_crop).unsqueeze(0).to(self._device)\n# Prepare depth crop (expected in mm)\ndepth_cv = depth_image.numpy() * 1000\ndepth_crop = rbppose.crop_resize_by_warp_affine(\ndepth_cv,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n)\ndepth_crop = torch.FloatTensor(depth_crop)[None, None].to(self._device)\n# Prepare category\ncategory_input = torch.LongTensor([category_id]).to(self._device)\n# Prepare ROI Mask\nmask_np = instance_mask.float().numpy()\nroi_mask = rbppose.crop_resize_by_warp_affine(\nmask_np,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n)\nroi_mask = torch.FloatTensor(roi_mask)[None, None].to(self._device)\n# Prepare mean shape (size?)\nmean_shape = rbppose.get_mean_shape(category_str) / 1000.0\nmean_shape = torch.FloatTensor(mean_shape).unsqueeze(0).to(self._device)\n# Prepare shape prior\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\nshape_prior = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Prepare 2D coordinates\ncoord_2d = rbppose.get_2d_coord_np(width, height).transpose(1, 2, 0)\nroi_coord_2d = rbppose.crop_resize_by_warp_affine(\ncoord_2d,\nbbox_center,\nscale,\nrbppose.FLAGS.img_size,\ninterpolation=cv2.INTER_NEAREST,\n).transpose(2, 0, 1)\nroi_coord_2d = torch.FloatTensor(roi_coord_2d).unsqueeze(0).to(self._device)\noutput_dict = self._net(\nrgb=rgb_crop,\ndepth=depth_crop,\nobj_id=category_input,\ncamK=camera_matrix,\ndef_mask=roi_mask,\nmean_shape=mean_shape,\nshape_prior=shape_prior,\ngt_2D=roi_coord_2d,\n)\np_green_R_vec = output_dict[\"p_green_R\"].detach().cpu()\np_red_R_vec = output_dict[\"p_red_R\"].detach().cpu()\np_T = output_dict[\"Pred_T\"].detach().cpu()\nf_green_R = output_dict[\"f_green_R\"].detach().cpu()\nf_red_R = output_dict[\"f_red_R\"].detach().cpu()\nsym = torch.FloatTensor(rbppose.get_sym_info(category_str)).unsqueeze(0)\npred_RT = rbppose.generate_RT(\n[p_green_R_vec, p_red_R_vec],\n[f_green_R, f_red_R],\np_T,\nmode=\"vec\",\nsym=sym,\n)[0]\nposition = output_dict[\"Pred_T\"][0].detach().cpu()\norientation_mat = pred_RT[:3, :3].detach().cpu()\norientation = Rotation.from_matrix(orientation_mat.numpy())\norientation_q = torch.FloatTensor(orientation.as_quat())\nextents = output_dict[\"Pred_s\"][0].detach().cpu()\nscale = torch.linalg.norm(extents)\nreconstructed_points = output_dict[\"recon_model\"][0].detach().cpu()\nreconstructed_points *= scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents = torch.FloatTensor([extents[2], extents[1], extents[0]])\nreturn {\n\"position\": position,\n\"orientation\": orientation_q,\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/","title":"sdfest.py","text":""},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest","title":"cpas_toolbox.cpas_methods.sdfest","text":"<p>This module defines SDFEst interface.</p> <p>Method is described in SDFEst: Categorical Pose and Shape Estimation of Objects From RGB-D Using Signed Distance Fields, Bruns, 2022.</p> <p>Implementation based on https://github.com/roym899/sdfest/</p>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst","title":"SDFEst","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for SDFEst.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>class SDFEst(CPASMethod):\n\"\"\"Wrapper class for SDFEst.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for SDFEst.\n        All keys supported by SDFPipeline are supported and will overwrite config\n        contained in sdfest_... files. The keys specified here are used by this\n        script only.\n        The two keys sdfest_..._config_files  will be parsed with SDFEst install\n        directory as part of the search paths. This allows to use the default config\n        that comes with SDFEst installation.\n        Attributes:\n            sdfest_default_config_file: Default configuration file loaded first.\n            sdfest_category_config_files: Per-category configuration file loaded second.\n            device: Device used for computation.\n            num_points: Numbner of points extracted from mesh.\n            prior: Prior distribution to modify orientation distribution.\n            visualize_optimization:\n                Whether to show additional optimization visualization.\n        \"\"\"\ndefault_config: Config = {\n\"sdfest_default_config_file\": \"estimation/configs/default.yaml\",\n\"sdfest_category_config_files\": {\n\"bottle\": \"estimation/configs/models/bottle.yaml\",\n\"bowl\": \"estimation/configs/models/bowl.yaml\",\n\"laptop\": \"estimation/configs/models/laptop.yaml\",\n\"can\": \"estimation/configs/models/can.yaml\",\n\"camera\": \"estimation/configs/models/camera.yaml\",\n\"mug\": \"estimation/configs/models/mug.yaml\",\n},\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SDFEst models.\n        Configuration loaded in following order\n            sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys\n        I.e., keys specified directly will take precedence over keys specified in\n        default file.\n        \"\"\"\ndefault_config = yoco.load_config_from_file(\nconfig[\"sdfest_default_config_file\"],\nsearch_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n)\nself._pipeline_dict = {}  # maps category to category-specific pipeline\nself._device = config[\"device\"]\nself._visualize_optimization = config[\"visualize_optimization\"]\nself._num_points = config[\"num_points\"]\nself._prior = config[\"prior\"] if \"prior\" in config else None\n# create per-categry models\nfor category_str in config[\"sdfest_category_config_files\"].keys():\ncategory_config = yoco.load_config_from_file(\nconfig[\"sdfest_category_config_files\"][category_str],\ncurrent_dict=default_config,\nsearch_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n)\ncategory_config = yoco.load_config(config, category_config)\nself._pipeline_dict[category_str] = SDFPipeline(category_config)\nself._pipeline_dict[category_str].cam = camera\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\"\"\"\n# skip unsupported category\nif category_str not in self._pipeline_dict:\nreturn {\n\"position\": torch.tensor([0, 0, 0]),\n\"orientation\": torch.tensor([0, 0, 0, 1]),\n\"extents\": torch.tensor([1, 1, 1]),\n\"reconstructed_pointcloud\": torch.tensor([[0, 0, 0]]),\n\"reconstructed_mesh\": None,\n}\npipeline = self._pipeline_dict[category_str]\n# move inputs to device\ncolor_image = color_image.to(self._device)\ndepth_image = depth_image.to(self._device, copy=True)\ninstance_mask = instance_mask.to(self._device)\nif self._prior is not None:\nprior = torch.tensor(self._prior[category_str], device=self._device)\nprior /= torch.sum(prior)\nelse:\nprior = None\nposition, orientation, scale, shape = pipeline(\ndepth_image,\ninstance_mask,\ncolor_image,\nvisualize=self._visualize_optimization,\nprior_orientation_distribution=prior,\n)\n# outputs of SDFEst are OpenGL camera, ShapeNet object convention\nposition_cv = pointset_utils.change_position_camera_convention(\nposition[0], \"opengl\", \"opencv\"\n)\norientation_cv = pointset_utils.change_orientation_camera_convention(\norientation[0], \"opengl\", \"opencv\"\n)\n# reconstruction + extent\nmesh = pipeline.generate_mesh(shape, scale, True).get_transformed_o3d_geometry()\nreconstructed_points = torch.from_numpy(\nnp.asarray(mesh.sample_points_uniformly(self._num_points).points)\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position_cv.detach().cpu(),\n\"orientation\": orientation_cv.detach().cpu(),\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": mesh,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SDFEst.</p> <p>All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only.</p> <p>The two keys sdfest_..._config_files  will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation.</p> ATTRIBUTE DESCRIPTION <code>sdfest_default_config_file</code> <p>Default configuration file loaded first.</p> <p> </p> <code>sdfest_category_config_files</code> <p>Per-category configuration file loaded second.</p> <p> </p> <code>device</code> <p>Device used for computation.</p> <p> </p> <code>num_points</code> <p>Numbner of points extracted from mesh.</p> <p> </p> <code>prior</code> <p>Prior distribution to modify orientation distribution.</p> <p> </p> <code>visualize_optimization</code> <p>Whether to show additional optimization visualization.</p> <p> </p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for SDFEst.\n    All keys supported by SDFPipeline are supported and will overwrite config\n    contained in sdfest_... files. The keys specified here are used by this\n    script only.\n    The two keys sdfest_..._config_files  will be parsed with SDFEst install\n    directory as part of the search paths. This allows to use the default config\n    that comes with SDFEst installation.\n    Attributes:\n        sdfest_default_config_file: Default configuration file loaded first.\n        sdfest_category_config_files: Per-category configuration file loaded second.\n        device: Device used for computation.\n        num_points: Numbner of points extracted from mesh.\n        prior: Prior distribution to modify orientation distribution.\n        visualize_optimization:\n            Whether to show additional optimization visualization.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SDFEst models.</p> <p>Configuration loaded in following order     sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys I.e., keys specified directly will take precedence over keys specified in default file.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SDFEst models.\n    Configuration loaded in following order\n        sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys\n    I.e., keys specified directly will take precedence over keys specified in\n    default file.\n    \"\"\"\ndefault_config = yoco.load_config_from_file(\nconfig[\"sdfest_default_config_file\"],\nsearch_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n)\nself._pipeline_dict = {}  # maps category to category-specific pipeline\nself._device = config[\"device\"]\nself._visualize_optimization = config[\"visualize_optimization\"]\nself._num_points = config[\"num_points\"]\nself._prior = config[\"prior\"] if \"prior\" in config else None\n# create per-categry models\nfor category_str in config[\"sdfest_category_config_files\"].keys():\ncategory_config = yoco.load_config_from_file(\nconfig[\"sdfest_category_config_files\"][category_str],\ncurrent_dict=default_config,\nsearch_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n)\ncategory_config = yoco.load_config(config, category_config)\nself._pipeline_dict[category_str] = SDFPipeline(category_config)\nself._pipeline_dict[category_str].cam = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See CPASMethod.inference.\"\"\"\n# skip unsupported category\nif category_str not in self._pipeline_dict:\nreturn {\n\"position\": torch.tensor([0, 0, 0]),\n\"orientation\": torch.tensor([0, 0, 0, 1]),\n\"extents\": torch.tensor([1, 1, 1]),\n\"reconstructed_pointcloud\": torch.tensor([[0, 0, 0]]),\n\"reconstructed_mesh\": None,\n}\npipeline = self._pipeline_dict[category_str]\n# move inputs to device\ncolor_image = color_image.to(self._device)\ndepth_image = depth_image.to(self._device, copy=True)\ninstance_mask = instance_mask.to(self._device)\nif self._prior is not None:\nprior = torch.tensor(self._prior[category_str], device=self._device)\nprior /= torch.sum(prior)\nelse:\nprior = None\nposition, orientation, scale, shape = pipeline(\ndepth_image,\ninstance_mask,\ncolor_image,\nvisualize=self._visualize_optimization,\nprior_orientation_distribution=prior,\n)\n# outputs of SDFEst are OpenGL camera, ShapeNet object convention\nposition_cv = pointset_utils.change_position_camera_convention(\nposition[0], \"opengl\", \"opencv\"\n)\norientation_cv = pointset_utils.change_orientation_camera_convention(\norientation[0], \"opengl\", \"opencv\"\n)\n# reconstruction + extent\nmesh = pipeline.generate_mesh(shape, scale, True).get_transformed_o3d_geometry()\nreconstructed_points = torch.from_numpy(\nnp.asarray(mesh.sample_points_uniformly(self._num_points).points)\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": position_cv.detach().cpu(),\n\"orientation\": orientation_cv.detach().cpu(),\n\"extents\": extents,\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": mesh,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/","title":"sgpa.py","text":""},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa","title":"cpas_toolbox.cpas_methods.sgpa","text":"<p>This module defines CR-Net interface.</p> <p>Method is described in SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation, Chen, 2021.</p> <p>Implementation based on https://github.com/ck-kai/SGPA</p>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA","title":"SGPA","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for SGPA.</p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>class SGPA(CPASMethod):\n\"\"\"Wrapper class for SGPA.\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for SGPA.\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            num_input_points: Number of 3D input points.\n            num_structure_points: Number of keypoints used for pose estimation.\n            image_size: Image size consumed by network (crop will be resized to this).\n            model: File path for model weights.\n            model_url: URL to download model weights if file is not found.\n            mean_shape: File path for mean shape file.\n            mean_shape_url: URL to download mean shape file if it is not found.\n            device: Device string for the model.\n        \"\"\"\nmodel: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nnum_structure_points: int\nimage_size: int\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\ndevice: str\ndefault_config: Config = {\n\"model\": None,\n\"num_categories\": None,\n\"num_shape_points\": None,\n\"num_input_points\": None,\n\"num_structure_points\": None,\n\"image_size\": None,\n\"model\": None,\n\"model_url\": None,\n\"mean_shape\": None,\n\"mean_shape_url\": None,\n\"device\": \"cuda\",\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SGPA model.\n        Args:\n            config: SGPA configuration. See SGPA.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=SGPA.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_path = utils.resolve_path(config[\"model\"])\nself._model_url = config[\"model_url\"]\nself._mean_shape_path = utils.resolve_path(config[\"mean_shape\"])\nself._mean_shape_url = config[\"mean_shape_url\"]\nself._check_paths()\nself._sgpa = sgpa.SGPANet(\nconfig[\"num_categories\"],\nconfig[\"num_shape_points\"],\nnum_structure_points=config[\"num_structure_points\"],\n)\nself._sgpa.cuda()\nself._sgpa = torch.nn.DataParallel(self._sgpa, device_ids=[self._device])\nself._sgpa.load_state_dict(\ntorch.load(self._model_path, map_location=self._device)\n)\nself._sgpa.eval()\nself._mean_shape_pointsets = np.load(self._mean_shape_path)\nself._num_input_points = config[\"num_input_points\"]\nself._image_size = config[\"image_size\"]\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_path) or not os.path.exists(\nself._mean_shape_path\n):\nprint(\"SGPA model weights not found, do you want to download to \")\nprint(\"  \", self._model_path)\nprint(\"  \", self._mean_shape_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"SGPA model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_path):\nos.makedirs(os.path.dirname(self._model_path), exist_ok=True)\nutils.download(\nself._model_url,\nself._model_path,\n)\nif not os.path.exists(self._mean_shape_path):\nos.makedirs(os.path.dirname(self._mean_shape_path), exist_ok=True)\nutils.download(\nself._mean_shape_url,\nself._mean_shape_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n        Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n        \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = sgpa.get_bbox([y1, x1, y2, x2])\nvalid_mask = (depth_image != 0) * instance_mask\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\n# if len(choose) &lt; 32:\n#     f_sRT[i] = np.identity(4, dtype=float)\n#     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n#     continue\n# else:\n#     valid_inst.append(i)\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# Move inputs to device and convert to right shape\ncolor_input = color_input.unsqueeze(0).to(self._device)\npoints = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.cuda.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call SGPA\n_, assign_matrix, deltas = self._sgpa(\npoints,\ncolor_input,\npoint_indices,\ncategory_id,\nmean_shape_pointset,\n)\n# Postprocess output\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = sgpa.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SGPA.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>num_input_points</code> <p>Number of 3D input points.</p> <p> TYPE: <code>int</code> </p> <code>num_structure_points</code> <p>Number of keypoints used for pose estimation.</p> <p> TYPE: <code>int</code> </p> <code>image_size</code> <p>Image size consumed by network (crop will be resized to this).</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>File path for model weights.</p> <p> TYPE: <code>str</code> </p> <code>model_url</code> <p>URL to download model weights if file is not found.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape</code> <p>File path for mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_url</code> <p>URL to download mean shape file if it is not found.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for SGPA.\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        num_input_points: Number of 3D input points.\n        num_structure_points: Number of keypoints used for pose estimation.\n        image_size: Image size consumed by network (crop will be resized to this).\n        model: File path for model weights.\n        model_url: URL to download model weights if file is not found.\n        mean_shape: File path for mean shape file.\n        mean_shape_url: URL to download mean shape file if it is not found.\n        device: Device string for the model.\n    \"\"\"\nmodel: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nnum_structure_points: int\nimage_size: int\nmodel: str\nmodel_url: str\nmean_shape: str\nmean_shape_url: str\ndevice: str\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SGPA model.</p> PARAMETER DESCRIPTION <code>config</code> <p>SGPA configuration. See SGPA.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SGPA model.\n    Args:\n        config: SGPA configuration. See SGPA.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=SGPA.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n    Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n    \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# Get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = sgpa.get_bbox([y1, x1, y2, x2])\nvalid_mask = (depth_image != 0) * instance_mask\n# Prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\n# Prepare input points\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\n# if len(choose) &lt; 32:\n#     f_sRT[i] = np.identity(4, dtype=float)\n#     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n#     continue\n# else:\n#     valid_inst.append(i)\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# Move inputs to device and convert to right shape\ncolor_input = color_input.unsqueeze(0).to(self._device)\npoints = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.cuda.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call SGPA\n_, assign_matrix, deltas = self._sgpa(\npoints,\ncolor_input,\npoint_indices,\ncategory_id,\nmean_shape_pointset,\n)\n# Postprocess output\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = sgpa.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/","title":"spd.py","text":""},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd","title":"cpas_toolbox.cpas_methods.spd","text":"<p>This module defines SPD interface.</p> <p>Method is described in Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation, Tian, 2020.</p> <p>Implementation based on https://github.com/mentian/object-deformnet</p>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD","title":"SPD","text":"<p>         Bases: <code>CPASMethod</code></p> <p>Wrapper class for Shape Prior Deformation (SPD).</p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>class SPD(CPASMethod):\n\"\"\"Wrapper class for Shape Prior Deformation (SPD).\"\"\"\nclass Config(TypedDict):\n\"\"\"Configuration dictionary for SPD.\n        Attributes:\n            model_file: Path to model.\n            mean_shape_file: Path to mean shape file.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            num_input_points: Number of input points.\n            image_size: Size of image crop.\n            device: Device string for the model.\n        \"\"\"\nmodel_file: str\nmean_shape_file: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nimage_size: int\ndevice: str\ndefault_config: Config = {\n\"model_file\": None,\n\"mean_shape_file\": None,\n\"num_categories\": None,\n\"num_shape_points\": None,\n\"num_input_points\": None,\n\"image_size\": None,\n\"device\": \"cuda\",\n}\ndef __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SPD model.\n        Args:\n            config: SPD configuration. See SPD.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=SPD.default_config)\nself._parse_config(config)\nself._camera = camera\ndef _parse_config(self, config: Config) -&gt; None:\nself._device = config[\"device\"]\nself._model_file_path = utils.resolve_path(config[\"model_file\"])\nself._mean_shape_file_path = utils.resolve_path(config[\"mean_shape_file\"])\nself._check_paths()\nself._spd_net = spd.DeformNet(\nconfig[\"num_categories\"], config[\"num_shape_points\"]\n)\nself._spd_net.to(self._device)\nself._spd_net.load_state_dict(\ntorch.load(self._model_file_path, map_location=self._device)\n)\nself._spd_net.eval()\nself._mean_shape_pointsets = np.load(self._mean_shape_file_path)\nself._num_input_points = config[\"num_input_points\"]\nself._image_size = config[\"image_size\"]\ndef _check_paths(self) -&gt; None:\nif not os.path.exists(self._model_file_path) or not os.path.exists(\nself._mean_shape_file_path\n):\nprint(\"SPD model weights not found, do you want to download to \")\nprint(\"  \", self._model_file_path)\nprint(\"  \", self._mean_shape_file_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_weights()\nbreak\nelif decision == \"n\":\nprint(\"SPD model weights not found. Aborting.\")\nexit(0)\ndef _download_weights(self) -&gt; None:\nif not os.path.exists(self._model_file_path):\nos.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\ndownload_dir_path = os.path.dirname(self._model_file_path)\nzip_path = os.path.join(download_dir_path, \"temp.zip\")\nutils.download(\n\"https://drive.google.com/u/0/uc?id=1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc&amp;\"\n\"export=download\",\nzip_path,\n)\nz = zipfile.ZipFile(zip_path)\nz.extract(\"deformnet_eval/real/model_50.pth\", download_dir_path)\nz.close()\nos.remove(zip_path)\nshutil.move(\nos.path.join(\ndownload_dir_path, \"deformnet_eval\", \"real\", \"model_50.pth\"\n),\ndownload_dir_path,\n)\nshutil.rmtree(os.path.join(download_dir_path, \"deformnet_eval\"))\nif not os.path.exists(self._mean_shape_file_path):\nos.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\nutils.download(\n\"https://github.com/mentian/object-deformnet/raw/master/assets/\"\n\"mean_points_emb.npy\",\nself._mean_shape_file_path,\n)\ndef inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See MethodWrapper.inference.\n        Based on spd.evaluate.\n        \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = spd.get_bbox([y1, x1, y2, x2])\nbb_mask = torch.zeros_like(depth_image)\nbb_mask[rmin:rmax, cmin:cmax] = 1.0\nvalid_mask = (depth_image != 0) * instance_mask\n# prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\ncolor_input = color_input.unsqueeze(0)  # add batch dim\n# convert depth to pointcloud\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# move inputs to device\ncolor_input = color_input.to(self._device)\npoints = torch.Tensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.Tensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call SPD network\nassign_matrix, deltas = self._spd_net(\npoints, color_input, point_indices, category_id, mean_shape_pointset\n)\n# Postprocess outputs\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = spd.align.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SPD.</p> ATTRIBUTE DESCRIPTION <code>model_file</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_file</code> <p>Path to mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>num_input_points</code> <p>Number of input points.</p> <p> TYPE: <code>int</code> </p> <code>image_size</code> <p>Size of image crop.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>class Config(TypedDict):\n\"\"\"Configuration dictionary for SPD.\n    Attributes:\n        model_file: Path to model.\n        mean_shape_file: Path to mean shape file.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        num_input_points: Number of input points.\n        image_size: Size of image crop.\n        device: Device string for the model.\n    \"\"\"\nmodel_file: str\nmean_shape_file: str\nnum_categories: int\nnum_shape_points: int\nnum_input_points: int\nimage_size: int\ndevice: str\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SPD model.</p> PARAMETER DESCRIPTION <code>config</code> <p>SPD configuration. See SPD.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>camera_utils.Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n\"\"\"Initialize and load SPD model.\n    Args:\n        config: SPD configuration. See SPD.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=SPD.default_config)\nself._parse_config(config)\nself._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.inference","title":"inference","text":"<pre><code>inference(\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See MethodWrapper.inference.</p> <p>Based on spd.evaluate.</p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>def inference(\nself,\ncolor_image: torch.Tensor,\ndepth_image: torch.Tensor,\ninstance_mask: torch.Tensor,\ncategory_str: str,\n) -&gt; PredictionDict:\n\"\"\"See MethodWrapper.inference.\n    Based on spd.evaluate.\n    \"\"\"\ncategory_str_to_id = {\n\"bottle\": 0,\n\"bowl\": 1,\n\"camera\": 2,\n\"can\": 3,\n\"laptop\": 4,\n\"mug\": 5,\n}\ncategory_id = category_str_to_id[category_str]\nmean_shape_pointset = self._mean_shape_pointsets[category_id]\n# get bounding box\nx1 = min(instance_mask.nonzero()[:, 1]).item()\ny1 = min(instance_mask.nonzero()[:, 0]).item()\nx2 = max(instance_mask.nonzero()[:, 1]).item()\ny2 = max(instance_mask.nonzero()[:, 0]).item()\nrmin, rmax, cmin, cmax = spd.get_bbox([y1, x1, y2, x2])\nbb_mask = torch.zeros_like(depth_image)\nbb_mask[rmin:rmax, cmin:cmax] = 1.0\nvalid_mask = (depth_image != 0) * instance_mask\n# prepare image crop\ncolor_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\ncolor_input = cv2.resize(\ncolor_input,\n(self._image_size, self._image_size),\ninterpolation=cv2.INTER_LINEAR,\n)\ncolor_input = TF.normalize(\nTF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\nmean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225],\n)\ncolor_input = color_input.unsqueeze(0)  # add batch dim\n# convert depth to pointcloud\nfx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\nwidth = self._camera.width\nheight = self._camera.height\npoint_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\nxmap = np.array([[i for i in range(width)] for _ in range(height)])\nymap = np.array([[j for _ in range(width)] for j in range(height)])\nif len(point_indices) &gt; self._num_input_points:\n# take subset of points if two many depth points\npoint_indices_mask = np.zeros(len(point_indices), dtype=int)\npoint_indices_mask[: self._num_input_points] = 1\nnp.random.shuffle(point_indices_mask)\npoint_indices = point_indices[point_indices_mask.nonzero()]\nelse:\npoint_indices = np.pad(\npoint_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n)  # repeat points if not enough depth observation\ndepth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n:, None\n]\nxmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\nymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\npt2 = depth_masked.numpy()\npt0 = (xmap_masked - cx) * pt2 / fx\npt1 = (ymap_masked - cy) * pt2 / fy\npoints = np.concatenate((pt0, pt1, pt2), axis=1)\n# adjust indices for resizing of color image\ncrop_w = rmax - rmin\nratio = self._image_size / crop_w\ncol_idx = point_indices % crop_w\nrow_idx = point_indices // crop_w\npoint_indices = (\nnp.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n).astype(np.int64)\n# move inputs to device\ncolor_input = color_input.to(self._device)\npoints = torch.Tensor(points).unsqueeze(0).to(self._device)\npoint_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\ncategory_id = torch.LongTensor([category_id]).to(self._device)\nmean_shape_pointset = (\ntorch.Tensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n)\n# Call SPD network\nassign_matrix, deltas = self._spd_net(\npoints, color_input, point_indices, category_id, mean_shape_pointset\n)\n# Postprocess outputs\ninst_shape = mean_shape_pointset + deltas\nassign_matrix = torch.softmax(assign_matrix, dim=2)\ncoords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\npoint_indices = point_indices[0].cpu().numpy()\n_, point_indices = np.unique(point_indices, return_index=True)\nnocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\nextents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\npoints = points[0, point_indices, :].cpu().numpy()\nscale, orientation_m, position, _ = spd.align.estimateSimilarityTransform(\nnocs_coords, points\n)\norientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\nreconstructed_points = inst_shape[0].detach().cpu() * scale\n# Recenter for mug category\nif category_str == \"mug\":  # undo mug translation\nx_offset = (\n(\nself._mean_shape_pointsets[5].max(axis=0)[0]\n+ self._mean_shape_pointsets[5].min(axis=0)[0]\n)\n/ 2\n* scale\n)\nreconstructed_points[:, 0] -= x_offset\nposition += quaternion_utils.quaternion_apply(\norientation_q, torch.FloatTensor([x_offset, 0, 0])\n).numpy()\n# NOCS Object -&gt; ShapeNet Object convention\nobj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\norientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\nreconstructed_points = quaternion_utils.quaternion_apply(\nquaternion_utils.quaternion_invert(obj_fix),\nreconstructed_points,\n)\nextents, _ = reconstructed_points.abs().max(dim=0)\nextents *= 2.0\nreturn {\n\"position\": torch.Tensor(position),\n\"orientation\": orientation_q,\n\"extents\": torch.Tensor(extents),\n\"reconstructed_pointcloud\": reconstructed_points,\n\"reconstructed_mesh\": None,\n}\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/","title":"nocs_dataset.py","text":""},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset","title":"cpas_toolbox.datasets.nocs_dataset","text":"<p>Module providing dataset class for NOCS datasets (CAMERA / REAL).</p>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset","title":"NOCSDataset","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>Dataset class for NOCS dataset.</p> <p>CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275.</p> <p>Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master</p> Expected directory format <p>{root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/...</p> <p>Which is easily obtained by downloading all the provided files and extracting them into the same directory.</p> <p>Necessary preprocessing of this data is performed during first initialization per and is saved to     {root_dir}/cpas_toolbox/...</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class NOCSDataset(torch.utils.data.Dataset):\n\"\"\"Dataset class for NOCS dataset.\n    CAMERA* and REAL* are training sets.\n    CAMERA25 and REAL275 are test data.\n    Some papers use CAMERA25 as validation when benchmarking REAL275.\n    Datasets can be found here:\n    https://github.com/hughw19/NOCS_CVPR2019/tree/master\n    Expected directory format:\n        {root_dir}/real_train/...\n        {root_dir}/real_test/...\n        {root_dir}/gts/...\n        {root_dir}/obj_models/...\n        {root_dir}/camera_composed_depth/...\n        {root_dir}/val/...\n        {root_dir}/train/...\n        {root_dir}/fixed_real_test_obj_models/...\n    Which is easily obtained by downloading all the provided files and extracting them\n    into the same directory.\n    Necessary preprocessing of this data is performed during first initialization per\n    and is saved to\n        {root_dir}/cpas_toolbox/...\n    \"\"\"\nnum_categories = 7\ncategory_id_to_str = {\n0: \"unknown\",\n1: \"bottle\",\n2: \"bowl\",\n3: \"camera\",\n4: \"can\",\n5: \"laptop\",\n6: \"mug\",\n}\ncategory_str_to_id = {v: k for k, v in category_id_to_str.items()}\nclass Config(TypedDict, total=False):\n\"\"\"Configuration dictionary for NOCSDataset.\n        Attributes:\n            root_dir: See NOCSDataset docstring.\n            split:\n                The dataset split. The following strings are supported:\n                    \"camera_train\": 275000 images, synthetic objects + real background\n                    \"camera_val\": 25000 images, synthetic objects + real background\n                    \"real_train\": 4300 images in 7 scenes, real\n                    \"real_test\": 2750 images in 6 scenes, real\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. Currently only \"quaternion\"\n                supported.\n            remap_y_axis:\n                If not None, the NOCS y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See NOCSDataset.category_id_to_str for admissible category strings.\n        \"\"\"\nroot_dir: str\nsplit: str\nmask_pointcloud: bool\nnormalize_pointcloud: bool\nscale_convention: str\ncamera_convention: str\norientation_repr: str\nremap_y_axis: Optional[str]\nremap_x_axis: Optional[str]\ncategory_str: Optional[str]\ndefault_config: Config = {\n\"root_dir\": None,\n\"split\": None,\n\"mask_pointcloud\": False,\n\"normalize_pointcloud\": False,\n\"camera_convention\": \"opengl\",\n\"scale_convention\": \"half_max\",\n\"orientation_repr\": \"quaternion\",\n\"category_str\": None,\n\"remap_y_axis\": None,\n\"remap_x_axis\": None,\n}\ndef __init__(\nself,\nconfig: Config,\n) -&gt; None:\n\"\"\"Initialize the dataset.\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See NOCSDataset.Config for supported keys.\n        \"\"\"\nconfig = yoco.load_config(config, current_dict=NOCSDataset.default_config)\nself._root_dir_path = utils.resolve_path(config[\"root_dir\"])\nself._split = config[\"split\"]\nself._check_dirs()\nself._camera_convention = config[\"camera_convention\"]\nself._camera = self._get_split_camera()\nself._preprocess_path = os.path.join(\nself._root_dir_path, \"cpas_toolbox\", self._split\n)\nif not os.path.isdir(self._preprocess_path):\nself._preprocess_dataset()\nself._mask_pointcloud = config[\"mask_pointcloud\"]\nself._normalize_pointcloud = config[\"normalize_pointcloud\"]\nself._scale_convention = config[\"scale_convention\"]\nself._sample_files = self._get_sample_files(config[\"category_str\"])\nself._remap_y_axis = config[\"remap_y_axis\"]\nself._remap_x_axis = config[\"remap_x_axis\"]\nself._orientation_repr = config[\"orientation_repr\"]\ndef _check_dirs(self) -&gt; None:\ndirectories = self._get_dirs()\n# required directories\nif all(os.path.exists(directory) for directory in directories):\npass\nelse:\nprint(\nf\"NOCS dataset ({self._split} split) not found, do you want to download\"\n\" it into the following directory:\"\n)\nprint(\"  \", self._root_dir_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_dataset()\nbreak\nelif decision == \"n\":\nprint(\"Dataset not found. Aborting.\")\nexit(0)\ndef _get_dirs(self) -&gt; List[str]:\n\"\"\"Return required list of directories for current split.\"\"\"\ndirs = []\n# gts only available for real test and camera val\nif self._split in [\"real_test\", \"camera_val\"]:\ndirs.append(os.path.join(self._root_dir_path, \"gts\"))\ndirs.append(os.path.join(self._root_dir_path, \"obj_models\"))\n# Fixed object model, need to be downloaded separately\nif self._split == \"real_test\":\ndirs.append(os.path.join(self._root_dir_path, \"fixed_real_test_obj_models\"))\n# full depths for CAMERA\nif self._split in [\"camera_val\", \"camera_train\"]:\ndirs.append(os.path.join(self._root_dir_path, \"camera_full_depths\"))\nif self._split == \"camera_train\":\ndirs.append(os.path.join(self._root_dir_path, \"train\"))\nelif self._split == \"camera_val\":\ndirs.append(os.path.join(self._root_dir_path, \"val\"))\nelif self._split in [\"real_train\", \"real_test\"]:\ndirs.append(os.path.join(self._root_dir_path, self._split))\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\nreturn dirs\ndef _download_dataset(self) -&gt; None:\nmissing_dirs = [d for d in self._get_dirs() if not os.path.exists(d)]\nfor missing_dir in missing_dirs:\ndownload_dir, identifier = os.path.split(missing_dir)\nos.makedirs(download_dir, exist_ok=True)\nif identifier == \"gts\":\nzip_path = os.path.join(download_dir, \"gts.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/gts.zip\", zip_path\n)\nelif identifier == \"obj_models\":\nzip_path = os.path.join(download_dir, \"obj_models.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/obj_models.zip\",\nzip_path,\n)\nelif identifier == \"camera_full_depths\":\nzip_path = os.path.join(download_dir, \"camera_full_depths.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip\",\nzip_path,\n)\nz = zipfile.ZipFile(zip_path)\nelif identifier == \"train\":\nzip_path = os.path.join(download_dir, \"train.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/camera_train.zip\",\nzip_path,\n)\nelif identifier == \"val\":\nzip_path = os.path.join(download_dir, \"val.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip\",\nzip_path,\n)\nelif identifier == \"real_train\":\nzip_path = os.path.join(download_dir, \"real_train.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/real_train.zip\",\nzip_path,\n)\nelif identifier == \"real_test\":\nzip_path = os.path.join(download_dir, \"real_test.zip\")\nutils.download(\n\"http://download.cs.stanford.edu/orion/nocs/real_test.zip\", zip_path\n)\nelif identifier == \"fixed_real_test_obj_models\":\nzip_path = os.path.join(download_dir, \"fixed_real_test_obj_models.zip\")\nutils.download(\n\"https://drive.google.com/u/0/uc?id=1grAWfmWRm4gDmZnLRf9KF7-_eHX\"\n\"-12BO&amp;export=download\",\nzip_path,\n)\nelse:\nraise ValueError(f\"Downloading dir {missing_dir} unsupported.\")\nz = zipfile.ZipFile(zip_path)\nz.extractall(download_dir)\nz.close()\nos.remove(zip_path)\ndef __len__(self) -&gt; int:\n\"\"\"Return number of sample in dataset.\"\"\"\nreturn len(self._sample_files)\ndef __getitem__(self, idx: int) -&gt; dict:\n\"\"\"Return a sample of the dataset.\n        Args:\n            idx: Index of the instance.\n        Returns:\n            Sample containing the following items:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n        \"\"\"\nsample_file = self._sample_files[idx]\nsample_data = pickle.load(open(sample_file, \"rb\"))\nsample = self._sample_from_sample_data(sample_data)\nreturn sample\ndef _preprocess_dataset(self) -&gt; None:\n\"\"\"Create preprocessing files for the current split.\n        One file per sample, which currently means per valid object mask will be\n        created.\n        Preprocessing will be stored on disk to {root_dir}/cpas_toolbox/...\n        This function will not store the preprocessing, so it still has to be loaded\n        afterwards.\n        \"\"\"\nos.makedirs(self._preprocess_path)\nself._fix_obj_models()\nself._start_time = time.time()\nself._color_paths = self._get_color_files()\nParallel(n_jobs=-1)(\n(\ndelayed(self._preprocess_color_path)(i, color_path)\nfor i, color_path in enumerate(self._color_paths)\n)\n)\n# store dictionary to map category to files\nsample_files = self._get_sample_files()\ncategory_str_to_files = {\ncategory_str: [] for category_str in NOCSDataset.category_id_to_str.values()\n}\nfor sample_file in tqdm(sample_files):\nsample_data = pickle.load(open(sample_file, \"rb\"))\ncategory_id = sample_data[\"category_id\"]\ncategory_str = NOCSDataset.category_id_to_str[category_id]\n_, file_name = os.path.split(sample_file)\ncategory_str_to_files[category_str].append(file_name)\ncategory_json_path = os.path.join(self._preprocess_path, \"categories.json\")\nwith open(category_json_path, \"w\") as f:\njson.dump(dict(category_str_to_files), f)\nprint(f\"Finished preprocessing for {self._split}.\", end=\"\\033[K\\n\")\ndef _fix_obj_models(self) -&gt; None:\n\"\"\"Fix issues with fileextensions.\n        Some png files have jpg extension. This function fixes these models.\n        \"\"\"\nglob_pattern = os.path.join(self._root_dir_path, \"**\", \"*.jpg\")\nfiles = glob(glob_pattern, recursive=True)\nfor filepath in files:\nwhat = imghdr.what(filepath)\nif what == \"png\":\nprint(\"Fixing: \", filepath)\nobj_dir_path, problematic_filename = os.path.split(filepath)\nname, _ = problematic_filename.split(\".\")\nfixed_filename = f\"fixed_{name}.png\"\nfixed_filepath = os.path.join(obj_dir_path, fixed_filename)\nmtl_filepath = os.path.join(obj_dir_path, \"model.mtl\")\nbu_mtl_filepath = os.path.join(obj_dir_path, \"model.mtl.old\")\ncopyfile(mtl_filepath, bu_mtl_filepath)\ncopyfile(filepath, fixed_filepath)\ndef _update_preprocess_progress(self, image_id: int) -&gt; None:\ncurrent_time = time.time()\nduration = current_time - self._start_time\nimgs_per_sec = image_id / duration\nif image_id &gt; 10:\nremaining_imgs = len(self._color_paths) - image_id\nremaining_secs = remaining_imgs / imgs_per_sec\nremaining_time_str = str(datetime.timedelta(seconds=round(remaining_secs)))\nelse:\nremaining_time_str = \"N/A\"\nprint(\nf\"Preprocessing image: {image_id:&gt;10} / {len(self._color_paths)}\"\nf\" {image_id / len(self._color_paths) * 100:&gt;6.2f}%\"  # progress percentage\nf\" Remaining time: {remaining_time_str}\"  # remaining time\n\"\\033[K\",  # clear until end of line\nend=\"\\r\",  # overwrite previous\n)\ndef _preprocess_color_path(self, image_id: int, color_path: str) -&gt; None:\ncounter = 0\nself._update_preprocess_progress(image_id)\ndepth_path = self._depth_path_from_color_path(color_path)\nif not os.path.isfile(depth_path):\nprint(f\"Missing depth file {depth_path}. Skipping.\", end=\"\\033[K\\n\")\nreturn\nmask_path = self._mask_path_from_color_path(color_path)\nmeta_path = self._meta_path_from_color_path(color_path)\nmeta_data = pd.read_csv(\nmeta_path, sep=\" \", header=None, converters={2: lambda x: str(x)}\n)\ninstances_mask = self._load_mask(mask_path)\nmask_ids = np.unique(instances_mask).tolist()\ngt_id = 0  # GT only contains valid objects of interests and is 0-indexed\nfor mask_id in mask_ids:\nif mask_id == 255:  # 255 is background\ncontinue\nmatch = meta_data[meta_data.iloc[:, 0] == mask_id]\nif match.empty:\nprint(\nf\"Warning: mask {mask_id} not found in {meta_path}\", end=\"\\033[K\\n\"\n)\nelif match.shape[0] != 1:\nprint(\nf\"Warning: mask {mask_id} not unique in {meta_path}\", end=\"\\033[K\\n\"\n)\nmeta_row = match.iloc[0]\ncategory_id = meta_row.iloc[1]\nif category_id == 0:  # unknown / distractor object\ncontinue\ntry:\n(\nposition,\norientation_q,\nextents,\nnocs_transform,\n) = self._get_pose_and_scale(color_path, mask_id, gt_id, meta_row)\nexcept nocs_utils.PoseEstimationError:\nprint(\n\"Insufficient data for pose estimation. \"\nf\"Skipping {color_path}:{mask_id}.\",\nend=\"\\033[K\\n\",\n)\ncontinue\nexcept ObjectError:\nprint(\n\"Insufficient object mesh for pose estimation. \"\nf\"Skipping {color_path}:{mask_id}.\",\nend=\"\\033[K\\n\",\n)\ncontinue\nobj_path = self._get_obj_path(meta_row)\nsample_info = {\n\"color_path\": color_path,\n\"depth_path\": depth_path,\n\"mask_path\": mask_path,\n\"mask_id\": mask_id,\n\"category_id\": category_id,\n\"obj_path\": obj_path,\n\"nocs_transform\": nocs_transform,\n\"position\": position,\n\"orientation_q\": orientation_q,\n\"extents\": extents,\n\"nocs_scale\": torch.linalg.norm(extents),\n\"max_extent\": torch.max(extents),\n}\nout_file = os.path.join(\nself._preprocess_path, f\"{image_id:08}_{counter}.pkl\"\n)\npickle.dump(sample_info, open(out_file, \"wb\"))\ncounter += 1\ngt_id += 1\ndef _get_color_files(self) -&gt; list:\n\"\"\"Return list of paths of color images of the selected split.\"\"\"\nif self._split == \"camera_train\":\nglob_pattern = os.path.join(\nself._root_dir_path, \"train\", \"**\", \"*_color.png\"\n)\nreturn sorted(glob(glob_pattern, recursive=True))\nelif self._split == \"camera_val\":\nglob_pattern = os.path.join(self._root_dir_path, \"val\", \"**\", \"*_color.png\")\nreturn sorted(glob(glob_pattern, recursive=True))\nelif self._split == \"real_train\":\nglob_pattern = os.path.join(\nself._root_dir_path, \"real_train\", \"**\", \"*_color.png\"\n)\nreturn sorted(glob(glob_pattern, recursive=True))\nelif self._split == \"real_test\":\nglob_pattern = os.path.join(\nself._root_dir_path, \"real_test\", \"**\", \"*_color.png\"\n)\nreturn sorted(glob(glob_pattern, recursive=True))\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\ndef _get_sample_files(self, category_str: Optional[str] = None) -&gt; list:\n\"\"\"Return sorted list of sample file paths.\n        Sample files are generated by NOCSDataset._preprocess_dataset.\n        Args:\n            category_str:\n                If not None, only instances of the provided category will be returned.\n        Returns:\n            List of sample_data files.\n        \"\"\"\nglob_pattern = os.path.join(self._preprocess_path, \"*.pkl\")\nsample_files = glob(glob_pattern)\nsample_files.sort()\nif category_str is None:\nreturn sample_files\nif category_str not in NOCSDataset.category_str_to_id:\nraise ValueError(f\"Unsupported category_str {category_str}.\")\ncategory_json_path = os.path.join(self._preprocess_path, \"categories.json\")\nwith open(category_json_path, \"r\") as f:\ncategory_str_to_filenames = json.load(f)\nfiltered_sample_files = [\nos.path.join(self._preprocess_path, fn)\nfor fn in category_str_to_filenames[category_str]\n]\nreturn filtered_sample_files\ndef _get_split_camera(self) -&gt; None:\n\"\"\"Return camera information for selected split.\"\"\"\n# from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py\nif self._split in [\"real_train\", \"real_test\"]:\nreturn camera_utils.Camera(\nwidth=640,\nheight=480,\nfx=591.0125,\nfy=590.16775,\ncx=322.525,\ncy=244.11084,\npixel_center=0.0,\n)\nelif self._split in [\"camera_train\", \"camera_val\"]:\nreturn camera_utils.Camera(\nwidth=640,\nheight=480,\nfx=577.5,\nfy=577.5,\ncx=319.5,\ncy=239.5,\npixel_center=0.0,\n)\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\ndef _sample_from_sample_data(self, sample_data: dict) -&gt; dict:\n\"\"\"Create dictionary containing a single sample.\"\"\"\ncolor = torch.from_numpy(\nnp.asarray(Image.open(sample_data[\"color_path\"]), dtype=np.float32) / 255\n)\ndepth = self._load_depth(sample_data[\"depth_path\"])\ninstances_mask = self._load_mask(sample_data[\"mask_path\"])\ninstance_mask = instances_mask == sample_data[\"mask_id\"]\npointcloud_mask = instance_mask if self._mask_pointcloud else None\npointcloud = pointset_utils.depth_to_pointcloud(\ndepth,\nself._camera,\nmask=pointcloud_mask,\nconvention=self._camera_convention,\n)\n# adjust camera convention for position, orientation and scale\nposition = pointset_utils.change_position_camera_convention(\nsample_data[\"position\"], \"opencv\", self._camera_convention\n)\n# orientation / scale\norientation_q, extents = self._change_axis_convention(\nsample_data[\"orientation_q\"], sample_data[\"extents\"]\n)\norientation_q = pointset_utils.change_orientation_camera_convention(\norientation_q, \"opencv\", self._camera_convention\n)\norientation = self._quat_to_orientation_repr(orientation_q)\nscale = self._get_scale(sample_data, extents)\n# normalize pointcloud &amp; position\nif self._normalize_pointcloud:\npointcloud, centroid = pointset_utils.normalize_points(pointcloud)\nposition = position - centroid\nsample = {\n\"color\": color,\n\"depth\": depth,\n\"pointset\": pointcloud,\n\"mask\": instance_mask,\n\"position\": position,\n\"orientation\": orientation,\n\"quaternion\": orientation_q,\n\"scale\": scale,\n\"color_path\": sample_data[\"color_path\"],\n\"obj_path\": sample_data[\"obj_path\"],\n\"category_id\": sample_data[\"category_id\"],\n\"category_str\": NOCSDataset.category_id_to_str[sample_data[\"category_id\"]],\n}\nreturn sample\ndef _depth_path_from_color_path(self, color_path: str) -&gt; str:\n\"\"\"Return path to depth file from color filepath.\"\"\"\nif self._split in [\"real_train\", \"real_test\"]:\ndepth_path = color_path.replace(\"color\", \"depth\")\nelif self._split in [\"camera_train\"]:\ndepth_path = color_path.replace(\"color\", \"composed\")\ndepth_path = depth_path.replace(\"/train/\", \"/camera_full_depths/train/\")\nelif self._split in [\"camera_val\"]:\ndepth_path = color_path.replace(\"color\", \"composed\")\ndepth_path = depth_path.replace(\"/val/\", \"/camera_full_depths/val/\")\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\nreturn depth_path\ndef _mask_path_from_color_path(self, color_path: str) -&gt; str:\n\"\"\"Return path to mask file from color filepath.\"\"\"\nmask_path = color_path.replace(\"color\", \"mask\")\nreturn mask_path\ndef _meta_path_from_color_path(self, color_path: str) -&gt; str:\n\"\"\"Return path to meta file from color filepath.\"\"\"\nmeta_path = color_path.replace(\"color.png\", \"meta.txt\")\nreturn meta_path\ndef _nocs_map_path_from_color_path(self, color_path: str) -&gt; str:\n\"\"\"Return NOCS map filepath from color filepath.\"\"\"\nnocs_map_path = color_path.replace(\"color.png\", \"coord.png\")\nreturn nocs_map_path\ndef _get_pose_and_scale(\nself, color_path: str, mask_id: int, gt_id: int, meta_row: pd.Series\n) -&gt; tuple:\n\"\"\"Return position, orientation, scale and NOCS transform.\n        All of those follow OpenCV (x right, y down, z forward) convention.\n        Args:\n            color_path: Path to the color file.\n            mask_id: Instance id in the instances mask.\n            gt_id:\n                Ground truth id. This is 0-indexed id of valid instances in meta file.\n            meta_row:\n                Matching row of meta file. Contains necessary information about mesh.\n        Returns:\n            position (torch.Tensor):\n                Position of object center in camera frame. Shape (3,).\n            quaternion (torch.Tensor):\n                Orientation of object in camera frame.\n                Scalar-last quaternion, shape (4,).\n            extents (torch.Tensor):\n                Bounding box side lengths.\n            nocs_transformation (torch.Tensor):\n                Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera.\n        \"\"\"\ngts_path = self._get_gts_path(color_path)\nobj_path = self._get_obj_path(meta_row)\nif self._split == \"real_test\":\n# only use gt for real test data, since there are errors in camera val\ngts_data = pickle.load(open(gts_path, \"rb\"))\nnocs_transform = gts_data[\"gt_RTs\"][gt_id]\nposition = nocs_transform[0:3, 3]\nrot_scale = nocs_transform[0:3, 0:3]\nnocs_scales = np.sqrt(np.sum(rot_scale**2, axis=0))\nrotation_matrix = rot_scale / nocs_scales[:, None]\nnocs_scale = nocs_scales[0]\nelse:  # camera_train, camera_val, real_train\n# use ground truth NOCS mask to perform alignment\n(\nposition,\nrotation_matrix,\nnocs_scale,\nnocs_transform,\n) = self._estimate_object(color_path, mask_id)\norientation_q = Rotation.from_matrix(rotation_matrix).as_quat()\nmesh_extents = self._get_mesh_extents_from_obj(obj_path)\nif \"camera\" in self._split:\n# CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1\n# get metric extents by scaling with the diagonal\nextents = nocs_scale * mesh_extents\nelif \"real\" in self._split:\n# REAL object meshes are not normalized\nextents = mesh_extents\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\nposition = torch.Tensor(position)\norientation_q = torch.Tensor(orientation_q)\nextents = torch.Tensor(extents)\nnocs_transform = torch.Tensor(nocs_transform)\nreturn position, orientation_q, extents, nocs_transform\ndef _get_gts_path(self, color_path: str) -&gt; Optional[str]:\n\"\"\"Return path to gts file from color filepath.\n        Return None if split does not have ground truth information.\n        \"\"\"\nif self._split == \"real_test\":\ngts_dir_path = os.path.join(self._root_dir_path, \"gts\", \"real_test\")\nelif self._split == \"camera_val\":\ngts_dir_path = os.path.join(self._root_dir_path, \"gts\", \"val\")\nelse:\nreturn None\npath = os.path.normpath(color_path)\nsplit_path = path.split(os.sep)\nnumber = path[-14:-10]\ngts_filename = f\"results_{split_path[-3]}_{split_path[-2]}_{number}.pkl\"\ngts_path = os.path.join(gts_dir_path, gts_filename)\nreturn gts_path\ndef _get_obj_path(self, meta_row: pd.Series) -&gt; str:\n\"\"\"Return path to object file from meta data row.\"\"\"\nif \"camera\" in self._split:  # ShapeNet mesh\nsynset_id = meta_row.iloc[2]\nobject_id = meta_row.iloc[3]\nobj_path = os.path.join(\nself._root_dir_path,\n\"obj_models\",\nself._split.replace(\"camera_\", \"\"),\nsynset_id,\nobject_id,\n\"model.obj\",\n)\nelif \"real_test\" in self._split:  # Fixed REAL test meshes\nobject_id = meta_row.iloc[2]\nobj_path = os.path.join(\nself._root_dir_path,\n\"fixed_real_test_obj_models\",\nobject_id + \".obj\",\n)\nelif \"real_train\" in self._split:  # REAL train mesh (not complete)\nobject_id = meta_row.iloc[2]\nobj_path = os.path.join(\nself._root_dir_path, \"obj_models\", self._split, object_id + \".obj\"\n)\nelse:\nraise ValueError(f\"Specified split {self._split} is not supported.\")\nreturn obj_path\ndef _get_mesh_extents_from_obj(self, obj_path: str) -&gt; torch.Tensor:\n\"\"\"Return maximum extent of bounding box from obj filepath.\n        Note that this is normalized extent (with diagonal == 1) in the case of CAMERA\n        dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset.\n        \"\"\"\nmesh = o3d.io.read_triangle_mesh(obj_path)\nvertices = np.asarray(mesh.vertices)\nif len(vertices) == 0:\nraise ObjectError()\nextents = np.max(vertices, axis=0) - np.min(vertices, axis=0)\nreturn torch.Tensor(extents)\ndef _load_mask(self, mask_path: str) -&gt; torch.Tensor:\n\"\"\"Load mask from mask filepath.\"\"\"\nmask_img = np.asarray(Image.open(mask_path), dtype=np.uint8)\nif mask_img.ndim == 3 and mask_img.shape[2] == 4:  # CAMERA masks are RGBA\ninstances_mask = mask_img[:, :, 0]  # use first channel only\nelse:  # REAL masks are grayscale\ninstances_mask = mask_img\nreturn torch.from_numpy(instances_mask)\ndef _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n\"\"\"Load depth from depth filepath.\"\"\"\ndepth = torch.from_numpy(\nnp.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n)\nreturn depth\ndef _load_nocs_map(self, nocs_map_path: str) -&gt; torch.Tensor:\n\"\"\"Load NOCS map from NOCS map filepath.\n        Returns:\n            NOCS map where each channel corresponds to one dimension in NOCS.\n            Coordinates are normalized to [0,1], shape (H,W,3).\n        \"\"\"\nnocs_map = torch.from_numpy(\nnp.asarray(Image.open(nocs_map_path), dtype=np.float32) / 255\n)\n# z-coordinate has to be flipped\n# see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501\nnocs_map[:, :, 2] = 1 - nocs_map[:, :, 2]\nreturn nocs_map[:, :, :3]\ndef _estimate_object(self, color_path: str, mask_id: int) -&gt; tuple:\n\"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\"\nposition = rotation_matrix = scale = out_transform = None\ndepth_path = self._depth_path_from_color_path(color_path)\ndepth = self._load_depth(depth_path)\nmask_path = self._mask_path_from_color_path(color_path)\ninstances_mask = self._load_mask(mask_path)\ninstance_mask = instances_mask == mask_id\nnocs_map_path = self._nocs_map_path_from_color_path(color_path)\nnocs_map = self._load_nocs_map(nocs_map_path)\nvalid_instance_mask = instance_mask * depth != 0\nnocs_map[~valid_instance_mask] = 0\ncentered_nocs_points = nocs_map[valid_instance_mask] - 0.5\nmeasured_points = pointset_utils.depth_to_pointcloud(\ndepth, self._camera, mask=valid_instance_mask, convention=\"opencv\"\n)\n# require at least 30 point correspondences to prevent outliers\nif len(measured_points) &lt; 30:\nraise nocs_utils.PoseEstimationError()\n# skip object if it cointains errorneous depth\nif torch.max(depth[valid_instance_mask]) &gt; 32.0:\nprint(\"Erroneous depth detected.\", end=\"\\033[K\\n\")\nraise nocs_utils.PoseEstimationError()\n(\nposition,\nrotation_matrix,\nscale,\nout_transform,\n) = nocs_utils.estimate_similarity_transform(\ncentered_nocs_points, measured_points, verbose=False\n)\nif position is None:\nraise nocs_utils.PoseEstimationError()\nreturn position, rotation_matrix, scale, out_transform\ndef _get_scale(self, sample_data: dict, extents: torch.Tensor) -&gt; float:\n\"\"\"Return scale from stored sample data and extents.\"\"\"\nif self._scale_convention == \"diagonal\":\nreturn sample_data[\"nocs_scale\"]\nelif self._scale_convention == \"max\":\nreturn sample_data[\"max_extent\"]\nelif self._scale_convention == \"half_max\":\nreturn 0.5 * sample_data[\"max_extent\"]\nelif self._scale_convention == \"full\":\nreturn extents\nelse:\nraise ValueError(\nf\"Specified scale convention {self._scale_convnetion} not supported.\"\n)\ndef _change_axis_convention(\nself, orientation_q: torch.Tensor, extents: torch.Tensor\n) -&gt; tuple:\n\"\"\"Adjust up-axis for orientation and extents.\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn orientation_q, extents\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nremapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n# quaternion so far: original -&gt; camera\n# we want a quaternion: new -&gt; camera\nrotation_n2o = rotation_o2n.T\nquaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\nremapped_orientation_q = quaternion_utils.quaternion_multiply(\norientation_q, quaternion_n2o\n)  # new -&gt; original -&gt; camera\nreturn remapped_orientation_q, remapped_extents\ndef _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n\"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\nrotation_o2n = np.zeros((3, 3))  # original to new object convention\nif self._remap_y_axis == \"x\":\nrotation_o2n[0, 1] = 1\nelif self._remap_y_axis == \"-x\":\nrotation_o2n[0, 1] = -1\nelif self._remap_y_axis == \"y\":\nrotation_o2n[1, 1] = 1\nelif self._remap_y_axis == \"-y\":\nrotation_o2n[1, 1] = -1\nelif self._remap_y_axis == \"z\":\nrotation_o2n[2, 1] = 1\nelif self._remap_y_axis == \"-z\":\nrotation_o2n[2, 1] = -1\nelse:\nraise ValueError(\"Unsupported remap_y_axis {self.remap_y}\")\nif self._remap_x_axis == \"x\":\nrotation_o2n[0, 0] = 1\nelif self._remap_x_axis == \"-x\":\nrotation_o2n[0, 0] = -1\nelif self._remap_x_axis == \"y\":\nrotation_o2n[1, 0] = 1\nelif self._remap_x_axis == \"-y\":\nrotation_o2n[1, 0] = -1\nelif self._remap_x_axis == \"z\":\nrotation_o2n[2, 0] = 1\nelif self._remap_x_axis == \"-z\":\nrotation_o2n[2, 0] = -1\nelse:\nraise ValueError(\"Unsupported remap_x_axis {self.remap_y}\")\n# infer last column\nrotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\nrotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\nif np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\nraise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\nreturn rotation_o2n\ndef _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Convert quaternion to selected orientation representation.\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\nif self._orientation_repr == \"quaternion\":\nreturn quaternion\nelse:\nraise NotImplementedError(\nf\"Orientation representation {self._orientation_repr} is not supported.\"\n)\ndef load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n\"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\nmesh = o3d.io.read_triangle_mesh(object_path)\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn mesh\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nmesh.rotate(\nrotation_o2n,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\nreturn mesh\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for NOCSDataset.</p> ATTRIBUTE DESCRIPTION <code>root_dir</code> <p>See NOCSDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>The dataset split. The following strings are supported:     \"camera_train\": 275000 images, synthetic objects + real background     \"camera_val\": 25000 images, synthetic objects + real background     \"real_train\": 4300 images in 7 scenes, real     \"real_test\": 2750 images in 6 scenes, real</p> <p> TYPE: <code>str</code> </p> <code>mask_pointcloud</code> <p>Whether the returned pointcloud will be masked.</p> <p> TYPE: <code>bool</code> </p> <code>normalize_pointcloud</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <p> TYPE: <code>bool</code> </p> <code>scale_convention</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <p> TYPE: <code>str</code> </p> <code>camera_convention</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <p> TYPE: <code>str</code> </p> <code>orientation_repr</code> <p>Which orientation representation is used. Currently only \"quaternion\" supported.</p> <p> TYPE: <code>str</code> </p> <code>remap_y_axis</code> <p>If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>remap_x_axis</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>category_str</code> <p>If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n\"\"\"Configuration dictionary for NOCSDataset.\n    Attributes:\n        root_dir: See NOCSDataset docstring.\n        split:\n            The dataset split. The following strings are supported:\n                \"camera_train\": 275000 images, synthetic objects + real background\n                \"camera_val\": 25000 images, synthetic objects + real background\n                \"real_train\": 4300 images in 7 scenes, real\n                \"real_test\": 2750 images in 6 scenes, real\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. Currently only \"quaternion\"\n            supported.\n        remap_y_axis:\n            If not None, the NOCS y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See NOCSDataset.category_id_to_str for admissible category strings.\n    \"\"\"\nroot_dir: str\nsplit: str\nmask_pointcloud: bool\nnormalize_pointcloud: bool\nscale_convention: str\ncamera_convention: str\norientation_repr: str\nremap_y_axis: Optional[str]\nremap_x_axis: Optional[str]\ncategory_str: Optional[str]\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initialize the dataset.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys.</p> <p> TYPE: <code>Config</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __init__(\nself,\nconfig: Config,\n) -&gt; None:\n\"\"\"Initialize the dataset.\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See NOCSDataset.Config for supported keys.\n    \"\"\"\nconfig = yoco.load_config(config, current_dict=NOCSDataset.default_config)\nself._root_dir_path = utils.resolve_path(config[\"root_dir\"])\nself._split = config[\"split\"]\nself._check_dirs()\nself._camera_convention = config[\"camera_convention\"]\nself._camera = self._get_split_camera()\nself._preprocess_path = os.path.join(\nself._root_dir_path, \"cpas_toolbox\", self._split\n)\nif not os.path.isdir(self._preprocess_path):\nself._preprocess_dataset()\nself._mask_pointcloud = config[\"mask_pointcloud\"]\nself._normalize_pointcloud = config[\"normalize_pointcloud\"]\nself._scale_convention = config[\"scale_convention\"]\nself._sample_files = self._get_sample_files(config[\"category_str\"])\nself._remap_y_axis = config[\"remap_y_axis\"]\nself._remap_x_axis = config[\"remap_x_axis\"]\nself._orientation_repr = config[\"orientation_repr\"]\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of sample in dataset.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return number of sample in dataset.\"\"\"\nreturn len(self._sample_files)\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict\n</code></pre> <p>Return a sample of the dataset.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Index of the instance.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\"</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n\"\"\"Return a sample of the dataset.\n    Args:\n        idx: Index of the instance.\n    Returns:\n        Sample containing the following items:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n    \"\"\"\nsample_file = self._sample_files[idx]\nsample_data = pickle.load(open(sample_file, \"rb\"))\nsample = self._sample_from_sample_data(sample_data)\nreturn sample\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.load_mesh","title":"load_mesh","text":"<pre><code>load_mesh(object_path: str) -&gt; o3d.geometry.TriangleMesh\n</code></pre> <p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n\"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\nmesh = o3d.io.read_triangle_mesh(object_path)\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn mesh\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nmesh.rotate(\nrotation_o2n,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\nreturn mesh\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.ObjectError","title":"ObjectError","text":"<p>         Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class ObjectError(Exception):\n\"\"\"Error if something with the mesh is wrong.\"\"\"\npass\n</code></pre>"},{"location":"api_reference/datasets/nocs_utils/","title":"nocs_utils.py","text":""},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils","title":"cpas_toolbox.datasets.nocs_utils","text":"<p>Module for utility function related to NOCS dataset.</p> <p>This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets.</p> Aligning code by Srinath Sridhar <p>https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py</p>"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.PoseEstimationError","title":"PoseEstimationError","text":"<p>         Bases: <code>Exception</code></p> <p>Error if pose estimation encountered an error.</p> Source code in <code>cpas_toolbox/datasets/nocs_utils.py</code> <pre><code>class PoseEstimationError(Exception):\n\"\"\"Error if pose estimation encountered an error.\"\"\"\npass\n</code></pre>"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.estimate_similarity_transform","title":"estimate_similarity_transform","text":"<pre><code>estimate_similarity_transform(\nsource: np.ndarray, target: np.ndarray, verbose: bool = False\n) -&gt; tuple\n</code></pre> <p>Estimate similarity transform from source to target from point correspondences.</p> <p>Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation.</p> <p>A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points.</p> <p>Note that the returned values fulfill the following equations     transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side.</p> PARAMETER DESCRIPTION <code>source</code> <p>Source points that will be transformed, shape (N,3).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>target</code> <p>Target points to which source will be aligned to, shape (N,3).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>verbose</code> <p>If true additional information will be printed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>position</code> <p>Translation to translate source to target, shape (3,).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>rotation_matrix</code> <p>Rotation to rotate source to target, shape (3,3).</p> <p> TYPE: <code>np.ndarray</code> </p> <code>scale</code> <p>Scaling factor along each axis, to scale source to target.</p> <p> TYPE: <code>float</code> </p> <code>transform</code> <p>Homogeneous transformation matrix, shape (4,4).</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_utils.py</code> <pre><code>def estimate_similarity_transform(\nsource: np.ndarray, target: np.ndarray, verbose: bool = False\n) -&gt; tuple:\n\"\"\"Estimate similarity transform from source to target from point correspondences.\n    Source and target are pairwise correponding pointsets, i.e., they include same\n    number of points and the first point of source corresponds to the first point of\n    target. RANSAC is used for outlier-robust estimation.\n    A similarity transform is estimated (i.e., isotropic scale, rotation and\n    translation) that transforms source points onto the target points.\n    Note that the returned values fulfill the following equations\n        transform @ source_points = scale * rotation_matrix @ source_points + position\n    when ignoring homogeneous coordinate for left-hand side.\n    Args:\n        source: Source points that will be transformed, shape (N,3).\n        target: Target points to which source will be aligned to, shape (N,3).\n        verbose: If true additional information will be printed.\n    Returns:\n        position (np.ndarray): Translation to translate source to target, shape (3,).\n        rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3).\n        scale (float):\n            Scaling factor along each axis, to scale source to target.\n        transform (np.ndarray): Homogeneous transformation matrix, shape (4,4).\n    \"\"\"\nif len(source) &lt; 5 or len(target) &lt; 5:\nprint(\"Pose estimation failed. Not enough point correspondences: \", len(source))\nreturn None, None, None, None\n# make points homogeneous\nsource_hom = np.transpose(np.hstack([source, np.ones([source.shape[0], 1])]))  # 4,N\ntarget_hom = np.transpose(np.hstack([target, np.ones([source.shape[0], 1])]))  # 4,N\n# Auto-parameter selection based on source-target heuristics\ntarget_norm = np.mean(np.linalg.norm(target, axis=1))  # mean distance from origin\nsource_norm = np.mean(np.linalg.norm(source, axis=1))\nratio_ts = target_norm / source_norm\nratio_st = source_norm / target_norm\npass_t = ratio_st if (ratio_st &gt; ratio_ts) else ratio_ts\npass_t *= 0.01  # tighter bound\nstop_t = pass_t / 100\nn_iter = 100\nif verbose:\nprint(\"Pass threshold: \", pass_t)\nprint(\"Stop threshold: \", stop_t)\nprint(\"Number of iterations: \", n_iter)\nsource_inliers_hom, target_inliers_hom, best_inlier_ratio = _get_ransac_inliers(\nsource_hom,\ntarget_hom,\nmax_iterations=n_iter,\npass_threshold=pass_t,\nstop_threshold=stop_t,\n)\nif best_inlier_ratio &lt; 0.1:\nprint(\"Pose estimation failed. Small inlier ratio: \", best_inlier_ratio)\nreturn None, None, None, None\nscales, rotation_matrix, position, out_transform = _estimate_similarity_umeyama(\nsource_inliers_hom, target_inliers_hom\n)\nscale = scales[0]\nif verbose:\nprint(\"BestInlierRatio:\", best_inlier_ratio)\nprint(\"Rotation:\\n\", rotation_matrix)\nprint(\"Position:\\n\", position)\nprint(\"Scales:\", scales)\nreturn position, rotation_matrix, scale, out_transform\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/","title":"redwood_dataset.py","text":""},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset","title":"cpas_toolbox.datasets.redwood_dataset","text":"<p>Module providing dataset class for annotated Redwood dataset.</p>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset","title":"AnnotatedRedwoodDataset","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>Dataset class for annotated Redwood dataset.</p> <p>Data can be found here: http://redwood-data.org/3dscan/index.html</p> <p>Annotations are of repo.</p> Expected directory format <p>{root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class AnnotatedRedwoodDataset(torch.utils.data.Dataset):\n\"\"\"Dataset class for annotated Redwood dataset.\n    Data can be found here:\n    http://redwood-data.org/3dscan/index.html\n    Annotations are of repo.\n    Expected directory format:\n        {root_dir}/{category_str}/rgbd/{sequence_id}/...\n        {ann_dir}/{sequence_id}.obj\n        {ann_dir}/annotations.json\n    \"\"\"\nnum_categories = 3\ncategory_id_to_str = {\n0: \"bottle\",\n1: \"bowl\",\n2: \"mug\",\n}\ncategory_str_to_id = {v: k for k, v in category_id_to_str.items()}\nclass Config(TypedDict, total=False):\n\"\"\"Configuration dictionary for annoated Redwood dataset.\n        Attributes:\n            root_dir: See AnnotatedRedwoodDataset docstring.\n            ann_dir: See AnnotatedRedwoodDataset docstring.\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. Currently only \"quaternion\"\n                supported.\n            remap_y_axis:\n                If not None, the Redwood y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n                strings.\n        \"\"\"\nroot_dir: str\nann_dir: str\nsplit: str\nmask_pointcloud: bool\nnormalize_pointcloud: bool\nscale_convention: str\ncamera_convention: str\norientation_repr: str\norientation_grid_resolution: int\nremap_y_axis: Optional[str]\nremap_x_axis: Optional[str]\ncategory_str: Optional[str]\ndefault_config: Config = {\n\"root_dir\": None,\n\"ann_dir\": None,\n\"mask_pointcloud\": False,\n\"normalize_pointcloud\": False,\n\"camera_convention\": \"opengl\",\n\"scale_convention\": \"half_max\",\n\"orientation_repr\": \"quaternion\",\n\"orientation_grid_resolution\": None,\n\"category_str\": None,\n\"remap_y_axis\": None,\n\"remap_x_axis\": None,\n}\ndef __init__(\nself,\nconfig: Config,\n) -&gt; None:\n\"\"\"Initialize the dataset.\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n        \"\"\"\nconfig = yoco.load_config(\nconfig, current_dict=AnnotatedRedwoodDataset.default_config\n)\nself._root_dir_path = utils.resolve_path(config[\"root_dir\"])\nself._ann_dir_path = utils.resolve_path(config[\"ann_dir\"])\nself._check_dirs()\nself._camera_convention = config[\"camera_convention\"]\nself._mask_pointcloud = config[\"mask_pointcloud\"]\nself._normalize_pointcloud = config[\"normalize_pointcloud\"]\nself._scale_convention = config[\"scale_convention\"]\nself._remap_y_axis = config[\"remap_y_axis\"]\nself._remap_x_axis = config[\"remap_x_axis\"]\nself._orientation_repr = config[\"orientation_repr\"]\nself._category_str = config[\"category_str\"]\nself._load_annotations()\nself._camera = camera_utils.Camera(\nwidth=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5\n)\ndef _check_dirs(self) -&gt; None:\nif os.path.exists(self._root_dir_path) and os.path.exists(self._ann_dir_path):\npass\nelse:\nprint(\n\"REDWOOD75 dataset not found, do you want to download it into the \"\n\"following directories:\"\n)\nprint(\"  \", self._root_dir_path)\nprint(\"  \", self._ann_dir_path)\nwhile True:\ndecision = input(\"(Y/n) \").lower()\nif decision == \"\" or decision == \"y\":\nself._download_dataset()\nbreak\nelif decision == \"n\":\nprint(\"Dataset not found. Aborting.\")\nexit(0)\ndef _download_dataset(self) -&gt; None:\n# Download anns\nif not os.path.exists(self._ann_dir_path):\nzip_path = os.path.join(self._ann_dir_path, \"redwood75.zip\")\nos.makedirs(self._ann_dir_path, exist_ok=True)\nurl = (\n\"https://drive.google.com/u/0/uc?id=1PMvIblsXWDxEJykVwhUk_QEjy4_bmDU\"\n\"-&amp;export=download\"\n)\nutils.download(url, zip_path)\nz = zipfile.ZipFile(zip_path)\nz.extractall(os.path.join(self._ann_dir_path, \"..\"))\nz.close()\nos.remove(zip_path)\nann_json = os.path.join(self._ann_dir_path, \"annotations.json\")\nwith open(ann_json, \"r\") as f:\nanns_dict = json.load(f)\nbaseurl = \"https://s3.us-west-1.wasabisys.com/redwood-3dscan/rgbd/\"\nfor seq_id in anns_dict.keys():\ndownload_dir_path = os.path.join(\nself._root_dir_path, anns_dict[seq_id][\"category\"]\n)\nos.makedirs(download_dir_path, exist_ok=True)\nzip_path = os.path.join(download_dir_path, f\"{seq_id}.zip\")\nos.makedirs(os.path.dirname(zip_path), exist_ok=True)\nutils.download(baseurl + f\"{seq_id}.zip\", zip_path)\nz = zipfile.ZipFile(zip_path)\nseq_dir_path = os.path.join(download_dir_path, \"rgbd\", seq_id)\nos.makedirs(seq_dir_path, exist_ok=True)\nz.extractall(seq_dir_path)\nz.close()\nos.remove(zip_path)\ndef _load_annotations(self) -&gt; None:\n\"\"\"Load annotations into memory.\"\"\"\nann_json = os.path.join(self._ann_dir_path, \"annotations.json\")\nwith open(ann_json, \"r\") as f:\nanns_dict = json.load(f)\nself._raw_samples = []\nfor seq_id, seq_anns in anns_dict.items():\nif (\nself._category_str is not None\nand self._category_str != seq_anns[\"category\"]\n):\ncontinue\nfor pose_ann in seq_anns[\"pose_anns\"]:\nself._raw_samples.append(\nself._create_raw_sample(seq_id, seq_anns, pose_ann)\n)\ndef _create_raw_sample(\nself, seq_id: str, sequence_dict: dict, annotation_dict: dict\n) -&gt; dict:\n\"\"\"Create raw sample from information in annotations file.\"\"\"\nposition = torch.tensor(annotation_dict[\"position\"])\norientation_q = torch.tensor(annotation_dict[\"orientation\"])\nrgb_filename = annotation_dict[\"rgb_file\"]\ndepth_filename = annotation_dict[\"depth_file\"]\nmesh_filename = sequence_dict[\"mesh\"]\nmesh_path = os.path.join(self._ann_dir_path, mesh_filename)\ncategory_str = sequence_dict[\"category\"]\ncolor_path = os.path.join(\nself._root_dir_path, category_str, \"rgbd\", seq_id, \"rgb\", rgb_filename\n)\ndepth_path = os.path.join(\nself._root_dir_path, category_str, \"rgbd\", seq_id, \"depth\", depth_filename\n)\nextents = torch.tensor(sequence_dict[\"scale\"]) * 2\nreturn {\n\"position\": position,\n\"orientation_q\": orientation_q,\n\"extents\": extents,\n\"color_path\": color_path,\n\"depth_path\": depth_path,\n\"mesh_path\": mesh_path,\n\"category_str\": category_str,\n}\ndef __len__(self) -&gt; int:\n\"\"\"Return number of sample in dataset.\"\"\"\nreturn len(self._raw_samples)\ndef __getitem__(self, idx: int) -&gt; dict:\n\"\"\"Return a sample of the dataset.\n        Args:\n            idx: Index of the instance.\n        Returns:\n            Sample containing the following keys:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n                \"obj_path\"\n                \"category_id\"\n                \"category_str\"\n        \"\"\"\nraw_sample = self._raw_samples[idx]\ncolor = torch.from_numpy(\nnp.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n)\ndepth = self._load_depth(raw_sample[\"depth_path\"])\ninstance_mask = self._compute_mask(depth, raw_sample)\npointcloud_mask = instance_mask if self._mask_pointcloud else None\npointcloud = pointset_utils.depth_to_pointcloud(\ndepth,\nself._camera,\nmask=pointcloud_mask,\nconvention=self._camera_convention,\n)\n# adjust camera convention for position, orientation and scale\nposition = pointset_utils.change_position_camera_convention(\nraw_sample[\"position\"], \"opencv\", self._camera_convention\n)\n# orientation / scale\norientation_q, extents = self._change_axis_convention(\nraw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n)\norientation_q = pointset_utils.change_orientation_camera_convention(\norientation_q, \"opencv\", self._camera_convention\n)\norientation = self._quat_to_orientation_repr(orientation_q)\nscale = self._get_scale(extents)\n# normalize pointcloud &amp; position\nif self._normalize_pointcloud:\npointcloud, centroid = pointset_utils.normalize_points(pointcloud)\nposition = position - centroid\ncategory_str = raw_sample[\"category_str\"]\nsample = {\n\"color\": color,\n\"depth\": depth,\n\"pointset\": pointcloud,\n\"mask\": instance_mask,\n\"position\": position,\n\"orientation\": orientation,\n\"quaternion\": orientation_q,\n\"scale\": scale,\n\"color_path\": raw_sample[\"color_path\"],\n\"obj_path\": raw_sample[\"mesh_path\"],\n\"category_id\": self.category_str_to_id[category_str],\n\"category_str\": category_str,\n}\nreturn sample\ndef _compute_mask(self, depth: torch.Tensor, raw_sample: dict) -&gt; torch.Tensor:\nposed_mesh = o3d.io.read_triangle_mesh(raw_sample[\"mesh_path\"])\nR = Rotation.from_quat(raw_sample[\"orientation_q\"]).as_matrix()\nposed_mesh.rotate(R, center=np.array([0, 0, 0]))\nposed_mesh.translate(raw_sample[\"position\"])\nposed_mesh.compute_vertex_normals()\ngt_depth = torch.from_numpy(_draw_depth_geometry(posed_mesh, self._camera))\nmask = gt_depth != 0\n# exclude occluded parts from mask\nmask[(depth != 0) * (depth &lt; gt_depth - 0.01)] = 0\nreturn mask\ndef _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n\"\"\"Load depth from depth filepath.\"\"\"\ndepth = torch.from_numpy(\nnp.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n)\nreturn depth\ndef _get_scale(self, extents: torch.Tensor) -&gt; float:\n\"\"\"Return scale from stored sample data and extents.\"\"\"\nif self._scale_convention == \"diagonal\":\nreturn torch.linalg.norm(extents)\nelif self._scale_convention == \"max\":\nreturn extents.max()\nelif self._scale_convention == \"half_max\":\nreturn 0.5 * extents.max()\nelif self._scale_convention == \"full\":\nreturn extents\nelse:\nraise ValueError(\nf\"Specified scale convention {self._scale_convention} not supported.\"\n)\ndef _change_axis_convention(\nself, orientation_q: torch.Tensor, extents: torch.Tensor\n) -&gt; tuple:\n\"\"\"Adjust up-axis for orientation and extents.\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn orientation_q, extents\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nremapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n# quaternion so far: original -&gt; camera\n# we want a quaternion: new -&gt; camera\nrotation_n2o = rotation_o2n.T\nquaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\nremapped_orientation_q = quaternion_utils.quaternion_multiply(\norientation_q, quaternion_n2o\n)  # new -&gt; original -&gt; camera\nreturn remapped_orientation_q, remapped_extents\ndef _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n\"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\nrotation_o2n = np.zeros((3, 3))  # original to new object convention\nif self._remap_y_axis == \"x\":\nrotation_o2n[0, 1] = 1\nelif self._remap_y_axis == \"-x\":\nrotation_o2n[0, 1] = -1\nelif self._remap_y_axis == \"y\":\nrotation_o2n[1, 1] = 1\nelif self._remap_y_axis == \"-y\":\nrotation_o2n[1, 1] = -1\nelif self._remap_y_axis == \"z\":\nrotation_o2n[2, 1] = 1\nelif self._remap_y_axis == \"-z\":\nrotation_o2n[2, 1] = -1\nelse:\nraise ValueError(\"Unsupported remap_y_axis {self.remap_y}\")\nif self._remap_x_axis == \"x\":\nrotation_o2n[0, 0] = 1\nelif self._remap_x_axis == \"-x\":\nrotation_o2n[0, 0] = -1\nelif self._remap_x_axis == \"y\":\nrotation_o2n[1, 0] = 1\nelif self._remap_x_axis == \"-y\":\nrotation_o2n[1, 0] = -1\nelif self._remap_x_axis == \"z\":\nrotation_o2n[2, 0] = 1\nelif self._remap_x_axis == \"-z\":\nrotation_o2n[2, 0] = -1\nelse:\nraise ValueError(\"Unsupported remap_x_axis {self.remap_y}\")\n# infer last column\nrotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\nrotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\nif np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\nraise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\nreturn rotation_o2n\ndef _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Convert quaternion to selected orientation representation.\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\nif self._orientation_repr == \"quaternion\":\nreturn quaternion\nelif self._orientation_repr == \"discretized\":\nindex = self._orientation_grid.quat_to_index(quaternion.numpy())\nreturn torch.tensor(\nindex,\ndtype=torch.long,\n)\nelse:\nraise NotImplementedError(\nf\"Orientation representation {self._orientation_repr} is not supported.\"\n)\ndef load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n\"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\nmesh = o3d.io.read_triangle_mesh(object_path)\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn mesh\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nmesh.rotate(\nrotation_o2n,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\nreturn mesh\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.Config","title":"Config","text":"<p>         Bases: <code>TypedDict</code></p> <p>Configuration dictionary for annoated Redwood dataset.</p> ATTRIBUTE DESCRIPTION <code>root_dir</code> <p>See AnnotatedRedwoodDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>ann_dir</code> <p>See AnnotatedRedwoodDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>mask_pointcloud</code> <p>Whether the returned pointcloud will be masked.</p> <p> TYPE: <code>bool</code> </p> <code>normalize_pointcloud</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <p> TYPE: <code>bool</code> </p> <code>scale_convention</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <p> TYPE: <code>str</code> </p> <code>camera_convention</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <p> TYPE: <code>str</code> </p> <code>orientation_repr</code> <p>Which orientation representation is used. Currently only \"quaternion\" supported.</p> <p> TYPE: <code>str</code> </p> <code>remap_y_axis</code> <p>If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>remap_x_axis</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>category_str</code> <p>If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n\"\"\"Configuration dictionary for annoated Redwood dataset.\n    Attributes:\n        root_dir: See AnnotatedRedwoodDataset docstring.\n        ann_dir: See AnnotatedRedwoodDataset docstring.\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. Currently only \"quaternion\"\n            supported.\n        remap_y_axis:\n            If not None, the Redwood y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n            strings.\n    \"\"\"\nroot_dir: str\nann_dir: str\nsplit: str\nmask_pointcloud: bool\nnormalize_pointcloud: bool\nscale_convention: str\ncamera_convention: str\norientation_repr: str\norientation_grid_resolution: int\nremap_y_axis: Optional[str]\nremap_x_axis: Optional[str]\ncategory_str: Optional[str]\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initialize the dataset.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys.</p> <p> TYPE: <code>Config</code> </p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __init__(\nself,\nconfig: Config,\n) -&gt; None:\n\"\"\"Initialize the dataset.\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n    \"\"\"\nconfig = yoco.load_config(\nconfig, current_dict=AnnotatedRedwoodDataset.default_config\n)\nself._root_dir_path = utils.resolve_path(config[\"root_dir\"])\nself._ann_dir_path = utils.resolve_path(config[\"ann_dir\"])\nself._check_dirs()\nself._camera_convention = config[\"camera_convention\"]\nself._mask_pointcloud = config[\"mask_pointcloud\"]\nself._normalize_pointcloud = config[\"normalize_pointcloud\"]\nself._scale_convention = config[\"scale_convention\"]\nself._remap_y_axis = config[\"remap_y_axis\"]\nself._remap_x_axis = config[\"remap_x_axis\"]\nself._orientation_repr = config[\"orientation_repr\"]\nself._category_str = config[\"category_str\"]\nself._load_annotations()\nself._camera = camera_utils.Camera(\nwidth=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5\n)\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of sample in dataset.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return number of sample in dataset.\"\"\"\nreturn len(self._raw_samples)\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict\n</code></pre> <p>Return a sample of the dataset.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Index of the instance.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\"</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n\"\"\"Return a sample of the dataset.\n    Args:\n        idx: Index of the instance.\n    Returns:\n        Sample containing the following keys:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n            \"obj_path\"\n            \"category_id\"\n            \"category_str\"\n    \"\"\"\nraw_sample = self._raw_samples[idx]\ncolor = torch.from_numpy(\nnp.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n)\ndepth = self._load_depth(raw_sample[\"depth_path\"])\ninstance_mask = self._compute_mask(depth, raw_sample)\npointcloud_mask = instance_mask if self._mask_pointcloud else None\npointcloud = pointset_utils.depth_to_pointcloud(\ndepth,\nself._camera,\nmask=pointcloud_mask,\nconvention=self._camera_convention,\n)\n# adjust camera convention for position, orientation and scale\nposition = pointset_utils.change_position_camera_convention(\nraw_sample[\"position\"], \"opencv\", self._camera_convention\n)\n# orientation / scale\norientation_q, extents = self._change_axis_convention(\nraw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n)\norientation_q = pointset_utils.change_orientation_camera_convention(\norientation_q, \"opencv\", self._camera_convention\n)\norientation = self._quat_to_orientation_repr(orientation_q)\nscale = self._get_scale(extents)\n# normalize pointcloud &amp; position\nif self._normalize_pointcloud:\npointcloud, centroid = pointset_utils.normalize_points(pointcloud)\nposition = position - centroid\ncategory_str = raw_sample[\"category_str\"]\nsample = {\n\"color\": color,\n\"depth\": depth,\n\"pointset\": pointcloud,\n\"mask\": instance_mask,\n\"position\": position,\n\"orientation\": orientation,\n\"quaternion\": orientation_q,\n\"scale\": scale,\n\"color_path\": raw_sample[\"color_path\"],\n\"obj_path\": raw_sample[\"mesh_path\"],\n\"category_id\": self.category_str_to_id[category_str],\n\"category_str\": category_str,\n}\nreturn sample\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.load_mesh","title":"load_mesh","text":"<pre><code>load_mesh(object_path: str) -&gt; o3d.geometry.TriangleMesh\n</code></pre> <p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n\"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\nmesh = o3d.io.read_triangle_mesh(object_path)\nif self._remap_y_axis is None and self._remap_x_axis is None:\nreturn mesh\nelif self._remap_y_axis is None or self._remap_x_axis is None:\nraise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\nrotation_o2n = self._get_o2n_object_rotation_matrix()\nmesh.rotate(\nrotation_o2n,\ncenter=np.array([0.0, 0.0, 0.0])[:, None],\n)\nreturn mesh\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.ObjectError","title":"ObjectError","text":"<p>         Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class ObjectError(Exception):\n\"\"\"Error if something with the mesh is wrong.\"\"\"\npass\n</code></pre>"}]}