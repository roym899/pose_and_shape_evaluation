{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>CPAS Toolbox is a package for evaluation of categorical pose and shape estimation methods. It contains metrics, and wrappers for datasets and methods.</p>"},{"location":"#installation","title":"Installation","text":"<p>Run <pre><code>pip install cpas_toolbox\n</code></pre> to install the latest release of the toolbox. There is no need to download any additional weights or datasets. Upon first usage the evaluation script will ask to download the weights if they are not available at the expected path.</p>"},{"location":"#evaluation-of-baseline-methods","title":"Evaluation of baseline methods","text":"<p>To reproduce the REAL275 benchmark run: <pre><code>python -m cpas_toolbox.evaluate --config real275.yaml all_baselines.yaml --out_dir ./results/\n</code></pre> To reproduce the REDWOOD75 benchmark run: <pre><code>python -m cpas_toolbox.evaluate --config redwood75.yaml all_baselines.yaml --out_dir ./results/\n</code></pre></p> <p>We can overwrite settings of the configuration via the command-line. For example,  <pre><code>python -m cpas_toolbox.evaluate --config redwood75.yaml all_baselines.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True\n</code></pre> enables interactive visualization of ground truth and predictions. Alternatively, you could specify <code>--store_visualization True</code> to save the visualization of every prediction in the results directory.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this library useful in your research, consider citing our publication: <pre><code>@article{bruns2022evaluation,\n  title={On the Evaluation of {RGB-D}-based Categorical Pose and Shape Estimation},\n  author={Bruns, Leonard and Jensfelt, Patric},\n  journal={arXiv preprint arXiv:2202.10346},\n  year={2022}\n}\n</code></pre></p>"},{"location":"data_preparation/","title":"Data preparation","text":"<p>On first usage of a dataset the script will download and preprocess the datasets automatically. This is the recommended way to use the package as it ensures an unmodified dataset.</p> <p>If you already downloaded a dataset and want to use symlinks instead of storing them again to save storage space, you can follow the manual instructions below.</p> <p>The default directories can be found in the configuration file for the respective dataset (REAL275, REDWOOD75).</p>"},{"location":"data_preparation/#real275","title":"REAL275","text":"<p>For download links check the NOCS repository.</p> <p>The expected directory structure for REAL275 evaluation is as follows: <pre><code>    {root_dir}/real_test/...\n    {root_dir}/gts/...\n    {root_dir}/obj_models/...\n</code></pre> An additional directory <code>{root_dir}/cpas_toolbox/</code> will be created to store preprocessed files. By default <code>{root_dir}</code> will be <code>data/nocs/</code> (i.e., relative to the current working directory, when executing the evaluation script), but it can be modified.</p>"},{"location":"data_preparation/#redwood75","title":"REDWOOD75","text":"<p>To download the raw data check the redwood-data website. You can download the REDWOOD75 annotations here. Only the Redwood sequences ids contained in this file are required for evaluation.</p> <p>The expected directory structure for REDWOOD75 evaluation is as follows: <pre><code>    {root_dir}/bottle/rgbd/00049/depth/*.png\n    {root_dir}/bottle/rgbd/...\n    {root_dir}/bowl/...\n    {root_dir}/mug/...\n    {ann_dir}/obj_models/...\n</code></pre> By default <code>{root_dir}</code> will be <code>data/redwood/</code> (i.e., relative to the current working directory, when executing the evaluation script) <code>{ann_dir}</code> will be <code>data/redwood75/</code>, but those can be modified.</p>"},{"location":"tutorial_evaluation/","title":"Tutorial: Evaluate new method","text":"<p>In this tutorial we will walk through how to use the toolbox to evaluate a new method for pose and shape estimation. We assume the toolbox has already been installed successfully and the standard commands work.</p> <p>To evaluate a new method, we need to implement the interface between the method and the toolbox by defining a class inheriting from CPASMethod. The derived class needs to define at least the inference function, which is called by the evaluation script. The __init__ typically also has to be defined as it provides the dataset's camera parameters which are typically required for inference.</p> <p>Typically three steps have to implemented: </p> <ol> <li>Converting the input to the expected input format</li> <li>Call the inference code of the new method</li> <li>Convert the output of the method to the expected output format for the toolbox</li> </ol> <pre><code>from cpas_toolbox.cpas_method import CPASMethod, PredictionDict\nimport torch\n\nclass MyCPASMethod(CPASMethod):\n    def __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize categorical pose and shape estimation method.\n\n        Args:\n            config: Method configuration dictionary.\n            camera: Camera used for the input image.\n        \"\"\"\n        # save / convert camera as needed by inference\n        pass\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"Run a method to predict pose and shape of an object.\n\n        Args:\n            color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n            depth_image: The depth image, shape (H, W), meters, float.\n            instance_mask: Mask of object of interest. (H, W), bool.\n            category_str: The category of the object.\n        \"\"\"\n        # Step 1\n        # convert color_image, depth_image, instance_mask, category_str\n        # to whatever convention your inference code expects\n\n        # Step 2\n        # run your inference code\n\n        # Step 3\n        # convert output of your method to prediction dictionary\n        pass\n</code></pre>"},{"location":"tutorial_evaluation/#step-1-converting-the-input","title":"Step 1: Converting the input","text":"<p>Most methods follow different conventions for the color and depth image.</p> <p>Common pitfalls: </p> <ul> <li>OpenCV uses BGR instead of RGB, use <code>color_image = color_image[:,:,::-1]</code> to convert</li> <li>range of color image might be expected to be 0-255 instead of 0-1</li> <li>depth_image might be expected to contain millimeter instead of meter</li> <li>often a batch dimension has to be added</li> <li>data has to be moved to same device as model (all parameters will be on CPU by default)</li> </ul>"},{"location":"tutorial_evaluation/#step-2-calling-the-method","title":"Step 2: Calling the method","text":"<p>This step is method specific. Ideally this is a single call to a method's inference function.</p>"},{"location":"tutorial_evaluation/#step-3-converting-the-output","title":"Step 3: Converting the output","text":"<p>The expected output is a dictionary as specified by PredictionDict.</p> <p>Position and orientation should be in OpenCV convention, which refers to x-axis right, y-axis down, z-axis forward (see this page for visualization).</p> <p>Common pitfalls / things to make sure are right:</p> <ul> <li>some methods predict in OpenGL camera convention (x-axis right, y-axis up, z-axis backward), both position and orientation has to be adjusted in this case (see, e.g., sdfest.py)</li> <li>some methods use different canonical object orientations; our toolbox normalizes all datasets to follow ShapeNet convention (see below), changing the convention requires adjusting the orientation, extent, and reconstruction (see, e.g., spd.py)</li> </ul> <p></p>"},{"location":"tutorial_evaluation/#create-config-file-and-run-the-evaluation","title":"Create config file and run the evaluation","text":"<p>Once the class has been created and saved as a Python module (<code>mycpasmethod.py</code>) we can run the evaluation for any dataset. To do so, we must create the following config file (<code>mycpasmethod.yaml</code>): <pre><code>methods:\n  mycpasmethod:\n    # name of method (used in results files)\n    name: MyMethodName\n\n    # fully-specified name of class\n    method_type: mycpasmethod.MyCPASMethod\n\n    # this dictionary will be passed to __init__ of MyCPASMethod\n    config_dict:\n      x: 1  \n</code></pre> To run the evaluation on REAL275 we can now use <pre><code>python -m cpas_toolbox.evaluate --config real275.yaml mycpasmethod.yaml --out_dir ./results/ --visualize_gt True --visualize_prediction True\n</code></pre> <code>real275.yaml</code> can simply be replaced with <code>redwood75.yaml</code> to run the evaluation on REDWOOD75. The visualization of the prediction can be used to debug mismatched conventions and should be deactivated to evaluate the whole dataset.</p>"},{"location":"api_reference/camera_utils/","title":"camera_utils.py","text":""},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils","title":"cpas_toolbox.camera_utils","text":"<p>This module provides a pinhole camera class.</p>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera","title":"Camera","text":"<p>Pinhole camera parameters.</p> <p>This class allows conversion between different pixel conventions, i.e., pixel center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in computer vision.</p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>class Camera:\n    \"\"\"Pinhole camera parameters.\n\n    This class allows conversion between different pixel conventions, i.e., pixel\n    center at (0.5, 0.5) (as common in computer graphics), and (0, 0) as common in\n    computer vision.\n    \"\"\"\n\n    def __init__(\n        self,\n        width: int,\n        height: int,\n        fx: float,\n        fy: float,\n        cx: float,\n        cy: float,\n        s: float = 0.0,\n        pixel_center: float = 0.0,\n    ) -&gt; None:\n        \"\"\"Initialize camera parameters.\n\n        Note that the principal point is only fully defined in combination with\n        pixel_center.\n\n        The pixel_center defines the relation between continuous image plane\n        coordinates and discrete pixel coordinates.\n\n        A discrete image coordinate (x, y) will correspond to the continuous\n        image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n        will be either 0 or 0.5. During calibration it depends on the convention\n        the point features used to compute the calibration matrix.\n\n        Note that if pixel_center == 0, the corresponding continuous coordinate\n        interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n        to convert from continuous coordinate to the corresponding discrete coordinate.\n\n        For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n        pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n        coordinate to the corresponding discrete coordinate.\n\n        Args:\n            width: Number of pixels in horizontal direction.\n            height: Number of pixels in vertical direction.\n            fx: Horizontal focal length.\n            fy: Vertical focal length.\n            cx: Principal point x-coordinate.\n            cy: Principal point y-coordinate.\n            s: Skew.\n            pixel_center: The center offset for the provided principal point.\n        \"\"\"\n        # focal length\n        self.fx = fx\n        self.fy = fy\n\n        # principal point\n        self.cx = cx\n        self.cy = cy\n\n        self.pixel_center = pixel_center\n\n        # skew\n        self.s = s\n\n        # image dimensions\n        self.width = width\n        self.height = height\n\n    def get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n        \"\"\"Convert camera to Open3D pinhole camera parameters.\n\n        Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n        values are in front of camera). Open3D expects camera with pixel_center = 0\n        and does not support skew.\n\n        Returns:\n            The pinhole camera parameters.\n        \"\"\"\n        fx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\n        params = o3d.camera.PinholeCameraParameters()\n        params.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\n        params.extrinsic = np.array(\n            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n        )\n        return params\n\n    def get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n        \"\"\"Convert camera to general camera parameters.\n\n        Args:\n            pixel_center:\n                At which ratio of a square the pixel center should be for the resulting\n                parameters. Typically 0 or 0.5. See class documentation for more info.\n\n        Returns:\n            - fx, fy: The horizontal and vertical focal length\n            - cx, cy:\n                The position of the principal point in continuous image plane\n                coordinates considering the provided pixel center and the pixel center\n                specified during the construction.\n            - s: The skew.\n        \"\"\"\n        cx_corrected = self.cx - self.pixel_center + pixel_center\n        cy_corrected = self.cy - self.pixel_center + pixel_center\n        return self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.__init__","title":"__init__","text":"<pre><code>__init__(\n    width: int,\n    height: int,\n    fx: float,\n    fy: float,\n    cx: float,\n    cy: float,\n    s: float = 0.0,\n    pixel_center: float = 0.0,\n) -&gt; None\n</code></pre> <p>Initialize camera parameters.</p> <p>Note that the principal point is only fully defined in combination with pixel_center.</p> <p>The pixel_center defines the relation between continuous image plane coordinates and discrete pixel coordinates.</p> <p>A discrete image coordinate (x, y) will correspond to the continuous image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center will be either 0 or 0.5. During calibration it depends on the convention the point features used to compute the calibration matrix.</p> <p>Note that if pixel_center == 0, the corresponding continuous coordinate interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done to convert from continuous coordinate to the corresponding discrete coordinate.</p> <p>For pixel_center == 0.5, the corresponding continuous coordinate interval for a pixel are [x, x+1). I.e., floor is sufficient to convert from continuous coordinate to the corresponding discrete coordinate.</p> PARAMETER  DESCRIPTION <code>width</code> <p>Number of pixels in horizontal direction.</p> <p> TYPE: <code>int</code> </p> <code>height</code> <p>Number of pixels in vertical direction.</p> <p> TYPE: <code>int</code> </p> <code>fx</code> <p>Horizontal focal length.</p> <p> TYPE: <code>float</code> </p> <code>fy</code> <p>Vertical focal length.</p> <p> TYPE: <code>float</code> </p> <code>cx</code> <p>Principal point x-coordinate.</p> <p> TYPE: <code>float</code> </p> <code>cy</code> <p>Principal point y-coordinate.</p> <p> TYPE: <code>float</code> </p> <code>s</code> <p>Skew.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>pixel_center</code> <p>The center offset for the provided principal point.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def __init__(\n    self,\n    width: int,\n    height: int,\n    fx: float,\n    fy: float,\n    cx: float,\n    cy: float,\n    s: float = 0.0,\n    pixel_center: float = 0.0,\n) -&gt; None:\n    \"\"\"Initialize camera parameters.\n\n    Note that the principal point is only fully defined in combination with\n    pixel_center.\n\n    The pixel_center defines the relation between continuous image plane\n    coordinates and discrete pixel coordinates.\n\n    A discrete image coordinate (x, y) will correspond to the continuous\n    image coordinate (x + pixel_center, y + pixel_center). Normally pixel_center\n    will be either 0 or 0.5. During calibration it depends on the convention\n    the point features used to compute the calibration matrix.\n\n    Note that if pixel_center == 0, the corresponding continuous coordinate\n    interval for a pixel are [x-0.5, x+0.5). I.e., proper rounding has to be done\n    to convert from continuous coordinate to the corresponding discrete coordinate.\n\n    For pixel_center == 0.5, the corresponding continuous coordinate interval for a\n    pixel are [x, x+1). I.e., floor is sufficient to convert from continuous\n    coordinate to the corresponding discrete coordinate.\n\n    Args:\n        width: Number of pixels in horizontal direction.\n        height: Number of pixels in vertical direction.\n        fx: Horizontal focal length.\n        fy: Vertical focal length.\n        cx: Principal point x-coordinate.\n        cy: Principal point y-coordinate.\n        s: Skew.\n        pixel_center: The center offset for the provided principal point.\n    \"\"\"\n    # focal length\n    self.fx = fx\n    self.fy = fy\n\n    # principal point\n    self.cx = cx\n    self.cy = cy\n\n    self.pixel_center = pixel_center\n\n    # skew\n    self.s = s\n\n    # image dimensions\n    self.width = width\n    self.height = height\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_o3d_pinhole_camera_parameters","title":"get_o3d_pinhole_camera_parameters","text":"<pre><code>get_o3d_pinhole_camera_parameters() -&gt; (\n    o3d.camera.PinholeCameraParameters()\n)\n</code></pre> <p>Convert camera to Open3D pinhole camera parameters.</p> <p>Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z values are in front of camera). Open3D expects camera with pixel_center = 0 and does not support skew.</p> RETURNS DESCRIPTION <code>PinholeCameraParameters()</code> <p>The pinhole camera parameters.</p> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def get_o3d_pinhole_camera_parameters(self) -&gt; o3d.camera.PinholeCameraParameters():\n    \"\"\"Convert camera to Open3D pinhole camera parameters.\n\n    Open3D camera is at (0,0,0) looking along positive z axis (i.e., positive z\n    values are in front of camera). Open3D expects camera with pixel_center = 0\n    and does not support skew.\n\n    Returns:\n        The pinhole camera parameters.\n    \"\"\"\n    fx, fy, cx, cy, _ = self.get_pinhole_camera_parameters(0)\n    params = o3d.camera.PinholeCameraParameters()\n    params.intrinsic.set_intrinsics(self.width, self.height, fx, fy, cx, cy)\n    params.extrinsic = np.array(\n        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    return params\n</code></pre>"},{"location":"api_reference/camera_utils/#cpas_toolbox.camera_utils.Camera.get_pinhole_camera_parameters","title":"get_pinhole_camera_parameters","text":"<pre><code>get_pinhole_camera_parameters(pixel_center: float) -&gt; Tuple\n</code></pre> <p>Convert camera to general camera parameters.</p> PARAMETER  DESCRIPTION <code>pixel_center</code> <p>At which ratio of a square the pixel center should be for the resulting parameters. Typically 0 or 0.5. See class documentation for more info.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tuple</code> <ul> <li>fx, fy: The horizontal and vertical focal length</li> </ul> <code>Tuple</code> <ul> <li>cx, cy: The position of the principal point in continuous image plane coordinates considering the provided pixel center and the pixel center specified during the construction.</li> </ul> <code>Tuple</code> <ul> <li>s: The skew.</li> </ul> Source code in <code>cpas_toolbox/camera_utils.py</code> <pre><code>def get_pinhole_camera_parameters(self, pixel_center: float) -&gt; Tuple:\n    \"\"\"Convert camera to general camera parameters.\n\n    Args:\n        pixel_center:\n            At which ratio of a square the pixel center should be for the resulting\n            parameters. Typically 0 or 0.5. See class documentation for more info.\n\n    Returns:\n        - fx, fy: The horizontal and vertical focal length\n        - cx, cy:\n            The position of the principal point in continuous image plane\n            coordinates considering the provided pixel center and the pixel center\n            specified during the construction.\n        - s: The skew.\n    \"\"\"\n    cx_corrected = self.cx - self.pixel_center + pixel_center\n    cy_corrected = self.cy - self.pixel_center + pixel_center\n    return self.fx, self.fy, cx_corrected, cy_corrected, self.s\n</code></pre>"},{"location":"api_reference/cpas_method/","title":"cpas_method.py","text":""},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method","title":"cpas_toolbox.cpas_method","text":"<p>This module defines the interface for categorical pose and shape estimation methods.</p> <p>This module defines two classes: PredictionDict and CPASMethod. PredictionDict defines the prediction produced by a CPASMethod. CPASMethod defines the interface used to evaluate categorical pose and shape estimation methods.</p>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.PredictionDict","title":"PredictionDict","text":"<p>             Bases: <code>TypedDict</code></p> <p>Pose and shape prediction.</p> ATTRIBUTE DESCRIPTION <code>position</code> <p>Position of object center in camera frame. OpenCV convention. Shape (3,).</p> <p> TYPE: <code>Tensor</code> </p> <code>orientation</code> <p>Orientation of object in camera frame. OpenCV convention. Scalar-last quaternion, shape (4,).</p> <p> TYPE: <code>Tensor</code> </p> <code>extents</code> <p>Bounding box side lengths, shape (3,).</p> <p> TYPE: <code>Tensor</code> </p> <code>reconstructed_pointcloud</code> <p>Metrically-scaled reconstructed pointcloud in object frame. None if method does not perform reconstruction.</p> <p> TYPE: <code>Optional[Tensor]</code> </p> <code>reconstructed_mesh</code> <p>Metrically-scaled reconstructed mesh in object frame. None if method does not perform reconstruction.</p> <p> TYPE: <code>Optional[TriangleMesh]</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>class PredictionDict(TypedDict):\n    \"\"\"Pose and shape prediction.\n\n    Attributes:\n        position:\n            Position of object center in camera frame. OpenCV convention. Shape (3,).\n        orientation:\n            Orientation of object in camera frame. OpenCV convention.\n            Scalar-last quaternion, shape (4,).\n        extents:\n            Bounding box side lengths, shape (3,).\n        reconstructed_pointcloud:\n            Metrically-scaled reconstructed pointcloud in object frame.\n            None if method does not perform reconstruction.\n        reconstructed_mesh:\n            Metrically-scaled reconstructed mesh in object frame.\n            None if method does not perform reconstruction.\n    \"\"\"\n\n    position: torch.Tensor\n    orientation: torch.Tensor\n    extents: torch.Tensor\n    reconstructed_pointcloud: Optional[torch.Tensor]\n    reconstructed_mesh: Optional[o3d.geometry.TriangleMesh]\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod","title":"CPASMethod","text":"<p>             Bases: <code>ABC</code></p> <p>Interface class for categorical pose and shape estimation methods.</p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>class CPASMethod(ABC):\n    \"\"\"Interface class for categorical pose and shape estimation methods.\"\"\"\n\n    def __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize categorical pose and shape estimation method.\n\n        Args:\n            config: Method configuration dictionary.\n            camera: Camera used for the input image.\n        \"\"\"\n        pass\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"Run a method to predict pose and shape of an object.\n\n        Args:\n            color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n            depth_image: The depth image, shape (H, W), meters, float.\n            instance_mask: Mask of object of interest. (H, W), bool.\n            category_str: The category of the object.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod.__init__","title":"__init__","text":"<pre><code>__init__(config: dict, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize categorical pose and shape estimation method.</p> PARAMETER  DESCRIPTION <code>config</code> <p>Method configuration dictionary.</p> <p> TYPE: <code>dict</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>def __init__(self, config: dict, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize categorical pose and shape estimation method.\n\n    Args:\n        config: Method configuration dictionary.\n        camera: Camera used for the input image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cpas_method/#cpas_toolbox.cpas_method.CPASMethod.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>Run a method to predict pose and shape of an object.</p> PARAMETER  DESCRIPTION <code>color_image</code> <p>The color image, shape (H, W, 3), RGB, 0-1, float.</p> <p> TYPE: <code>Tensor</code> </p> <code>depth_image</code> <p>The depth image, shape (H, W), meters, float.</p> <p> TYPE: <code>Tensor</code> </p> <code>instance_mask</code> <p>Mask of object of interest. (H, W), bool.</p> <p> TYPE: <code>Tensor</code> </p> <code>category_str</code> <p>The category of the object.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_method.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"Run a method to predict pose and shape of an object.\n\n    Args:\n        color_image: The color image, shape (H, W, 3), RGB, 0-1, float.\n        depth_image: The depth image, shape (H, W), meters, float.\n        instance_mask: Mask of object of interest. (H, W), bool.\n        category_str: The category of the object.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/evaluate/","title":"evaluate.py","text":""},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate","title":"cpas_toolbox.evaluate","text":"<p>Script to run pose and shape evaluation for different datasets and methods.</p>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator","title":"Evaluator","text":"<p>Class to evaluate various pose and shape estimation algorithms.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>class Evaluator:\n    \"\"\"Class to evaluate various pose and shape estimation algorithms.\"\"\"\n\n    # ShapeNetV2 convention for all objects and datasets assumed\n    # for simplicity assume all cans, bowls and bottles to be rotation symmetric\n    SYMMETRY_AXIS_DICT = {\n        \"mug\": None,\n        \"laptop\": None,\n        \"camera\": None,\n        \"can\": 1,\n        \"bowl\": 1,\n        \"bottle\": 1,\n    }\n\n    def __init__(self, config: dict) -&gt; None:\n        \"\"\"Initialize model wrappers and evaluator.\"\"\"\n        self._parse_config(config)\n\n    def _parse_config(self, config: dict) -&gt; None:\n        \"\"\"Read config and initialize method wrappers.\"\"\"\n        self._init_dataset(config[\"dataset_config\"])\n\n        self._visualize_input = config[\"visualize_input\"]\n        self._visualize_prediction = config[\"visualize_prediction\"]\n        self._visualize_gt = config[\"visualize_gt\"]\n        self._fast_eval = config[\"fast_eval\"]\n        self._store_visualization = config[\"store_visualization\"]\n        self._run_name = (\n            f\"{self._dataset_name}_eval_{config['run_name']}_\"\n            f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n        )\n        self._out_dir_path = config[\"out_dir\"]\n        self._metrics = config[\"metrics\"]\n        self._num_gt_points = config[\"num_gt_points\"]\n        self._vis_camera_json = config[\"vis_camera_json\"]\n        self._render_options_json = config[\"render_options_json\"]\n\n        self._cam = camera_utils.Camera(**config[\"camera\"])\n        self._init_wrappers(config[\"methods\"])\n\n        self._config = config\n\n    def _init_dataset(self, dataset_config: dict) -&gt; None:\n        \"\"\"Initialize reading of dataset.\n\n        This includes sanity checks whether the provided path is correct.\n        \"\"\"\n        self._dataset_name = dataset_config[\"name\"]\n        print(f\"Initializing {self._dataset_name} dataset...\")\n        dataset_type = utils.str_to_object(dataset_config[\"type\"])\n        self._dataset = dataset_type(config=dataset_config[\"config_dict\"])\n        # Faster but probably only worth it if whole evaluation supports batches\n        # self._dataloader = DataLoader(self._dataset, 1, num_workers=8)\n        if len(self._dataset) == 0:\n            print(f\"No images found for dataset {self._dataset_name}\")\n            exit()\n        print(f\"{len(self._dataset)} samples found for dataset {self._dataset_name}.\")\n\n    def _init_wrappers(self, method_configs: dict) -&gt; None:\n        \"\"\"Initialize method wrappers.\"\"\"\n        self._wrappers = {}\n        for method_dict in method_configs.values():\n            method_name = method_dict[\"name\"]\n            print(f\"Initializing {method_name}...\")\n            method_type = utils.str_to_object(method_dict[\"method_type\"])\n            if method_type is None:\n                print(f\"Could not find class {method_dict['method_type']}\")\n                continue\n            self._wrappers[method_name] = method_type(\n                config=method_dict[\"config_dict\"], camera=self._cam\n            )\n\n    def _eval_method(self, method_name: str, method_wrapper: CPASMethod) -&gt; None:\n        \"\"\"Run and evaluate method on all samples.\"\"\"\n        print(f\"Run {method_name}...\")\n        self._init_metrics()\n        indices = list(range(len(self._dataset)))\n        random.seed(0)\n        random.shuffle(indices)\n        for i in tqdm(indices):\n            if self._fast_eval and i % 10 != 0:\n                continue\n            sample = self._dataset[i]\n            if self._visualize_input:\n                _, ((ax1, ax2), (ax3, _)) = plt.subplots(2, 2)\n                ax1.imshow(sample[\"color\"].numpy())\n                ax2.imshow(sample[\"depth\"].numpy())\n                ax3.imshow(sample[\"mask\"].numpy())\n                plt.show()\n\n            t_start = time.time()\n            prediction = method_wrapper.inference(\n                color_image=sample[\"color\"],\n                depth_image=sample[\"depth\"],\n                instance_mask=sample[\"mask\"],\n                category_str=sample[\"category_str\"],\n            )\n            inference_time = time.time() - t_start\n\n            self._runtime_data[\"total\"] += inference_time\n            self._runtime_data[\"total_squared\"] += inference_time**2\n            self._runtime_data[\"count\"] += 1\n            self._runtime_data[\"min\"] = min(self._runtime_data[\"min\"], inference_time)\n            self._runtime_data[\"max\"] = max(self._runtime_data[\"max\"], inference_time)\n\n            if self._visualize_gt:\n                visualize_estimation(\n                    color_image=sample[\"color\"],\n                    depth_image=sample[\"depth\"],\n                    local_cv_position=sample[\"position\"],\n                    local_cv_orientation_q=sample[\"quaternion\"],\n                    reconstructed_mesh=self._dataset.load_mesh(sample[\"obj_path\"]),\n                    extents=sample[\"scale\"],\n                    camera=self._cam,\n                    vis_camera_json=self._vis_camera_json,\n                    render_options_json=self._render_options_json,\n                )  # GT estimate\n            if self._visualize_prediction:\n                visualize_estimation(\n                    color_image=sample[\"color\"],\n                    depth_image=sample[\"depth\"],\n                    local_cv_position=prediction[\"position\"],\n                    local_cv_orientation_q=prediction[\"orientation\"],\n                    extents=prediction[\"extents\"],\n                    reconstructed_points=prediction[\"reconstructed_pointcloud\"],\n                    reconstructed_mesh=prediction[\"reconstructed_mesh\"],\n                    camera=self._cam,\n                    vis_camera_json=self._vis_camera_json,\n                    render_options_json=self._render_options_json,\n                )\n            if self._store_visualization:\n                vis_dir_path = os.path.join(\n                    self._out_dir_path, self._run_name, \"visualization\"\n                )\n                os.makedirs(vis_dir_path, exist_ok=True)\n                vis_file_path = os.path.join(vis_dir_path, f\"{i:06}_{method_name}.jpg\")\n                visualize_estimation(\n                    color_image=sample[\"color\"],\n                    depth_image=sample[\"depth\"],\n                    local_cv_position=prediction[\"position\"],\n                    local_cv_orientation_q=prediction[\"orientation\"],\n                    extents=prediction[\"extents\"],\n                    reconstructed_points=prediction[\"reconstructed_pointcloud\"],\n                    reconstructed_mesh=prediction[\"reconstructed_mesh\"],\n                    camera=self._cam,\n                    vis_camera_json=self._vis_camera_json,\n                    render_options_json=self._render_options_json,\n                    vis_file_path=vis_file_path,\n                )\n\n            self._eval_prediction(prediction, sample)\n        self._finalize_metrics(method_name)\n\n    def _eval_prediction(self, prediction: PredictionDict, sample: dict) -&gt; None:\n        \"\"\"Evaluate all metrics for a prediction.\"\"\"\n        # correctness metric\n        for metric_name in self._metrics.keys():\n            self._eval_metric(metric_name, prediction, sample)\n\n    def _init_metrics(self) -&gt; None:\n        \"\"\"Initialize metrics.\"\"\"\n        self._metric_data = {}\n        self._runtime_data = {\n            \"total\": 0.0,\n            \"total_squared\": 0.0,\n            \"count\": 0.0,\n            \"min\": 1e10,\n            \"max\": 0.0,\n        }\n        for metric_name, metric_config_dict in self._metrics.items():\n            self._metric_data[metric_name] = self._init_metric_data(metric_config_dict)\n\n    def _init_metric_data(self, metric_config_dict: dict) -&gt; dict:\n        \"\"\"Create data structure necessary to compute a metric.\"\"\"\n        metric_data = {}\n        if \"position_thresholds\" in metric_config_dict:\n            pts = metric_config_dict[\"position_thresholds\"]\n            dts = metric_config_dict[\"deg_thresholds\"]\n            its = metric_config_dict[\"iou_thresholds\"]\n            fts = metric_config_dict[\"f_thresholds\"]\n            metric_data[\"correct_counters\"] = np.zeros(\n                (\n                    len(pts),\n                    len(dts),\n                    len(its),\n                    len(fts),\n                    self._dataset.num_categories + 1,\n                )\n            )\n            metric_data[\"total_counters\"] = np.zeros(self._dataset.num_categories + 1)\n        elif \"pointwise_f\" in metric_config_dict:\n            metric_data[\"means\"] = np.zeros(self._dataset.num_categories + 1)\n            metric_data[\"m2s\"] = np.zeros(self._dataset.num_categories + 1)\n            metric_data[\"counts\"] = np.zeros(self._dataset.num_categories + 1)\n        else:\n            raise NotImplementedError(\"Unsupported metric configuration.\")\n        return metric_data\n\n    def _eval_metric(\n        self, metric_name: str, prediction: PredictionDict, sample: dict\n    ) -&gt; None:\n        \"\"\"Evaluate and update single metric for a single prediction.\n\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\n        metric_config_dict = self._metrics[metric_name]\n        if \"position_thresholds\" in metric_config_dict:  # correctness metrics\n            self._eval_correctness_metric(metric_name, prediction, sample)\n        elif \"pointwise_f\" in metric_config_dict:  # pointwise reconstruction metrics\n            self._eval_pointwise_metric(metric_name, prediction, sample)\n        else:\n            raise NotImplementedError(\n                f\"Unsupported metric configuration with name {metric_name}.\"\n            )\n\n    def _eval_correctness_metric(\n        self, metric_name: str, prediction: PredictionDict, sample: dict\n    ) -&gt; None:\n        \"\"\"Evaluate and update single correctness metric for a single prediction.\n\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\n        metric_dict = self._metrics[metric_name]\n        correct_counters = self._metric_data[metric_name][\"correct_counters\"]\n        total_counters = self._metric_data[metric_name][\"total_counters\"]\n        category_id = sample[\"category_id\"]\n        total_counters[category_id] += 1\n        total_counters[-1] += 1\n        gt_points, pred_points = self._get_points(sample, prediction, True)\n        for pi, p in enumerate(metric_dict[\"position_thresholds\"]):\n            for di, d in enumerate(metric_dict[\"deg_thresholds\"]):\n                for ii, i in enumerate(metric_dict[\"iou_thresholds\"]):\n                    for fi, f in enumerate(metric_dict[\"f_thresholds\"]):\n                        correct = metrics.correct_thresh(\n                            position_gt=sample[\"position\"].cpu().numpy(),\n                            position_prediction=prediction[\"position\"].cpu().numpy(),\n                            orientation_gt=Rotation.from_quat(sample[\"quaternion\"]),\n                            orientation_prediction=Rotation.from_quat(\n                                prediction[\"orientation\"]\n                            ),\n                            extent_gt=sample[\"scale\"].cpu().numpy(),\n                            extent_prediction=prediction[\"extents\"].cpu().numpy(),\n                            points_gt=gt_points,\n                            points_prediction=pred_points,\n                            position_threshold=p,\n                            degree_threshold=d,\n                            iou_3d_threshold=i,\n                            fscore_threshold=f,\n                            rotational_symmetry_axis=self.SYMMETRY_AXIS_DICT[\n                                sample[\"category_str\"]\n                            ],\n                        )\n                        correct_counters[pi, di, ii, fi, category_id] += correct\n                        correct_counters[pi, di, ii, fi, -1] += correct  # all\n\n    def _eval_pointwise_metric(\n        self, metric_name: str, prediction: PredictionDict, sample: dict\n    ) -&gt; None:\n        \"\"\"Evaluate and update single pointwise metric for a single prediction.\n\n        Args:\n            metric_name: Name of metric to evaluate.\n            prediction: Dictionary containing prediction data.\n            sample: Sample containing ground truth information.\n        \"\"\"\n        metric_config_dict = self._metrics[metric_name]\n        means = self._metric_data[metric_name][\"means\"]\n        m2s = self._metric_data[metric_name][\"m2s\"]\n        counts = self._metric_data[metric_name][\"counts\"]\n        category_id = sample[\"category_id\"]\n        point_metric = utils.str_to_object(metric_config_dict[\"pointwise_f\"])\n\n        gt_points, pred_points = self._get_points(\n            sample, prediction, metric_config_dict[\"posed\"]\n        )\n\n        result = point_metric(\n            gt_points.numpy(), pred_points.numpy(), **metric_config_dict[\"kwargs\"]\n        )\n\n        # Use Welfords algorithm to update mean and variance\n        # for category\n        counts[category_id] += 1\n        delta = result - means[category_id]\n        means[category_id] += delta / counts[category_id]\n        delta2 = result - means[category_id]\n        m2s[category_id] += delta * delta2\n\n        # for all\n        counts[-1] += 1\n        delta = result - means[-1]\n        means[-1] += delta / counts[-1]\n        delta2 = result - means[-1]\n        m2s[-1] += delta * delta2\n\n    def _get_points(\n        self, sample: dict, prediction: PredictionDict, posed: bool\n    ) -&gt; Tuple[np.ndarray]:\n        # load ground truth mesh\n        gt_mesh = self._dataset.load_mesh(sample[\"obj_path\"])\n        gt_points = torch.from_numpy(\n            np.asarray(gt_mesh.sample_points_uniformly(self._num_gt_points).points)\n        )\n        pred_points = prediction[\"reconstructed_pointcloud\"]\n\n        # transform points if posed\n        if posed:\n            gt_points = quaternion_utils.quaternion_apply(\n                sample[\"quaternion\"], gt_points\n            )\n            gt_points += sample[\"position\"]\n            pred_points = quaternion_utils.quaternion_apply(\n                prediction[\"orientation\"], pred_points\n            )\n            pred_points += prediction[\"position\"]\n        return gt_points, pred_points\n\n    def _finalize_runtime_metric(self) -&gt; dict:\n        mean = self._runtime_data[\"total\"] / self._runtime_data[\"count\"]\n        mean_squared = self._runtime_data[\"total_squared\"] / self._runtime_data[\"count\"]\n        variance = mean_squared - mean**2\n        std = math.sqrt(variance)\n        return {\n            \"mean\": mean,\n            \"variance\": variance,\n            \"std\": std,\n            \"min\": self._runtime_data[\"min\"],\n            \"max\": self._runtime_data[\"max\"],\n        }\n\n    def _finalize_metrics(self, method_name: str) -&gt; None:\n        \"\"\"Finalize metrics after all samples have been evaluated.\n\n        Also writes results to disk and create plot if applicable.\n        \"\"\"\n        results_dir_path = os.path.join(self._out_dir_path, self._run_name)\n        os.makedirs(results_dir_path, exist_ok=True)\n        yaml_file_path = os.path.join(results_dir_path, \"results.yaml\")\n\n        self._results_dict[method_name] = {}\n\n        self._runtime_results_dict[method_name] = self._finalize_runtime_metric()\n\n        for metric_name, metric_dict in self._metrics.items():\n            if \"position_thresholds\" in metric_dict:  # correctness metrics\n                correct_counter = self._metric_data[metric_name][\"correct_counters\"]\n                total_counter = self._metric_data[metric_name][\"total_counters\"]\n                correct_percentage = correct_counter / total_counter\n                self._results_dict[method_name][\n                    metric_name\n                ] = correct_percentage.tolist()\n                self._create_metric_plot(\n                    method_name,\n                    metric_name,\n                    metric_dict,\n                    correct_percentage,\n                    results_dir_path,\n                )\n            elif \"pointwise_f\" in metric_dict:  # pointwise reconstruction metrics\n                counts = self._metric_data[metric_name][\"counts\"]\n                m2s = self._metric_data[metric_name][\"m2s\"]\n                means = self._metric_data[metric_name][\"means\"]\n                variances = m2s / counts\n                stds = np.sqrt(variances)\n                self._results_dict[method_name][metric_name] = {\n                    \"means\": means.tolist(),\n                    \"variances\": variances.tolist(),\n                    \"std\": stds.tolist(),\n                }\n            else:\n                raise NotImplementedError(\n                    f\"Unsupported metric configuration with name {metric_name}.\"\n                )\n\n        results_dict = {\n            **self._config,\n            \"results\": self._results_dict,\n            \"runtime_results\": self._runtime_results_dict,\n        }\n        yoco.save_config_to_file(yaml_file_path, results_dict)\n        print(f\"Results saved to: {yaml_file_path}\")\n\n    def _create_metric_plot(\n        self,\n        method_name: str,\n        metric_name: str,\n        metric_dict: dict,\n        correct_percentage: np.ndarray,\n        out_dir: str,\n    ) -&gt; None:\n        \"\"\"Create metric plot if applicable.\n\n        Applicable means only one of the thresholds has multiple values.\n\n        Args:\n            correct_percentage:\n                Array holding the percentage of correct predictions.\n                Shape (NUM_POS_THRESH,NUM_DEG_THRESH,NUM_IOU_THRESH,NUM_CATEGORIES + 1).\n        \"\"\"\n        axis = None\n        for i, s in enumerate(correct_percentage.shape[:4]):\n            if s != 1 and axis is None:\n                axis = i\n            elif s != 1:  # multiple axis with != 1 size\n                return\n        if axis is None:\n            return\n        axis_to_threshold_key = {\n            0: \"position_thresholds\",\n            1: \"deg_thresholds\",\n            2: \"iou_thresholds\",\n            3: \"f_thresholds\",\n        }\n        threshold_key = axis_to_threshold_key[axis]\n        x_values = metric_dict[threshold_key]\n\n        for category_id in range(self._dataset.num_categories + 1):\n            y_values = correct_percentage[..., category_id].flatten()\n            if category_id in self._dataset.category_id_to_str:\n                label = self._dataset.category_id_to_str[category_id]\n            else:\n                label = \"all\"\n            plt.plot(x_values, y_values, label=label)\n\n        figure_file_path = os.path.join(out_dir, f\"{method_name}_{metric_name}\")\n        plt.xlabel(threshold_key)\n        plt.ylabel(\"Correct\")\n        plt.legend()\n        plt.grid()\n\n        tikzplotlib.save(figure_file_path + \".tex\")\n        plt.savefig(figure_file_path + \".png\")\n        plt.close()\n\n    def run(self) -&gt; None:\n        \"\"\"Run the evaluation.\"\"\"\n        self._results_dict = {}\n        self._runtime_results_dict = {}\n        for method_name, method_wrapper in self._wrappers.items():\n            self._eval_method(method_name, method_wrapper)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.__init__","title":"__init__","text":"<pre><code>__init__(config: dict) -&gt; None\n</code></pre> <p>Initialize model wrappers and evaluator.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def __init__(self, config: dict) -&gt; None:\n    \"\"\"Initialize model wrappers and evaluator.\"\"\"\n    self._parse_config(config)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.Evaluator.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the evaluation.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run the evaluation.\"\"\"\n    self._results_dict = {}\n    self._runtime_results_dict = {}\n    for method_name, method_wrapper in self._wrappers.items():\n        self._eval_method(method_name, method_wrapper)\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.visualize_estimation","title":"visualize_estimation","text":"<pre><code>visualize_estimation(\n    color_image: Optional[torch.Tensor] = None,\n    depth_image: Optional[torch.Tensor] = None,\n    camera: Optional[camera_utils.Camera] = None,\n    local_cv_position: Optional[torch.Tensor] = None,\n    local_cv_orientation_q: Optional[torch.Tensor] = None,\n    instance_mask: Optional[torch.Tensor] = None,\n    extents: Optional[torch.Tensor] = None,\n    reconstructed_points: Optional[torch.Tensor] = None,\n    reconstructed_mesh: Optional[o3d.geometry.TriangleMesh] = None,\n    vis_camera_json: Optional[str] = None,\n    render_options_json: Optional[str] = None,\n    vis_file_path: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Visualize prediction and ask for confirmation.</p> PARAMETER  DESCRIPTION <code>color_image</code> <p>The unmasked color image. Not visualized if None. Shape (H,W,3), RGB, 0-1, float.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>depth_image</code> <p>The unmasked depth image visualized as a point set. Not visualized if None. Shape (H,W), float (meters along z).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>camera</code> <p>Camera used to project depth image to point set.</p> <p> TYPE: <code>Optional[Camera]</code> DEFAULT: <code>None</code> </p> <code>local_cv_position</code> <p>The position in the OpenCV camera frame. Not visualized if None. Shape (3,).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>local_cv_orientation_q</code> <p>The orientation in the OpenCV camera frame. Not visualized if None. Scalar last, shape (4,).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>extents</code> <p>Extents of the bounding box. Not visualized if None. Shape (3,).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>instance_mask</code> <p>The instance mask. No masking if None. Shape (H,W).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>reconstructed_points</code> <p>Reconstructed points in object coordinate frame. Not visualized if None. The points must already metrically scaled. Shape (M,3).</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>reconstructed_mesh</code> <p>Reconstructed mesh in object coordinate frame. Not visualized if None. The mesh must already metrically scaled.</p> <p> TYPE: <code>Optional[TriangleMesh]</code> DEFAULT: <code>None</code> </p> <code>vis_camera_json</code> <p>Path to open3d camera options json file that will be applied. Generated by pressing p in desired view. No render options will be applied if None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>vis_file_path</code> <p>If not None, the image will be rendered off screen and saved at the specified path.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>True if confirmation was positive. False if negative.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def visualize_estimation(\n    color_image: Optional[torch.Tensor] = None,\n    depth_image: Optional[torch.Tensor] = None,\n    camera: Optional[camera_utils.Camera] = None,\n    local_cv_position: Optional[torch.Tensor] = None,\n    local_cv_orientation_q: Optional[torch.Tensor] = None,\n    instance_mask: Optional[torch.Tensor] = None,\n    extents: Optional[torch.Tensor] = None,\n    reconstructed_points: Optional[torch.Tensor] = None,\n    reconstructed_mesh: Optional[o3d.geometry.TriangleMesh] = None,\n    vis_camera_json: Optional[str] = None,\n    render_options_json: Optional[str] = None,\n    vis_file_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Visualize prediction and ask for confirmation.\n\n    Args:\n        color_image:\n            The unmasked color image. Not visualized if None.\n            Shape (H,W,3), RGB, 0-1, float.\n        depth_image:\n            The unmasked depth image visualized as a point set. Not visualized if None.\n            Shape (H,W), float (meters along z).\n        camera: Camera used to project depth image to point set.\n        local_cv_position:\n            The position in the OpenCV camera frame. Not visualized if None. Shape (3,).\n        local_cv_orientation_q:\n            The orientation in the OpenCV camera frame. Not visualized if None.\n            Scalar last, shape (4,).\n        extents: Extents of the bounding box. Not visualized if None. Shape (3,).\n        instance_mask: The instance mask. No masking if None. Shape (H,W).\n        reconstructed_points:\n            Reconstructed points in object coordinate frame. Not visualized if None.\n            The points must already metrically scaled.\n            Shape (M,3).\n        reconstructed_mesh:\n            Reconstructed mesh in object coordinate frame. Not visualized if None.\n            The mesh must already metrically scaled.\n        vis_camera_json:\n            Path to open3d camera options json file that will be applied.\n            Generated by pressing p in desired view.\n            No render options will be applied if None.\n        vis_file_path:\n            If not None, the image will be rendered off screen and saved at the\n            specified path.\n\n    Returns:\n        True if confirmation was positive. False if negative.\n    \"\"\"\n    o3d_geometries = []\n\n    if depth_image is not None:\n        if instance_mask is not None:\n            valid_depth_mask = (depth_image != 0) * instance_mask\n        else:\n            valid_depth_mask = depth_image != 0\n        masked_pointset = pointset_utils.depth_to_pointcloud(\n            depth_image,\n            camera,\n            normalize=False,\n            mask=instance_mask,\n            convention=\"opencv\",\n        )\n        o3d_points = o3d.geometry.PointCloud(\n            points=o3d.utility.Vector3dVector(masked_pointset.cpu().numpy())\n        )\n\n        if color_image is not None:\n            pointset_colors = color_image[valid_depth_mask]\n            o3d_points.colors = o3d.utility.Vector3dVector(\n                pointset_colors.cpu().numpy()\n            )\n\n        o3d_geometries.append(o3d_points)\n\n    # coordinate frame\n    if local_cv_position is not None:\n        local_cv_position = local_cv_position.cpu().double().numpy()  # shape (3,)\n        local_cv_orientation_q = (\n            local_cv_orientation_q.cpu().double().numpy()\n        )  # shape (4,)\n        local_cv_orientation_m = Rotation.from_quat(local_cv_orientation_q).as_matrix()\n        o3d_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n        o3d_frame.rotate(\n            local_cv_orientation_m,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        o3d_frame.translate(local_cv_position[:, None])\n        o3d_geometries.append(o3d_frame)\n\n    o3d_cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n    o3d_geometries.append(o3d_cam_frame)\n\n    if extents is not None:\n        extents = extents.cpu().double().numpy()\n        o3d_obb = o3d.geometry.OrientedBoundingBox(\n            center=local_cv_position[:, None],\n            R=local_cv_orientation_m,\n            extent=extents[:, None],\n        )\n        o3d_geometries.append(o3d_obb)\n\n    if reconstructed_points is not None and reconstructed_mesh is None:\n        o3d_rec_points = o3d.geometry.PointCloud(\n            points=o3d.utility.Vector3dVector(reconstructed_points.cpu().numpy())\n        )\n        o3d_rec_points.rotate(\n            local_cv_orientation_m,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        o3d_rec_points.translate(local_cv_position[:, None])\n        o3d_geometries.append(o3d_rec_points)\n\n    if reconstructed_mesh is not None:\n        # copy the mesh to keep original unmoved\n        posed_mesh = o3d.geometry.TriangleMesh(reconstructed_mesh)\n        posed_mesh.rotate(\n            local_cv_orientation_m,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        posed_mesh.translate(local_cv_position[:, None])\n        posed_mesh.compute_vertex_normals()\n        o3d_geometries.append(posed_mesh)\n\n    vis = o3d.visualization.Visualizer()\n    if vis_camera_json is not None:\n        vis_camera = o3d.io.read_pinhole_camera_parameters(vis_camera_json)\n        width = vis_camera.intrinsic.width\n        height = vis_camera.intrinsic.height\n    else:\n        width = 800\n        height = 600\n        vis_camera = None\n    vis.create_window(width=width, height=height, visible=(vis_file_path is None))\n\n    for g in o3d_geometries:\n        vis.add_geometry(g)\n\n    if vis_camera is not None:\n        view_control = vis.get_view_control()\n        view_control.convert_from_pinhole_camera_parameters(vis_camera)\n\n    if render_options_json is not None:\n        render_option = vis.get_render_option()\n        render_option.load_from_json(render_options_json)\n\n    if vis_file_path is not None:\n        vis.poll_events()\n        vis.update_renderer()\n        vis.capture_screen_image(vis_file_path, do_render=True)\n    else:\n        vis.run()\n</code></pre>"},{"location":"api_reference/evaluate/#cpas_toolbox.evaluate.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point of the evaluation program.</p> Source code in <code>cpas_toolbox/evaluate.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Entry point of the evaluation program.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Pose and shape estimation evaluation on REAL275 data\"\n    )\n    parser.add_argument(\"--config\", required=True, nargs=\"+\")\n    parser.add_argument(\"--out_dir\", required=True)\n\n    config = yoco.load_config_from_args(\n        parser,\n        search_paths=[\n            \".\",\n            \"~/.cpas_toolbox\",\n            os.path.join(os.path.dirname(__file__), \"config\"),\n            os.path.dirname(__file__),\n        ],\n    )\n\n    evaluator = Evaluator(config)\n    evaluator.run()\n</code></pre>"},{"location":"api_reference/metrics/","title":"metrics.py","text":""},{"location":"api_reference/metrics/#cpas_toolbox.metrics","title":"cpas_toolbox.metrics","text":"<p>Metrics for shape evaluation.</p>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.diameter","title":"diameter","text":"<pre><code>diameter(points: np.ndarray) -&gt; float\n</code></pre> <p>Compute largest Euclidean distance between any two points.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true</p> <p> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> </p> <p>Returns:     Ratio of reconstructed points with closest ground truth point closer than     threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def diameter(points: np.ndarray) -&gt; float:\n    \"\"\"Compute largest Euclidean distance between any two points.\n\n    Args:\n        points_gt: set of true\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\n    try:\n        hull = scipy.spatial.ConvexHull(points)\n    except scipy.spatial.qhull.QhullError:\n        # fallback to brute force distance matrix\n        return np.max(scipy.spatial.distance_matrix(points, points))\n\n    # this is wasteful, if too slow implement rotating caliper method\n    return np.max(\n        scipy.spatial.distance_matrix(points[hull.vertices], points[hull.vertices])\n    )\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_accuracy","title":"mean_accuracy","text":"<pre><code>mean_accuracy(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute accuracy metric.</p> <p>Accuracy metric is the same as the mean pointwise distance (or asymmetric chamfer distance) from rec to gt.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)     ground truth points.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def mean_accuracy(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute accuracy metric.\n\n    Accuracy metric is the same as the mean pointwise distance (or asymmetric chamfer\n    distance) from rec to gt.\n\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from reconstructed points to closest (in p-norm)\n        ground truth points.\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_gt)\n    d, _ = kd_tree.query(points_rec, p=p_norm)\n    if normalize:\n        return np.mean(d) / diameter(points_gt)\n    else:\n        return np.mean(d)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.mean_completeness","title":"mean_completeness","text":"<pre><code>mean_completeness(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute completeness metric.</p> <p>Completeness metric is the same as the mean pointwise distance (or asymmetric chamfer distance) from gt to rec.</p> <p>See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Arithmetic mean of p-norm from ground truth points to closest (in p-norm)     reconstructed points.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def mean_completeness(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute completeness metric.\n\n    Completeness metric is the same as the mean pointwise distance (or asymmetric\n    chamfer distance) from gt to rec.\n\n    See, for example, Occupancy Networks Learning 3D Reconstruction in Function Space,\n    Mescheder et al., 2019.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of p-norm from ground truth points to closest (in p-norm)\n        reconstructed points.\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_rec)\n    d, _ = kd_tree.query(points_gt, p=p_norm)\n    if normalize:\n        return np.mean(d) / diameter(points_gt)\n    else:\n        return np.mean(d)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.symmetric_chamfer","title":"symmetric_chamfer","text":"<pre><code>symmetric_chamfer(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute symmetric chamfer distance.</p> <p>There are various slightly different definitions for the chamfer distance.</p> <p>Note that completeness and accuracy are themselves sometimes referred to as chamfer distances, with symmetric chamfer distance being the combination of the two.</p> <p>Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D Reconstruction in Function Space, Mescheder et al., 2019) refers to using arithmetic mean (note that this is actually differently scaled from L1) when combining accuracy and completeness.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide result by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Arithmetic mean of accuracy and completeness metrics using the specified p-norm.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def symmetric_chamfer(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute symmetric chamfer distance.\n\n    There are various slightly different definitions for the chamfer distance.\n\n    Note that completeness and accuracy are themselves sometimes referred to as\n    chamfer distances, with symmetric chamfer distance being the combination of the two.\n\n    Chamfer L1 in the literature (see, for example, Occupancy Networks Learning 3D\n    Reconstruction in Function Space, Mescheder et al., 2019) refers to using\n    arithmetic mean (note that this is actually differently scaled from L1) when\n    combining accuracy and completeness.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide result by Euclidean extent of points_gt\n    Returns:\n        Arithmetic mean of accuracy and completeness metrics using the specified p-norm.\n    \"\"\"\n    return (\n        mean_completeness(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n        + mean_accuracy(points_gt, points_rec, p_norm=p_norm, normalize=normalize)\n    ) / 2\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.normalized_average_distance","title":"normalized_average_distance","text":"<pre><code>normalized_average_distance(\n    points_gt: np.ndarray, points_rec: np.ndarray, p_norm: int = 2\n) -&gt; float\n</code></pre> <p>Compute the maximum of the directed normalized average distances.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Maximum of normalized mean accuracy and mean completeness metrics using the</p> <code>float</code> <p>specified p-norm.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def normalized_average_distance(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    p_norm: int = 2,\n) -&gt; float:\n    \"\"\"Compute the maximum of the directed normalized average distances.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n\n    Returns:\n        Maximum of normalized mean accuracy and mean completeness metrics using the\n        specified p-norm.\n    \"\"\"\n    return max(\n        mean_completeness(points_gt, points_rec, p_norm=p_norm, normalize=True),\n        mean_accuracy(points_gt, points_rec, p_norm=p_norm, normalize=True),\n    )\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.completeness_thresh","title":"completeness_thresh","text":"<pre><code>completeness_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute thresholded completion metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Ratio of ground truth points with closest reconstructed point closer than     threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def completeness_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute thresholded completion metric.\n\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of ground truth points with closest reconstructed point closer than\n        threshold (in p-norm).\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_rec)\n    d, _ = kd_tree.query(points_gt, p=p_norm)\n    if normalize:\n        return np.sum(d / diameter(points_gt) &lt; threshold) / points_gt.shape[0]\n    else:\n        return np.sum(d &lt; threshold) / points_gt.shape[0]\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.accuracy_thresh","title":"accuracy_thresh","text":"<pre><code>accuracy_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute thresholded accuracy metric.</p> <p>See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Ratio of reconstructed points with closest ground truth point closer than     threshold (in p-norm).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def accuracy_thresh(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute thresholded accuracy metric.\n\n    See FroDO: From Detections to 3D Objects, Ru\u0308nz et al., 2020.\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Ratio of reconstructed points with closest ground truth point closer than\n        threshold (in p-norm).\n    \"\"\"\n    kd_tree = scipy.spatial.KDTree(points_gt)\n    d, _ = kd_tree.query(points_rec, p=p_norm)\n    if normalize:\n        return np.sum(d / diameter(points_gt) &lt; threshold) / points_rec.shape[0]\n    else:\n        return np.sum(d &lt; threshold) / points_rec.shape[0]\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.reconstruction_fscore","title":"reconstruction_fscore","text":"<pre><code>reconstruction_fscore(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Compute reconstruction fscore.</p> <p>See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019</p> PARAMETER  DESCRIPTION <code>points_gt</code> <p>set of true points, expected shape (N,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>points_rec</code> <p>set of reconstructed points, expected shape (M,3)</p> <p> TYPE: <code>ndarray</code> </p> <code>threshold</code> <p>distance threshold to count a point as correct</p> <p> TYPE: <code>float</code> </p> <code>p_norm</code> <p>which Minkowski p-norm is used for distance and nearest neighbor query</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>normalize</code> <p>whether to divide distances by Euclidean extent of points_gt</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     Harmonic mean of precision (thresholded accuracy) and recall (thresholded     completeness).</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def reconstruction_fscore(\n    points_gt: np.ndarray,\n    points_rec: np.ndarray,\n    threshold: float,\n    p_norm: int = 2,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"Compute reconstruction fscore.\n\n    See What Do Single-View 3D Reconstruction Networks Learn, Tatarchenko, 2019\n\n    Args:\n        points_gt: set of true points, expected shape (N,3)\n        points_rec: set of reconstructed points, expected shape (M,3)\n        threshold: distance threshold to count a point as correct\n        p_norm: which Minkowski p-norm is used for distance and nearest neighbor query\n        normalize: whether to divide distances by Euclidean extent of points_gt\n    Returns:\n        Harmonic mean of precision (thresholded accuracy) and recall (thresholded\n        completeness).\n    \"\"\"\n    recall = completeness_thresh(\n        points_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n    )\n    precision = accuracy_thresh(\n        points_gt, points_rec, threshold, p_norm=p_norm, normalize=normalize\n    )\n    if recall &lt; 1e-7 or precision &lt; 1e-7:\n        return 0\n    return 2 / (1 / recall + 1 / precision)\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d_sampling","title":"iou_3d_sampling","text":"<pre><code>iou_3d_sampling(\n    p1: np.ndarray,\n    r1: Rotation,\n    e1: np.ndarray,\n    p2: np.ndarray,\n    r2: Rotation,\n    e2: np.ndarray,\n    num_points: int = 10000,\n) -&gt; float\n</code></pre> <p>Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box.</p> PARAMETER  DESCRIPTION <code>p1</code> <p>Center position of first bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>r1</code> <p>Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e1</code> <p>Extents (i.e., side lengths) of first bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>p2</code> <p>Center position of second bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>r2</code> <p>Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e2</code> <p>Extents (i.e., side lengths) of second bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>num_points</code> <p>Number of points to sample in smaller bounding box.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Approximate intersection-over-union for the two oriented bounding boxes.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def iou_3d_sampling(\n    p1: np.ndarray,\n    r1: Rotation,\n    e1: np.ndarray,\n    p2: np.ndarray,\n    r2: Rotation,\n    e2: np.ndarray,\n    num_points: int = 10000,\n) -&gt; float:\n    \"\"\"Compute 3D IoU of oriented bounding boxes by sampling the smaller bounding box.\n\n    Args:\n        p1: Center position of first bounding box, shape (3,).\n        r1: Orientation of first bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e1: Extents (i.e., side lengths) of first bounding box, shape (3,).\n        p2: Center position of second bounding box, shape (3,).\n        r2: Orientation of second bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e2: Extents (i.e., side lengths) of second bounding box, shape (3,).\n        num_points: Number of points to sample in smaller bounding box.\n\n    Returns:\n        Approximate intersection-over-union for the two oriented bounding boxes.\n    \"\"\"\n    # sample smaller volume to estimate intersection\n    vol_1 = np.prod(e1)\n    vol_2 = np.prod(e2)\n    if vol_1 &lt; vol_2:\n        points_1_in_1 = e1 * np.random.rand(num_points, 3) - e1 / 2\n        points_1_in_w = r1.apply(points_1_in_1) + p1\n        points_1_in_2 = r2.inv().apply(points_1_in_w - p2)\n        ratio_1_in_2 = (\n            np.sum(\n                np.all(points_1_in_2 &lt; e2 / 2, axis=1)\n                * np.all(-e2 / 2 &lt; points_1_in_2, axis=1)\n            )\n            / num_points\n        )\n        intersection = ratio_1_in_2 * vol_1\n    else:\n        points_2_in_2 = e2 * np.random.rand(num_points, 3) - e2 / 2\n        points_2_in_w = r2.apply(points_2_in_2) + p2\n        points_2_in_1 = r1.inv().apply(points_2_in_w - p1)\n        ratio_2_in_1 = (\n            np.sum(\n                np.all(points_2_in_1 &lt; e1 / 2, axis=1)\n                * np.all(-e1 / 2 &lt; points_2_in_1, axis=1)\n            )\n            / num_points\n        )\n        intersection = ratio_2_in_1 * vol_2\n\n    union = vol_1 + vol_2 - intersection\n\n    return intersection / union\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.iou_3d","title":"iou_3d","text":"<pre><code>iou_3d(\n    p1: np.ndarray,\n    r1: Rotation,\n    e1: np.ndarray,\n    p2: np.ndarray,\n    r2: Rotation,\n    e2: np.ndarray,\n) -&gt; float\n</code></pre> <p>Compute 3D IoU of oriented bounding boxes analytically.</p> <p>Code partly based on https://github.com/google-research-datasets/Objectron/. Implementation uses half-space intersection instead of Sutherland-Hodgman algorithm.</p> PARAMETER  DESCRIPTION <code>p1</code> <p>Center position of first bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>r1</code> <p>Orientation of first bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e1</code> <p>Extents (i.e., side lengths) of first bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>p2</code> <p>Center position of second bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>r2</code> <p>Orientation of second bounding box. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>e2</code> <p>Extents (i.e., side lengths) of second bounding box, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Accurate intersection-over-union for the two oriented bounding boxes.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def iou_3d(\n    p1: np.ndarray,\n    r1: Rotation,\n    e1: np.ndarray,\n    p2: np.ndarray,\n    r2: Rotation,\n    e2: np.ndarray,\n) -&gt; float:\n    \"\"\"Compute 3D IoU of oriented bounding boxes analytically.\n\n    Code partly based on https://github.com/google-research-datasets/Objectron/.\n    Implementation uses half-space intersection instead of Sutherland-Hodgman algorithm.\n\n    Args:\n        p1: Center position of first bounding box, shape (3,).\n        r1: Orientation of first bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e1: Extents (i.e., side lengths) of first bounding box, shape (3,).\n        p2: Center position of second bounding box, shape (3,).\n        r2: Orientation of second bounding box.\n            This is the rotation that rotates points from bounding box to camera frame.\n        e2: Extents (i.e., side lengths) of second bounding box, shape (3,).\n\n    Returns:\n        Accurate intersection-over-union for the two oriented bounding boxes.\n    \"\"\"\n    # create halfspaces\n    halfspaces = np.zeros((12, 4))\n    halfspaces[0:3, 0:3] = r1.as_matrix().T\n    halfspaces[0:3, 3] = -halfspaces[0:3, 0:3] @ (r1.apply(e1 / 2) + p1)\n    halfspaces[3:6, 0:3] = -halfspaces[0:3, 0:3]\n    halfspaces[3:6, 3] = -halfspaces[3:6, 0:3] @ (r1.apply(-e1 / 2) + p1)\n    halfspaces[6:9, 0:3] = r2.as_matrix().T\n    halfspaces[6:9, 3] = -halfspaces[6:9, 0:3] @ (r2.apply(e2 / 2) + p2)\n    halfspaces[9:12, 0:3] = -halfspaces[6:9, 0:3]\n    halfspaces[9:12, 3] = -halfspaces[9:12, 0:3] @ (r2.apply(-e2 / 2) + p2)\n\n    # try to find point inside both bounding boxes\n    inside_point = _find_inside_point(p1, r1, e1, p2, r2, e2, halfspaces)\n    if inside_point is None:\n        return 0\n\n    # create halfspace intersection and compute IoU\n    hs = scipy.spatial.HalfspaceIntersection(halfspaces, inside_point)\n    ch = scipy.spatial.ConvexHull(hs.intersections)\n    intersection = ch.volume\n    vol_1 = np.prod(e1)\n    vol_2 = np.prod(e2)\n    union = vol_1 + vol_2 - intersection\n    return intersection / union\n</code></pre>"},{"location":"api_reference/metrics/#cpas_toolbox.metrics.correct_thresh","title":"correct_thresh","text":"<pre><code>correct_thresh(\n    position_gt: np.ndarray,\n    position_prediction: np.ndarray,\n    orientation_gt: Rotation,\n    orientation_prediction: Rotation,\n    extent_gt: Optional[np.ndarray] = None,\n    extent_prediction: Optional[np.ndarray] = None,\n    points_gt: Optional[np.ndarray] = None,\n    points_prediction: Optional[np.ndarray] = None,\n    position_threshold: Optional[float] = None,\n    degree_threshold: Optional[float] = None,\n    iou_3d_threshold: Optional[float] = None,\n    fscore_threshold: Optional[float] = None,\n    nad_threshold: Optional[float] = None,\n    rotational_symmetry_axis: Optional[int] = None,\n) -&gt; int\n</code></pre> <p>Classify a pose prediction as correct or incorrect.</p> PARAMETER  DESCRIPTION <code>position_gt</code> <p>Ground truth position, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>position_prediction</code> <p>Predicted position, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>position_threshold</code> <p>Position threshold in meters, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>orientation_gt</code> <p>Ground truth orientation. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>orientation_prediction</code> <p>Predicted orientation. This is the rotation that rotates points from bounding box to camera frame.</p> <p> TYPE: <code>Rotation</code> </p> <code>extent_gt</code> <p>Bounding box extents, shape (3,). Only used if IoU threshold specified.</p> <p> TYPE: <code>Optional[ndarray]</code> DEFAULT: <code>None</code> </p> <code>extent_prediction</code> <p>Bounding box extents, shape (3,). Only used if IoU threshold specified.</p> <p> TYPE: <code>Optional[ndarray]</code> DEFAULT: <code>None</code> </p> <code>point_gt</code> <p>Set of true points, shape (N,3).</p> <p> </p> <code>points_rec</code> <p>Set of reconstructed points, shape (M,3).</p> <p> </p> <code>degree_threshold</code> <p>Orientation threshold in degrees, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>iou_3d_threshold</code> <p>3D IoU threshold, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>nad_threshold</code> <p>Normalized average distance thresold, no threshold if None.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>rotational_symmetry_axis</code> <p>Specify axis along which rotation is ignored. If None, no axis is ignored. 0 for x-axis, 1 for y-axis, 2 for z-axis.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>1 if error is below all provided thresholds.  0 if error is above one provided</p> <code>int</code> <p>threshold.</p> Source code in <code>cpas_toolbox/metrics.py</code> <pre><code>def correct_thresh(\n    position_gt: np.ndarray,\n    position_prediction: np.ndarray,\n    orientation_gt: Rotation,\n    orientation_prediction: Rotation,\n    extent_gt: Optional[np.ndarray] = None,\n    extent_prediction: Optional[np.ndarray] = None,\n    points_gt: Optional[np.ndarray] = None,\n    points_prediction: Optional[np.ndarray] = None,\n    position_threshold: Optional[float] = None,\n    degree_threshold: Optional[float] = None,\n    iou_3d_threshold: Optional[float] = None,\n    fscore_threshold: Optional[float] = None,\n    nad_threshold: Optional[float] = None,\n    rotational_symmetry_axis: Optional[int] = None,\n) -&gt; int:\n    \"\"\"Classify a pose prediction as correct or incorrect.\n\n    Args:\n        position_gt: Ground truth position, shape (3,).\n        position_prediction: Predicted position, shape (3,).\n        position_threshold: Position threshold in meters, no threshold if None.\n        orientation_gt:\n            Ground truth orientation.\n            This is the rotation that rotates points from bounding box to camera frame.\n        orientation_prediction:\n            Predicted orientation.\n            This is the rotation that rotates points from bounding box to camera frame.\n        extent_gt:\n            Bounding box extents, shape (3,).\n            Only used if IoU threshold specified.\n        extent_prediction:\n            Bounding box extents, shape (3,).\n            Only used if IoU threshold specified.\n        point_gt: Set of true points, shape (N,3).\n        points_rec: Set of reconstructed points, shape (M,3).\n        degree_threshold: Orientation threshold in degrees, no threshold if None.\n        iou_3d_threshold: 3D IoU threshold, no threshold if None.\n        nad_threshold: Normalized average distance thresold, no threshold if None.\n        rotational_symmetry_axis:\n            Specify axis along which rotation is ignored. If None, no axis is ignored.\n            0 for x-axis, 1 for y-axis, 2 for z-axis.\n\n    Returns:\n        1 if error is below all provided thresholds.  0 if error is above one provided\n        threshold.\n    \"\"\"\n    if position_threshold is not None:\n        position_error = np.linalg.norm(position_gt - position_prediction)\n        if position_error &gt; position_threshold:\n            return 0\n    if degree_threshold is not None:\n        rad_threshold = degree_threshold * np.pi / 180.0\n        if rotational_symmetry_axis is not None:\n            p = np.array([0.0, 0.0, 0.0])\n            p[rotational_symmetry_axis] = 1.0\n            p1 = orientation_gt.apply(p)\n            p2 = orientation_prediction.apply(p)\n            rad_error = np.arccos(p1 @ p2)\n        else:\n            rad_error = (orientation_gt * orientation_prediction.inv()).magnitude()\n        if rad_error &gt; rad_threshold:\n            return 0\n    if iou_3d_threshold is not None:\n        if rotational_symmetry_axis is not None:\n            max_iou = 0\n\n            for r in np.linspace(0, np.pi, 100):\n                p = np.array([0.0, 0.0, 0.0])\n                p[rotational_symmetry_axis] = 1.0\n                p *= r\n                sym_rot = Rotation.from_rotvec(r)\n                iou = iou_3d(\n                    position_gt,\n                    orientation_gt,\n                    extent_gt,\n                    position_prediction,\n                    orientation_prediction * sym_rot,\n                    extent_prediction,\n                )\n                max_iou = max(iou, max_iou)\n            iou = max_iou\n        else:\n            iou = iou_3d(\n                position_gt,\n                orientation_gt,\n                extent_gt,\n                position_prediction,\n                orientation_prediction,\n                extent_prediction,\n            )\n        if iou &lt; iou_3d_threshold:\n            return 0\n    if fscore_threshold is not None:\n        # TODO make 0.01 a parameter\n        fscore = reconstruction_fscore(points_gt, points_prediction, 0.01)\n        if fscore &lt; fscore_threshold:\n            return 0\n    if nad_threshold is not None:\n        nad = normalized_average_distance(points_gt, points_prediction)\n        if nad &lt; nad_threshold:\n            return 0\n    return 1\n</code></pre>"},{"location":"api_reference/pointset_utils/","title":"pointset_utils.py","text":""},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils","title":"cpas_toolbox.pointset_utils","text":"<p>Utility functions to handle pointsets.</p>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.normalize_points","title":"normalize_points","text":"<pre><code>normalize_points(points: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize pointset to have zero mean.</p> <p>Normalization will be performed along second last dimension.</p> PARAMETER  DESCRIPTION <code>points</code> <p>The pointsets which will be normalized, shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.</p> <p> TYPE: <code>Tensor</code> </p> Return <p>normalized_points:     The normalized pointset, same shape as points. centroids:     The means of the pointclouds used to normalize points.     Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def normalize_points(points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize pointset to have zero mean.\n\n    Normalization will be performed along second last dimension.\n\n    Args:\n        points:\n            The pointsets which will be normalized,\n            shape (N, M, D) or shape (M, D), N pointsets with M points of dimension D.\n\n    Return:\n        normalized_points:\n            The normalized pointset, same shape as points.\n        centroids:\n            The means of the pointclouds used to normalize points.\n            Shape (N, D) or (D,), for (N, M, D) and (M, D) inputs, respectively.\n    \"\"\"\n    centroids = torch.mean(points, dim=-2, keepdim=True)\n    normalized_points = points - centroids\n    return normalized_points, centroids.squeeze()\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.depth_to_pointcloud","title":"depth_to_pointcloud","text":"<pre><code>depth_to_pointcloud(\n    depth_image: torch.Tensor,\n    camera: camera_utils.Camera,\n    normalize: bool = False,\n    mask: Optional[torch.Tensor] = None,\n    convention: str = \"opengl\",\n) -&gt; torch.Tensor\n</code></pre> <p>Convert depth image to pointcloud.</p> PARAMETER  DESCRIPTION <code>depth_image</code> <p>The depth image to convert to pointcloud, shape (H,W).</p> <p> TYPE: <code>Tensor</code> </p> <code>camera</code> <p>The camera used to lift the points.</p> <p> TYPE: <code>Camera</code> </p> <code>normalize</code> <p>Whether to normalize the pointcloud with 0 centroid.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mask</code> <p>Only points with mask != 0 will be added to pointcloud. No masking will be performed if None.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>convention</code> <p>The camera frame convention to use. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward</p> <p> TYPE: <code>str</code> DEFAULT: <code>'opengl'</code> </p> <p>Returns:     The pointcloud in the camera frame, in OpenGL convention, shape (N,3).</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def depth_to_pointcloud(\n    depth_image: torch.Tensor,\n    camera: camera_utils.Camera,\n    normalize: bool = False,\n    mask: Optional[torch.Tensor] = None,\n    convention: str = \"opengl\",\n) -&gt; torch.Tensor:\n    \"\"\"Convert depth image to pointcloud.\n\n    Args:\n        depth_image: The depth image to convert to pointcloud, shape (H,W).\n        camera: The camera used to lift the points.\n        normalize: Whether to normalize the pointcloud with 0 centroid.\n        mask:\n            Only points with mask != 0 will be added to pointcloud.\n            No masking will be performed if None.\n        convention:\n            The camera frame convention to use. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n    Returns:\n        The pointcloud in the camera frame, in OpenGL convention, shape (N,3).\n    \"\"\"\n    fx, fy, cx, cy, _ = camera.get_pinhole_camera_parameters(0.0)\n\n    if mask is None:\n        indices = torch.nonzero(depth_image, as_tuple=True)\n    else:\n        indices = torch.nonzero(depth_image * mask, as_tuple=True)\n    depth_values = depth_image[indices]\n    points = torch.cat(\n        (\n            indices[1][:, None].float(),\n            indices[0][:, None].float(),\n            depth_values[:, None],\n        ),\n        dim=1,\n    )\n\n    if convention == \"opengl\":\n        final_points = torch.empty_like(points)\n        final_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\n        final_points[:, 1] = -(points[:, 1] - cy) * points[:, 2] / fy\n        final_points[:, 2] = -points[:, 2]\n    elif convention == \"opencv\":\n        final_points = torch.empty_like(points)\n        final_points[:, 0] = (points[:, 0] - cx) * points[:, 2] / fx\n        final_points[:, 1] = (points[:, 1] - cy) * points[:, 2] / fy\n        final_points[:, 2] = points[:, 2]\n    else:\n        raise ValueError(f\"Unsupported camera convention {convention}.\")\n\n    if normalize:\n        final_points, _ = normalize_points(final_points)\n\n    return final_points\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_transform_camera_convention","title":"change_transform_camera_convention","text":"<pre><code>change_transform_camera_convention(\n    in_transform: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; torch.Tensor\n</code></pre> <p>Change the camera convention for a frame A -&gt; camera frame transform.</p> PARAMETER  DESCRIPTION <code>in_transform</code> <p>Transformtion matrix(es) from coordinate frame A to in_convention camera frame.  Shape (...,4,4).</p> <p> TYPE: <code>Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Transformtion matrix(es) from coordinate frame A to out_convention camera frame.</p> <code>Tensor</code> <p>Same shape as in_transform.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_transform_camera_convention(\n    in_transform: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; torch.Tensor:\n    \"\"\"Change the camera convention for a frame A -&gt; camera frame transform.\n\n    Args:\n        in_transform:\n            Transformtion matrix(es) from coordinate frame A to in_convention camera\n            frame.  Shape (...,4,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n\n    Returns:\n        Transformtion matrix(es) from coordinate frame A to out_convention camera frame.\n        Same shape as in_transform.\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_transform\n    else:\n        gl2cv_transform = torch.diag(\n            in_transform.new_tensor([1.0, -1.0, -1.0, 1.0])\n        )  # == cv2gl_transform\n        return gl2cv_transform @ in_transform\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_position_camera_convention","title":"change_position_camera_convention","text":"<pre><code>change_position_camera_convention(\n    in_position: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; tuple\n</code></pre> <p>Change the camera convention for a position in a camera frame.</p> PARAMETER  DESCRIPTION <code>in_position</code> <p>Position(s) of coordinate frame A in in_convention camera frame. Shape (...,3).</p> <p> TYPE: <code>Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_position. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_position_camera_convention(\n    in_position: torch.Tensor,\n    in_convention: str,\n    out_convention: str,\n) -&gt; tuple:\n    \"\"\"Change the camera convention for a position in a camera frame.\n\n    Args:\n        in_position:\n            Position(s) of coordinate frame A in in_convention camera frame.\n            Shape (...,3).\n        in_convention:\n            Camera convention for the in_position. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n\n    Returns:\n        Position(s) of coordinate frame A in out_convention camera frame. Shape (...,3).\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_position\n    else:\n        gl2cv = in_position.new_tensor([1.0, -1.0, -1.0])  # == cv2gl\n        return gl2cv * in_position\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.change_orientation_camera_convention","title":"change_orientation_camera_convention","text":"<pre><code>change_orientation_camera_convention(\n    in_orientation_q: torch.Tensor, in_convention: str, out_convention: str\n) -&gt; tuple\n</code></pre> <p>Change the camera convention for an orientation in a camera frame.</p> <p>Orientation is represented as a quaternion, that rotates points from a coordinate frame A to a camera frame (if those frames had the same origin).</p> PARAMETER  DESCRIPTION <code>in_orientation_q</code> <p>Quaternion(s) which transforms from coordinate frame A to in_convention camera frame. Scalar-last convention. Shape (...,4).</p> <p> TYPE: <code>Tensor</code> </p> <code>in_convention</code> <p>Camera convention for the in_transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> <code>out_convention</code> <p>Camera convention for the returned transform. One of \"opengl\", \"opencv\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Quaternion(s) which transforms from coordinate frame A to in_convention camera</p> <code>tuple</code> <p>frame. Scalar-last convention. Same shape as in_orientation_q.</p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def change_orientation_camera_convention(\n    in_orientation_q: torch.Tensor,\n    in_convention: str,\n    out_convention: str,\n) -&gt; tuple:\n    \"\"\"Change the camera convention for an orientation in a camera frame.\n\n    Orientation is represented as a quaternion, that rotates points from a\n    coordinate frame A to a camera frame (if those frames had the same origin).\n\n    Args:\n        in_orientation_q:\n            Quaternion(s) which transforms from coordinate frame A to in_convention\n            camera frame. Scalar-last convention. Shape (...,4).\n        in_convention:\n            Camera convention for the in_transform. One of \"opengl\", \"opencv\".\n        out_convention:\n            Camera convention for the returned transform. One of \"opengl\", \"opencv\".\n\n    Returns:\n        Quaternion(s) which transforms from coordinate frame A to in_convention camera\n        frame. Scalar-last convention. Same shape as in_orientation_q.\n    \"\"\"\n    # check whether valild convention was provided\n    if in_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"In camera convention {in_convention} not supported.\")\n    if out_convention not in [\"opengl\", \"opencv\"]:\n        raise ValueError(f\"Out camera convention {in_convention} not supported.\")\n\n    if in_convention == out_convention:\n        return in_orientation_q\n    else:\n        # rotate 180deg around x direction\n        gl2cv_q = in_orientation_q.new_tensor([1.0, 0, 0, 0])  # == cv2gl\n        return quaternion_utils.quaternion_multiply(gl2cv_q, in_orientation_q)\n</code></pre>"},{"location":"api_reference/pointset_utils/#cpas_toolbox.pointset_utils.visualize_pointset","title":"visualize_pointset","text":"<pre><code>visualize_pointset(pointset: torch.Tensor, max_points: int = 1000) -&gt; None\n</code></pre> <p>Visualize pointset as 3D scatter plot.</p> PARAMETER  DESCRIPTION <code>pointset</code> <p>The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.</p> <p> TYPE: <code>Tensor</code> </p> <code>max_points</code> <p>Maximum number of points. If N&gt;max_points only a random subset will be shown.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>cpas_toolbox/pointset_utils.py</code> <pre><code>def visualize_pointset(pointset: torch.Tensor, max_points: int = 1000) -&gt; None:\n    \"\"\"Visualize pointset as 3D scatter plot.\n\n    Args:\n        pointset:\n            The pointset to visualize. Either shape (N,3), xyz, or shape (N,6), xyzrgb.\n        max_points:\n            Maximum number of points.\n            If N&gt;max_points only a random subset will be shown.\n    \"\"\"\n    pointset_np = pointset.cpu().detach().numpy()\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n\n    if len(pointset_np) &gt; max_points:\n        indices = np.random.choice(len(pointset_np), replace=False, size=max_points)\n        pointset_np = pointset_np[indices]\n\n    if pointset_np.shape[1] == 6:\n        colors = pointset_np[:, 3:]\n    else:\n        colors = None\n\n    ax.scatter(pointset_np[:, 0], pointset_np[:, 1], pointset_np[:, 2], c=colors)\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_zlabel(\"z\")\n\n    ax.set_box_aspect(pointset_np.max(axis=0) - pointset_np.min(axis=0))\n    plt.show()\n</code></pre>"},{"location":"api_reference/quaternion_utils/","title":"quaternion_utils.py","text":""},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils","title":"cpas_toolbox.quaternion_utils","text":"<p>Functions to handle transformations with quaternions.</p> <p>Inspired by PyTorch3D, but using scalar-last convention and not enforcing scalar &gt; 0. https://github.com/facebookresearch/pytorch3d</p>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_multiply","title":"quaternion_multiply","text":"<pre><code>quaternion_multiply(\n    quaternions_1: torch.Tensor, quaternions_2: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Multiply two quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> PARAMETER  DESCRIPTION <code>quaternions_1</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>Tensor</code> </p> <code>quaternions_2</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>Tensor</code> </p> <p>Returns:     Composition of passed quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_multiply(\n    quaternions_1: torch.Tensor, quaternions_2: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Multiply two quaternions representing rotations.\n\n    Normal broadcasting rules apply.\n\n    Args:\n        quaternions_1:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        quaternions_2:\n            normalized quaternions of shape (..., 4), scalar-last convention\n    Returns:\n        Composition of passed quaternions.\n    \"\"\"\n    ax, ay, az, aw = torch.unbind(quaternions_1, -1)\n    bx, by, bz, bw = torch.unbind(quaternions_2, -1)\n    ox = aw * bx + ax * bw + ay * bz - az * by\n    oy = aw * by - ax * bz + ay * bw + az * bx\n    oz = aw * bz + ax * by - ay * bx + az * bw\n    ow = aw * bw - ax * bx - ay * by - az * bz\n    return torch.stack((ox, oy, oz, ow), -1)\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_apply","title":"quaternion_apply","text":"<pre><code>quaternion_apply(\n    quaternions: torch.Tensor, points: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Rotate points by quaternions representing rotations.</p> <p>Normal broadcasting rules apply.</p> PARAMETER  DESCRIPTION <code>quaternions</code> <p>normalized quaternions of shape (..., 4), scalar-last convention</p> <p> TYPE: <code>Tensor</code> </p> <code>points</code> <p>points of shape (..., 3)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Points rotated by the rotations representing quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_apply(quaternions: torch.Tensor, points: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Rotate points by quaternions representing rotations.\n\n    Normal broadcasting rules apply.\n\n    Args:\n        quaternions:\n            normalized quaternions of shape (..., 4), scalar-last convention\n        points:\n            points of shape (..., 3)\n\n    Returns:\n        Points rotated by the rotations representing quaternions.\n    \"\"\"\n    points_as_quaternions = points.new_zeros(points.shape[:-1] + (4,))\n    points_as_quaternions[..., :-1] = points\n    return quaternion_multiply(\n        quaternion_multiply(quaternions, points_as_quaternions),\n        quaternion_invert(quaternions),\n    )[..., :-1]\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.quaternion_invert","title":"quaternion_invert","text":"<pre><code>quaternion_invert(quaternions: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Invert quaternions representing orientations.</p> PARAMETER  DESCRIPTION <code>quaternions</code> <p>The quaternions to invert, shape (..., 4), scalar-last convention.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Inverted quaternions, same shape as quaternions.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def quaternion_invert(quaternions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Invert quaternions representing orientations.\n\n    Args:\n        quaternions:\n            The quaternions to invert, shape (..., 4), scalar-last convention.\n\n    Returns:\n        Inverted quaternions, same shape as quaternions.\n    \"\"\"\n    return quaternions * quaternions.new_tensor([-1, -1, -1, 1])\n</code></pre>"},{"location":"api_reference/quaternion_utils/#cpas_toolbox.quaternion_utils.geodesic_distance","title":"geodesic_distance","text":"<pre><code>geodesic_distance(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Compute geodesic distances between quaternions.</p> PARAMETER  DESCRIPTION <code>q1</code> <p>First set of quaterions, shape (N,4).</p> <p> TYPE: <code>Tensor</code> </p> <code>q2</code> <p>Second set of quaternions, shape (N,4).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Mean distance between the quaternions, scalar.</p> Source code in <code>cpas_toolbox/quaternion_utils.py</code> <pre><code>def geodesic_distance(q1: torch.Tensor, q2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute geodesic distances between quaternions.\n\n    Args:\n        q1: First set of quaterions, shape (N,4).\n        q2: Second set of quaternions, shape (N,4).\n\n    Returns:\n        Mean distance between the quaternions, scalar.\n    \"\"\"\n    abs_q1q2 = torch.clip(torch.abs(torch.sum(q1 * q2, dim=1)), 0, 1)\n    geodesic_distances = 2 * torch.acos(abs_q1q2)\n    return geodesic_distances\n</code></pre>"},{"location":"api_reference/utils/","title":"utils.py","text":""},{"location":"api_reference/utils/#cpas_toolbox.utils","title":"cpas_toolbox.utils","text":"<p>This module provides miscellaneous utility functions.</p>"},{"location":"api_reference/utils/#cpas_toolbox.utils.str_to_object","title":"str_to_object","text":"<pre><code>str_to_object(name: str) -&gt; Any\n</code></pre> <p>Try to find object with a given name.</p> <p>First scope of calling function is checked for the name, then current environment (in which case name has to be a fully qualified name). In the second case, the object is imported if found.</p> PARAMETER  DESCRIPTION <code>name</code> <p>Name of the object to resolve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The object which the provided name refers to. None if no object was found.</p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def str_to_object(name: str) -&gt; Any:\n    \"\"\"Try to find object with a given name.\n\n    First scope of calling function is checked for the name, then current environment\n    (in which case name has to be a fully qualified name). In the second case, the\n    object is imported if found.\n\n    Args:\n        name: Name of the object to resolve.\n\n    Returns:\n        The object which the provided name refers to. None if no object was found.\n    \"\"\"\n    # check callers local variables\n    caller_locals = inspect.currentframe().f_back.f_locals\n    if name in caller_locals:\n        return caller_locals[name]\n\n    # check callers global variables (i.e., imported modules etc.)\n    caller_globals = inspect.currentframe().f_back.f_globals\n    if name in caller_globals:\n        return caller_globals[name]\n\n    # check environment\n    return locate(name)\n</code></pre>"},{"location":"api_reference/utils/#cpas_toolbox.utils.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(path: str, search_paths: Optional[List[str]] = None) -&gt; str\n</code></pre> <p>Resolves a path to a full absolute path based on search_paths.</p> <p>This function considers paths of 5 different cases     /... -&gt; absolute path, nothing todo     ~/... -&gt; home dir, expand user     ./... -&gt; relative to current directory     ../... -&gt; relative to current parent directory     ... -&gt; relative to search paths</p> <p>Current directory is not implicitly included in search paths and has to be added with \".\" if desired. Search paths are handled in first to last order and considered correct if file or directory exists.</p> <p>Returns original path, if file does not exist.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The path to resolve.</p> <p> TYPE: <code>str</code> </p> <code>search_paths</code> <p>List of search paths to prepend relative paths which are not explicitly relative to current directory. If None, no search paths are assumed.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def resolve_path(path: str, search_paths: Optional[List[str]] = None) -&gt; str:\n    \"\"\"Resolves a path to a full absolute path based on search_paths.\n\n    This function considers paths of 5 different cases\n        /... -&gt; absolute path, nothing todo\n        ~/... -&gt; home dir, expand user\n        ./... -&gt; relative to current directory\n        ../... -&gt; relative to current parent directory\n        ... -&gt; relative to search paths\n\n    Current directory is not implicitly included in search paths and has to be added\n    with \".\" if desired. Search paths are handled in first to last order and considered\n    correct if file or directory exists.\n\n    Returns original path, if file does not exist.\n\n    Args:\n        path: The path to resolve.\n        search_paths:\n            List of search paths to prepend relative paths which are not explicitly\n            relative to current directory.\n            If None, no search paths are assumed.\n    \"\"\"\n    if search_paths is None:\n        search_paths = []\n\n    if os.path.isabs(path):\n        return path\n\n    parts = path.split(os.sep)\n\n    if parts[0] in [\".\", \"..\"]:\n        return os.path.abspath(path)\n    elif parts[0] == \"~\":\n        return os.path.expanduser(path)\n\n    for search_path in search_paths:\n        resolved_path = os.path.expanduser(os.path.join(search_path, path))\n        if os.path.exists(resolved_path):\n            return os.path.abspath(resolved_path)\n\n    return path\n</code></pre>"},{"location":"api_reference/utils/#cpas_toolbox.utils.download","title":"download","text":"<pre><code>download(url: str, download_path: str) -&gt; str\n</code></pre> <p>Download file from URL to a specified path.</p> Source code in <code>cpas_toolbox/utils.py</code> <pre><code>def download(url: str, download_path: str) -&gt; str:\n    \"\"\"Download file from URL to a specified path.\"\"\"\n    block_size = 100\n    if \"drive.google.com\" in url:\n        gdown.download(url, download_path)\n    else:\n        # adapted from https://stackoverflow.com/a/37573701\n        response = requests.get(url, stream=True)\n        total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n        progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n        with open(download_path, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n            print(\"ERROR, download failed\")\n            exit()\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/","title":"asmnet.py","text":""},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet","title":"cpas_toolbox.cpas_methods.asmnet","text":"<p>This module defines ASMNet interface.</p> <p>Method is described in ASM-Net: Category-level Pose and Shape, Akizuki, 2021</p> <p>Implementation based on https://github.com/sakizuki/asm-net</p>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet","title":"ASMNet","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for ASMNet.</p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>class ASMNet(CPASMethod):\n    \"\"\"Wrapper class for ASMNet.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for ASMNet.\n\n        Attributes:\n            model: Path to model.\n            device: Device string for the model.\n            models_dir:\n                Path to directory containing model parameters.\n                Must contain the following directory structure:\n                    {models_dir}/{category_0}/model.pth\n                    ...\n            asm_params_dir:\n                Path to direactory containing ASM parameters.\n                Must contain the following directory structure:\n                    {asm_params_directory}/{category_0}/train/info.npz\n                    ...\n            weights_url:\n                URL to download model and ASM params from if they do not exist yet.\n            categories:\n                List of categories. Each category requires corresponding directory with\n                model.pth and info.npz. See models_dir and asm_params_dir.\n            num_points: Number of input points.\n            deformation_dimension: Number of deformation parameters.\n            use_mean_shape:\n                Whether the mean shape (0) or the predicted shape deformation should\n                be used.\n            use_icp: Whether to use ICP to refine the pose.\n        \"\"\"\n\n        models_dir: str\n        asm_params_dir: str\n        weights_url: str\n        device: str\n        categories: List[str]\n        num_points: int\n        deformation_dimension: int\n        use_mean_shape: bool\n        use_icp: bool\n\n    default_config: Config = {\n        \"model_params_dir\": None,\n        \"asm_params_dir\": None,\n        \"weights_url\": None,\n        \"device\": \"cuda\",\n        \"categories\": [],\n        \"num_points\": 800,\n        \"deformation_dimension\": 3,\n        \"use_mean_shape\": False,\n        \"use_icp\": True,\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load ASMNet model.\n\n        Args:\n            config: ASMNet configuration. See ASMNet.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=ASMNet.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._weights_dir_path = utils.resolve_path(config[\"models_dir\"])\n        self._asm_params_dir_path = utils.resolve_path(config[\"asm_params_dir\"])\n        self._weights_url = config[\"weights_url\"]\n        self._check_paths()\n        synset_names = [\"placeholder\"] + config[\"categories\"]  # first will be ignored\n        self._asmds = asmnet.cr6d_utils.load_asmds(\n            self._asm_params_dir_path, synset_names\n        )\n        self._models = asmnet.cr6d_utils.load_models_release(\n            self._weights_dir_path,\n            synset_names,\n            config[\"deformation_dimension\"],\n            config[\"num_points\"],\n            self._device,\n        )\n        self._num_points = config[\"num_points\"]\n        self._use_mean_shape = config[\"use_mean_shape\"]\n        self._use_icp = config[\"use_icp\"]\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._weights_dir_path) or not os.path.exists(\n            self._asm_params_dir_path\n        ):\n            print(\"ASM-Net model weights not found, do you want to download to \")\n            print(\"  \", self._weights_dir_path)\n            print(\"  \", self._asm_params_dir_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"ASM-Net model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        download_dir_path = tempfile.mkdtemp()\n        zip_file_path = os.path.join(download_dir_path, \"asmnetweights.zip\")\n        utils.download(\n            self._weights_url,\n            zip_file_path,\n        )\n        zip_file = zipfile.ZipFile(zip_file_path)\n        zip_file.extractall(download_dir_path)\n        zip_file.close()\n        os.remove(zip_file_path)\n\n        if not os.path.exists(self._asm_params_dir_path):\n            os.makedirs(self._asm_params_dir_path, exist_ok=True)\n            source_dir_path = os.path.join(download_dir_path, \"params\", \"asm_params\")\n            file_names = os.listdir(source_dir_path)\n            for fn in file_names:\n                shutil.move(\n                    os.path.join(source_dir_path, fn), self._asm_params_dir_path\n                )\n\n        if not os.path.exists(self._weights_dir_path):\n            os.makedirs(self._weights_dir_path, exist_ok=True)\n            source_dir_path = os.path.join(download_dir_path, \"params\", \"weights\")\n            file_names = os.listdir(source_dir_path)\n            for fn in file_names:\n                shutil.move(os.path.join(source_dir_path, fn), self._weights_dir_path)\n\n        shutil.rmtree(os.path.join(download_dir_path, \"params\"))\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See CPASMethod.inference.\n\n        Based on asmnet.ASM_Net.test_net_nocs2019_release\n        \"\"\"\n        # torch -&gt; numpy\n        color_image = np.uint8(\n            (color_image * 255).numpy()\n        )  # (H, W, 3), uint8, 0-255, RGB\n        depth_image = np.uint16((depth_image * 1000).numpy())  # (H, W), uint16, mm\n        instance_mask = instance_mask.numpy()\n\n        # Noise reduction + pointcloud generation\n        masked_depth = depth_image * instance_mask\n        masked_depth = asmnet.common3Dfunc.image_statistical_outlier_removal(\n            masked_depth, factor=2.0\n        )\n        pcd_obj = asmnet.cr6d_utils.get_pcd_from_rgbd(\n            color_image.copy(),\n            masked_depth.copy(),\n            self._camera.get_o3d_pinhole_camera_parameters().intrinsic,\n        )\n        [pcd_obj, _] = pcd_obj.remove_statistical_outlier(100, 2.0)\n        pcd_in = copy.deepcopy(pcd_obj)\n        pcd_c, offset = asmnet.common3Dfunc.centering(pcd_in)\n        pcd_n, scale = asmnet.common3Dfunc.size_normalization(pcd_c)\n\n        # o3d -&gt; torch\n        np_pcd = np.array(pcd_n.points)\n        np_input = asmnet.cr6d_utils.random_sample(np_pcd, self._num_points)\n        np_input = np_input.astype(np.float32)\n        input_points = torch.from_numpy(np_input)\n\n        # prepare input shape\n        input_points = input_points.unsqueeze(0).transpose(2, 1).to(self._device)\n\n        # evaluate model\n        with torch.no_grad():\n            dparam_pred, q_pred = self._models[category_str](input_points)\n            dparam_pred = dparam_pred.cpu().numpy().squeeze()\n            pred_rot = asmnet.cr6d_utils.quaternion2rotationPT(q_pred)\n            pred_rot = pred_rot.cpu().numpy().squeeze()\n            pred_dp_param = dparam_pred[:-1]  # deformation params\n            pred_scaling_param = dparam_pred[-1]  # scale\n\n            # get shape prediction\n            pcd_pred = None\n            if self._use_mean_shape:\n                pcd_pred = self._asmds[category_str].deformation([0])\n            else:\n                pcd_pred = self._asmds[category_str].deformation(pred_dp_param)\n                pcd_pred = pcd_pred.remove_statistical_outlier(20, 1.0)[0]\n                pcd_pred.scale(pred_scaling_param, (0.0, 0.0, 0.0))\n\n            metric_pcd = copy.deepcopy(pcd_pred)\n            metric_pcd.scale(scale, (0.0, 0.0, 0.0))  # undo scale normalization\n\n            # ICP\n            pcd_pred_posed = copy.deepcopy(metric_pcd)\n            pcd_pred_posed.rotate(pred_rot)  # rotate metric reconstruction\n            pcd_pred_posed.translate(offset)  # move to center of cropped pcd\n            pred_rt = np.identity(4)\n            pred_rt[:3, :3] = pred_rot\n            if self._use_icp:\n                pcd_pred_posed_ds = pcd_pred_posed.voxel_down_sample(0.005)\n                if len(pcd_pred_posed_ds.points) &gt; 3:\n                    # remove hidden points\n                    pcd_pred_posed_visible = asmnet.common3Dfunc.applyHPR(\n                        pcd_pred_posed_ds\n                    )\n                    pcd_in = pcd_in.voxel_down_sample(0.005)\n                    reg_result = o3d.pipelines.registration.registration_icp(\n                        pcd_pred_posed_visible, pcd_in, max_correspondence_distance=0.02\n                    )\n                    pcd_pred_posed = copy.deepcopy(pcd_pred_posed_ds).transform(\n                        reg_result.transformation\n                    )\n                    pred_rt = np.dot(reg_result.transformation, pred_rt)\n                else:\n                    print(\n                        \"ASM-Net Warning: Couldn't perform ICP, too few points after\"\n                        \"voxel down sampling\"\n                    )\n\n            # center position\n            maxb = pcd_pred_posed.get_max_bound()  # bbox max\n            minb = pcd_pred_posed.get_min_bound()  # bbox min\n            center = (maxb - minb) / 2 + minb  # bbox center\n            pred_rt[:3, 3] = center.copy()\n\n            position = torch.Tensor(pred_rt[:3, 3])\n            orientation_q = torch.Tensor(\n                Rotation.from_matrix(pred_rt[:3, :3]).as_quat()\n            )\n            reconstructed_points = torch.from_numpy(np.asarray(metric_pcd.points))\n\n            # NOCS Object -&gt; ShapeNet Object convention\n            obj_fix = torch.tensor(\n                [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n            )  # CASS object to ShapeNet object\n            orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n            reconstructed_points = quaternion_utils.quaternion_apply(\n                quaternion_utils.quaternion_invert(obj_fix),\n                reconstructed_points,\n            )\n            extents, _ = reconstructed_points.abs().max(dim=0)\n            extents *= 2.0\n\n        return {\n            \"position\": position,\n            \"orientation\": orientation_q,\n            \"extents\": extents,\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for ASMNet.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> <code>models_dir</code> <p>Path to directory containing model parameters. Must contain the following directory structure:     {models_dir}/{category_0}/model.pth     ...</p> <p> TYPE: <code>str</code> </p> <code>asm_params_dir</code> <p>Path to direactory containing ASM parameters. Must contain the following directory structure:     {asm_params_directory}/{category_0}/train/info.npz     ...</p> <p> TYPE: <code>str</code> </p> <code>weights_url</code> <p>URL to download model and ASM params from if they do not exist yet.</p> <p> TYPE: <code>str</code> </p> <code>categories</code> <p>List of categories. Each category requires corresponding directory with model.pth and info.npz. See models_dir and asm_params_dir.</p> <p> TYPE: <code>List[str]</code> </p> <code>num_points</code> <p>Number of input points.</p> <p> TYPE: <code>int</code> </p> <code>deformation_dimension</code> <p>Number of deformation parameters.</p> <p> TYPE: <code>int</code> </p> <code>use_mean_shape</code> <p>Whether the mean shape (0) or the predicted shape deformation should be used.</p> <p> TYPE: <code>bool</code> </p> <code>use_icp</code> <p>Whether to use ICP to refine the pose.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for ASMNet.\n\n    Attributes:\n        model: Path to model.\n        device: Device string for the model.\n        models_dir:\n            Path to directory containing model parameters.\n            Must contain the following directory structure:\n                {models_dir}/{category_0}/model.pth\n                ...\n        asm_params_dir:\n            Path to direactory containing ASM parameters.\n            Must contain the following directory structure:\n                {asm_params_directory}/{category_0}/train/info.npz\n                ...\n        weights_url:\n            URL to download model and ASM params from if they do not exist yet.\n        categories:\n            List of categories. Each category requires corresponding directory with\n            model.pth and info.npz. See models_dir and asm_params_dir.\n        num_points: Number of input points.\n        deformation_dimension: Number of deformation parameters.\n        use_mean_shape:\n            Whether the mean shape (0) or the predicted shape deformation should\n            be used.\n        use_icp: Whether to use ICP to refine the pose.\n    \"\"\"\n\n    models_dir: str\n    asm_params_dir: str\n    weights_url: str\n    device: str\n    categories: List[str]\n    num_points: int\n    deformation_dimension: int\n    use_mean_shape: bool\n    use_icp: bool\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load ASMNet model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>ASMNet configuration. See ASMNet.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load ASMNet model.\n\n    Args:\n        config: ASMNet configuration. See ASMNet.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=ASMNet.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/asmnet/#cpas_toolbox.cpas_methods.asmnet.ASMNet.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> <p>Based on asmnet.ASM_Net.test_net_nocs2019_release</p> Source code in <code>cpas_toolbox/cpas_methods/asmnet.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See CPASMethod.inference.\n\n    Based on asmnet.ASM_Net.test_net_nocs2019_release\n    \"\"\"\n    # torch -&gt; numpy\n    color_image = np.uint8(\n        (color_image * 255).numpy()\n    )  # (H, W, 3), uint8, 0-255, RGB\n    depth_image = np.uint16((depth_image * 1000).numpy())  # (H, W), uint16, mm\n    instance_mask = instance_mask.numpy()\n\n    # Noise reduction + pointcloud generation\n    masked_depth = depth_image * instance_mask\n    masked_depth = asmnet.common3Dfunc.image_statistical_outlier_removal(\n        masked_depth, factor=2.0\n    )\n    pcd_obj = asmnet.cr6d_utils.get_pcd_from_rgbd(\n        color_image.copy(),\n        masked_depth.copy(),\n        self._camera.get_o3d_pinhole_camera_parameters().intrinsic,\n    )\n    [pcd_obj, _] = pcd_obj.remove_statistical_outlier(100, 2.0)\n    pcd_in = copy.deepcopy(pcd_obj)\n    pcd_c, offset = asmnet.common3Dfunc.centering(pcd_in)\n    pcd_n, scale = asmnet.common3Dfunc.size_normalization(pcd_c)\n\n    # o3d -&gt; torch\n    np_pcd = np.array(pcd_n.points)\n    np_input = asmnet.cr6d_utils.random_sample(np_pcd, self._num_points)\n    np_input = np_input.astype(np.float32)\n    input_points = torch.from_numpy(np_input)\n\n    # prepare input shape\n    input_points = input_points.unsqueeze(0).transpose(2, 1).to(self._device)\n\n    # evaluate model\n    with torch.no_grad():\n        dparam_pred, q_pred = self._models[category_str](input_points)\n        dparam_pred = dparam_pred.cpu().numpy().squeeze()\n        pred_rot = asmnet.cr6d_utils.quaternion2rotationPT(q_pred)\n        pred_rot = pred_rot.cpu().numpy().squeeze()\n        pred_dp_param = dparam_pred[:-1]  # deformation params\n        pred_scaling_param = dparam_pred[-1]  # scale\n\n        # get shape prediction\n        pcd_pred = None\n        if self._use_mean_shape:\n            pcd_pred = self._asmds[category_str].deformation([0])\n        else:\n            pcd_pred = self._asmds[category_str].deformation(pred_dp_param)\n            pcd_pred = pcd_pred.remove_statistical_outlier(20, 1.0)[0]\n            pcd_pred.scale(pred_scaling_param, (0.0, 0.0, 0.0))\n\n        metric_pcd = copy.deepcopy(pcd_pred)\n        metric_pcd.scale(scale, (0.0, 0.0, 0.0))  # undo scale normalization\n\n        # ICP\n        pcd_pred_posed = copy.deepcopy(metric_pcd)\n        pcd_pred_posed.rotate(pred_rot)  # rotate metric reconstruction\n        pcd_pred_posed.translate(offset)  # move to center of cropped pcd\n        pred_rt = np.identity(4)\n        pred_rt[:3, :3] = pred_rot\n        if self._use_icp:\n            pcd_pred_posed_ds = pcd_pred_posed.voxel_down_sample(0.005)\n            if len(pcd_pred_posed_ds.points) &gt; 3:\n                # remove hidden points\n                pcd_pred_posed_visible = asmnet.common3Dfunc.applyHPR(\n                    pcd_pred_posed_ds\n                )\n                pcd_in = pcd_in.voxel_down_sample(0.005)\n                reg_result = o3d.pipelines.registration.registration_icp(\n                    pcd_pred_posed_visible, pcd_in, max_correspondence_distance=0.02\n                )\n                pcd_pred_posed = copy.deepcopy(pcd_pred_posed_ds).transform(\n                    reg_result.transformation\n                )\n                pred_rt = np.dot(reg_result.transformation, pred_rt)\n            else:\n                print(\n                    \"ASM-Net Warning: Couldn't perform ICP, too few points after\"\n                    \"voxel down sampling\"\n                )\n\n        # center position\n        maxb = pcd_pred_posed.get_max_bound()  # bbox max\n        minb = pcd_pred_posed.get_min_bound()  # bbox min\n        center = (maxb - minb) / 2 + minb  # bbox center\n        pred_rt[:3, 3] = center.copy()\n\n        position = torch.Tensor(pred_rt[:3, 3])\n        orientation_q = torch.Tensor(\n            Rotation.from_matrix(pred_rt[:3, :3]).as_quat()\n        )\n        reconstructed_points = torch.from_numpy(np.asarray(metric_pcd.points))\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor(\n            [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n        )  # CASS object to ShapeNet object\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n    return {\n        \"position\": position,\n        \"orientation\": orientation_q,\n        \"extents\": extents,\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/","title":"cass.py","text":""},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass","title":"cpas_toolbox.cpas_methods.cass","text":"<p>This module defines CASS interface.</p> <p>Method is described in Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation, Chen, 2020</p> <p>Implementation based on https://github.com/densechen/CASS</p>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS","title":"CASS","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for CASS.</p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>class CASS(CPASMethod):\n    \"\"\"Wrapper class for CASS.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for CASS.\n\n        Attributes:\n            model: Path to model.\n            device: Device string for the model.\n        \"\"\"\n\n        model: str\n\n    default_config: Config = {\n        \"model\": None,\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load CASS model.\n\n        Args:\n            config: CASS configuration. See CASS.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=CASS.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_path = utils.resolve_path(config[\"model\"])\n        self._check_paths()\n        self._cass = cass.CASS(\n            num_points=config[\"num_points\"], num_obj=config[\"num_objects\"]\n        )\n        self._num_points = config[\"num_points\"]\n        self._cass.load_state_dict(\n            torch.load(self._model_path, map_location=self._device)\n        )\n        self._cass.to(self._device)\n        self._cass.eval()\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_path):\n            print(\"CASS model weights not found, do you want to download to \")\n            print(\"  \", self._model_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"CASS model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_path):\n            os.makedirs(os.path.dirname(self._model_path), exist_ok=True)\n            utils.download(\n                \"https://drive.google.com/u/0/uc?id=14K1a-Ft-YO9dUREEXxmWqF2ruUP4p7BZ&amp;\"\n                \"export=download\",\n                self._model_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See CPASMethod.inference.\n\n        Based on cass.tools.eval.\n        \"\"\"\n        # get bounding box\n        valid_mask = (depth_image != 0) * instance_mask\n        rmin, rmax, cmin, cmax = cass.get_bbox(valid_mask.numpy())\n        bb_mask = torch.zeros_like(depth_image)\n        bb_mask[rmin:rmax, cmin:cmax] = 1.0\n\n        # prepare image crop\n        color_input = torch.flip(color_image, (2,)).permute([2, 0, 1])  # RGB -&gt; BGR\n        color_input = color_input[:, rmin:rmax, cmin:cmax]  # bb crop\n        color_input = color_input.unsqueeze(0)  # add batch dim\n        color_input = TF.normalize(\n            color_input, mean=[0.51, 0.47, 0.44], std=[0.29, 0.27, 0.28]\n        )\n\n        # prepare points (fixed number of points, randomly picked)\n        point_indices = valid_mask.nonzero()\n        if len(point_indices) &gt; self._num_points:\n            subset = np.random.choice(\n                len(point_indices), replace=False, size=self._num_points\n            )\n            point_indices = point_indices[subset]\n        depth_mask = torch.zeros_like(depth_image)\n        depth_mask[point_indices[:, 0], point_indices[:, 1]] = 1.0\n        cropped_depth_mask = depth_mask[rmin:rmax, cmin:cmax]\n        point_indices_input = cropped_depth_mask.flatten().nonzero()[:, 0]\n\n        # prepare pointcloud\n        points = pointset_utils.depth_to_pointcloud(\n            depth_image,\n            self._camera,\n            normalize=False,\n            mask=depth_mask,\n            convention=\"opencv\",\n        )\n        if len(points) &lt; self._num_points:\n            wrap_indices = np.pad(\n                np.arange(len(points)), (0, self._num_points - len(points)), mode=\"wrap\"\n            )\n            points = points[wrap_indices]\n            point_indices_input = point_indices_input[wrap_indices]\n\n        # x, y inverted for some reason...\n        points[:, 0] *= -1\n        points[:, 1] *= -1\n        points = points.unsqueeze(0)\n        point_indices_input = point_indices_input.unsqueeze(0)\n\n        # move inputs to device\n        color_input = color_input.to(self._device)\n        points = points.to(self._device)\n        point_indices_input = point_indices_input.to(self._device)\n\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n\n        # CASS model uses 0-indexed categories, same order as NOCSDataset\n        category_index = torch.tensor([category_id], device=self._device)\n\n        # Call CASS network\n        folding_encode = self._cass.foldingnet.encode(\n            color_input, points, point_indices_input\n        )\n        posenet_encode = self._cass.estimator.encode(\n            color_input, points, point_indices_input\n        )\n        pred_r, pred_t, pred_c = self._cass.estimator.pose(\n            torch.cat([posenet_encode, folding_encode], dim=1), category_index\n        )\n        reconstructed_points = self._cass.foldingnet.recon(folding_encode)[0]\n\n        # Postprocess outputs\n        reconstructed_points = reconstructed_points.view(-1, 3).cpu()\n        pred_c = pred_c.view(1, self._num_points)\n        _, max_index = torch.max(pred_c, 1)\n        pred_t = pred_t.view(self._num_points, 1, 3)\n        orientation_q = pred_r[0][max_index[0]].view(-1).cpu()\n        points = points.view(self._num_points, 1, 3)\n        position = (points + pred_t)[max_index[0]].view(-1).cpu()\n        # output is scalar-first -&gt; scalar-last\n        orientation_q = torch.tensor([*orientation_q[1:], orientation_q[0]])\n\n        # Flip x and y axis of position and orientation (undo flipping of points)\n        # (x-left, y-up, z-forward) convention -&gt; OpenCV convention\n        position[0] *= -1\n        position[1] *= -1\n        cam_fix = torch.tensor([0.0, 0.0, 1.0, 0.0])\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor(\n            [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n        )  # CASS object to ShapeNet object\n        orientation_q = quaternion_utils.quaternion_multiply(cam_fix, orientation_q)\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n\n        # TODO refinement code from cass.tools.eval? (not mentioned in paper??)\n\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n        # pointset_utils.visualize_pointset(reconstructed_points)\n        return {\n            \"position\": position.detach(),\n            \"orientation\": orientation_q.detach(),\n            \"extents\": extents.detach(),\n            \"reconstructed_pointcloud\": reconstructed_points.detach(),\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for CASS.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for CASS.\n\n    Attributes:\n        model: Path to model.\n        device: Device string for the model.\n    \"\"\"\n\n    model: str\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load CASS model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>CASS configuration. See CASS.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load CASS model.\n\n    Args:\n        config: CASS configuration. See CASS.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=CASS.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/cass/#cpas_toolbox.cpas_methods.cass.CASS.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> <p>Based on cass.tools.eval.</p> Source code in <code>cpas_toolbox/cpas_methods/cass.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See CPASMethod.inference.\n\n    Based on cass.tools.eval.\n    \"\"\"\n    # get bounding box\n    valid_mask = (depth_image != 0) * instance_mask\n    rmin, rmax, cmin, cmax = cass.get_bbox(valid_mask.numpy())\n    bb_mask = torch.zeros_like(depth_image)\n    bb_mask[rmin:rmax, cmin:cmax] = 1.0\n\n    # prepare image crop\n    color_input = torch.flip(color_image, (2,)).permute([2, 0, 1])  # RGB -&gt; BGR\n    color_input = color_input[:, rmin:rmax, cmin:cmax]  # bb crop\n    color_input = color_input.unsqueeze(0)  # add batch dim\n    color_input = TF.normalize(\n        color_input, mean=[0.51, 0.47, 0.44], std=[0.29, 0.27, 0.28]\n    )\n\n    # prepare points (fixed number of points, randomly picked)\n    point_indices = valid_mask.nonzero()\n    if len(point_indices) &gt; self._num_points:\n        subset = np.random.choice(\n            len(point_indices), replace=False, size=self._num_points\n        )\n        point_indices = point_indices[subset]\n    depth_mask = torch.zeros_like(depth_image)\n    depth_mask[point_indices[:, 0], point_indices[:, 1]] = 1.0\n    cropped_depth_mask = depth_mask[rmin:rmax, cmin:cmax]\n    point_indices_input = cropped_depth_mask.flatten().nonzero()[:, 0]\n\n    # prepare pointcloud\n    points = pointset_utils.depth_to_pointcloud(\n        depth_image,\n        self._camera,\n        normalize=False,\n        mask=depth_mask,\n        convention=\"opencv\",\n    )\n    if len(points) &lt; self._num_points:\n        wrap_indices = np.pad(\n            np.arange(len(points)), (0, self._num_points - len(points)), mode=\"wrap\"\n        )\n        points = points[wrap_indices]\n        point_indices_input = point_indices_input[wrap_indices]\n\n    # x, y inverted for some reason...\n    points[:, 0] *= -1\n    points[:, 1] *= -1\n    points = points.unsqueeze(0)\n    point_indices_input = point_indices_input.unsqueeze(0)\n\n    # move inputs to device\n    color_input = color_input.to(self._device)\n    points = points.to(self._device)\n    point_indices_input = point_indices_input.to(self._device)\n\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n\n    # CASS model uses 0-indexed categories, same order as NOCSDataset\n    category_index = torch.tensor([category_id], device=self._device)\n\n    # Call CASS network\n    folding_encode = self._cass.foldingnet.encode(\n        color_input, points, point_indices_input\n    )\n    posenet_encode = self._cass.estimator.encode(\n        color_input, points, point_indices_input\n    )\n    pred_r, pred_t, pred_c = self._cass.estimator.pose(\n        torch.cat([posenet_encode, folding_encode], dim=1), category_index\n    )\n    reconstructed_points = self._cass.foldingnet.recon(folding_encode)[0]\n\n    # Postprocess outputs\n    reconstructed_points = reconstructed_points.view(-1, 3).cpu()\n    pred_c = pred_c.view(1, self._num_points)\n    _, max_index = torch.max(pred_c, 1)\n    pred_t = pred_t.view(self._num_points, 1, 3)\n    orientation_q = pred_r[0][max_index[0]].view(-1).cpu()\n    points = points.view(self._num_points, 1, 3)\n    position = (points + pred_t)[max_index[0]].view(-1).cpu()\n    # output is scalar-first -&gt; scalar-last\n    orientation_q = torch.tensor([*orientation_q[1:], orientation_q[0]])\n\n    # Flip x and y axis of position and orientation (undo flipping of points)\n    # (x-left, y-up, z-forward) convention -&gt; OpenCV convention\n    position[0] *= -1\n    position[1] *= -1\n    cam_fix = torch.tensor([0.0, 0.0, 1.0, 0.0])\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor(\n        [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n    )  # CASS object to ShapeNet object\n    orientation_q = quaternion_utils.quaternion_multiply(cam_fix, orientation_q)\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n\n    # TODO refinement code from cass.tools.eval? (not mentioned in paper??)\n\n    extents, _ = reconstructed_points.abs().max(dim=0)\n    extents *= 2.0\n\n    # pointset_utils.visualize_pointset(reconstructed_points)\n    return {\n        \"position\": position.detach(),\n        \"orientation\": orientation_q.detach(),\n        \"extents\": extents.detach(),\n        \"reconstructed_pointcloud\": reconstructed_points.detach(),\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/","title":"crnet.py","text":""},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet","title":"cpas_toolbox.cpas_methods.crnet","text":"<p>This module defines CR-Net interface.</p> <p>Method is described in Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks, Wang, 2021.</p> <p>Implementation based on https://github.com/JeremyWANGJZ/Category-6D-Pose</p>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet","title":"CRNet","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for CRNet.</p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>class CRNet(CPASMethod):\n    \"\"\"Wrapper class for CRNet.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for CRNet.\n\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            device: Device string for the model.\n        \"\"\"\n\n        model: str\n        num_categories: int\n\n    default_config: Config = {\n        \"model\": None,\n        \"num_categories\": None,\n        \"num_shape_points\": None,\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load CRNet model.\n\n        Args:\n            config: CRNet configuration. See CRNet.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=CRNet.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_path = utils.resolve_path(config[\"model\"])\n        self._model_url = config[\"model_url\"]\n        self._mean_shape_path = utils.resolve_path(config[\"mean_shape\"])\n        self._mean_shape_url = config[\"mean_shape_url\"]\n        self._check_paths()\n        self._crnet = crnet.DeformNet(\n            config[\"num_categories\"], config[\"num_shape_points\"]\n        )\n        self._crnet.cuda()\n        self._crnet = torch.nn.DataParallel(self._crnet, device_ids=[self._device])\n        self._crnet.load_state_dict(\n            torch.load(self._model_path, map_location=self._device)\n        )\n        self._crnet.eval()\n        self._mean_shape_pointsets = np.load(self._mean_shape_path)\n        self._num_input_points = config[\"num_input_points\"]\n        self._image_size = config[\"image_size\"]\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_path) or not os.path.exists(\n            self._mean_shape_path\n        ):\n            print(\"CRNet model weights not found, do you want to download to \")\n            print(\"  \", self._model_path)\n            print(\"  \", self._mean_shape_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"CRNet model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_path):\n            os.makedirs(os.path.dirname(self._model_path), exist_ok=True)\n            utils.download(\n                self._model_url,\n                self._model_path,\n            )\n        if not os.path.exists(self._mean_shape_path):\n            os.makedirs(os.path.dirname(self._mean_shape_path), exist_ok=True)\n            utils.download(\n                self._mean_shape_url,\n                self._mean_shape_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n        Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n        \"\"\"\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n        mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n        # Get bounding box\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        rmin, rmax, cmin, cmax = crnet.get_bbox([y1, x1, y2, x2])\n\n        valid_mask = (depth_image != 0) * instance_mask\n\n        # Prepare image crop\n        color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n        color_input = cv2.resize(\n            color_input,\n            (self._image_size, self._image_size),\n            interpolation=cv2.INTER_LINEAR,\n        )\n        color_input = TF.normalize(\n            TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n        # Prepare input points\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n        width = self._camera.width\n        height = self._camera.height\n        point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n        xmap = np.array([[i for i in range(width)] for _ in range(height)])\n        ymap = np.array([[j for _ in range(width)] for j in range(height)])\n        # if len(choose) &lt; 32:\n        #     f_sRT[i] = np.identity(4, dtype=float)\n        #     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n        #     continue\n        # else:\n        #     valid_inst.append(i)\n        if len(point_indices) &gt; self._num_input_points:\n            # take subset of points if two many depth points\n            point_indices_mask = np.zeros(len(point_indices), dtype=int)\n            point_indices_mask[: self._num_input_points] = 1\n            np.random.shuffle(point_indices_mask)\n            point_indices = point_indices[point_indices_mask.nonzero()]\n        else:\n            point_indices = np.pad(\n                point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n            )  # repeat points if not enough depth observation\n        depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n            :, None\n        ]\n        xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        pt2 = depth_masked.numpy()\n        pt0 = (xmap_masked - cx) * pt2 / fx\n        pt1 = (ymap_masked - cy) * pt2 / fy\n        points = np.concatenate((pt0, pt1, pt2), axis=1)\n        # adjust indices for resizing of color image\n        crop_w = rmax - rmin\n        ratio = self._image_size / crop_w\n        col_idx = point_indices % crop_w\n        row_idx = point_indices // crop_w\n        point_indices = (\n            np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n        ).astype(np.int64)\n\n        # Move inputs to device and convert to right shape\n        color_input = color_input.unsqueeze(0).to(self._device)\n        points = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\n        point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n        category_id = torch.cuda.LongTensor([category_id]).to(self._device)\n        mean_shape_pointset = (\n            torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n        )\n\n        # Call CRNet\n        assign_matrix, deltas = self._crnet(\n            points,\n            color_input,\n            point_indices,\n            category_id,\n            mean_shape_pointset,\n        )\n\n        # Postprocess output\n        inst_shape = mean_shape_pointset + deltas\n        assign_matrix = torch.softmax(assign_matrix, dim=2)\n        coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n        point_indices = point_indices[0].cpu().numpy()\n        _, point_indices = np.unique(point_indices, return_index=True)\n        nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n        extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n        points = points[0, point_indices, :].cpu().numpy()\n        scale, orientation_m, position, _ = crnet.estimateSimilarityTransform(\n            nocs_coords, points\n        )\n        orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n        reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n        # Recenter for mug category\n        if category_str == \"mug\":  # undo mug translation\n            x_offset = (\n                (\n                    self._mean_shape_pointsets[5].max(axis=0)[0]\n                    + self._mean_shape_pointsets[5].min(axis=0)[0]\n                )\n                / 2\n                * scale\n            )\n            reconstructed_points[:, 0] -= x_offset\n            position += quaternion_utils.quaternion_apply(\n                orientation_q, torch.FloatTensor([x_offset, 0, 0])\n            ).numpy()\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n        return {\n            \"position\": torch.Tensor(position),\n            \"orientation\": orientation_q,\n            \"extents\": torch.Tensor(extents),\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for CRNet.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>int</code> </p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for CRNet.\n\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        device: Device string for the model.\n    \"\"\"\n\n    model: str\n    num_categories: int\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load CRNet model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>CRNet configuration. See CRNet.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load CRNet model.\n\n    Args:\n        config: CRNet configuration. See CRNet.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=CRNet.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/crnet/#cpas_toolbox.cpas_methods.crnet.CRNet.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/crnet.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n    Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n    \"\"\"\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n    mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n    # Get bounding box\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    rmin, rmax, cmin, cmax = crnet.get_bbox([y1, x1, y2, x2])\n\n    valid_mask = (depth_image != 0) * instance_mask\n\n    # Prepare image crop\n    color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n    color_input = cv2.resize(\n        color_input,\n        (self._image_size, self._image_size),\n        interpolation=cv2.INTER_LINEAR,\n    )\n    color_input = TF.normalize(\n        TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n\n    # Prepare input points\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n    width = self._camera.width\n    height = self._camera.height\n    point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n    xmap = np.array([[i for i in range(width)] for _ in range(height)])\n    ymap = np.array([[j for _ in range(width)] for j in range(height)])\n    # if len(choose) &lt; 32:\n    #     f_sRT[i] = np.identity(4, dtype=float)\n    #     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n    #     continue\n    # else:\n    #     valid_inst.append(i)\n    if len(point_indices) &gt; self._num_input_points:\n        # take subset of points if two many depth points\n        point_indices_mask = np.zeros(len(point_indices), dtype=int)\n        point_indices_mask[: self._num_input_points] = 1\n        np.random.shuffle(point_indices_mask)\n        point_indices = point_indices[point_indices_mask.nonzero()]\n    else:\n        point_indices = np.pad(\n            point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n        )  # repeat points if not enough depth observation\n    depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n        :, None\n    ]\n    xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    pt2 = depth_masked.numpy()\n    pt0 = (xmap_masked - cx) * pt2 / fx\n    pt1 = (ymap_masked - cy) * pt2 / fy\n    points = np.concatenate((pt0, pt1, pt2), axis=1)\n    # adjust indices for resizing of color image\n    crop_w = rmax - rmin\n    ratio = self._image_size / crop_w\n    col_idx = point_indices % crop_w\n    row_idx = point_indices // crop_w\n    point_indices = (\n        np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n    ).astype(np.int64)\n\n    # Move inputs to device and convert to right shape\n    color_input = color_input.unsqueeze(0).to(self._device)\n    points = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\n    point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n    category_id = torch.cuda.LongTensor([category_id]).to(self._device)\n    mean_shape_pointset = (\n        torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n    )\n\n    # Call CRNet\n    assign_matrix, deltas = self._crnet(\n        points,\n        color_input,\n        point_indices,\n        category_id,\n        mean_shape_pointset,\n    )\n\n    # Postprocess output\n    inst_shape = mean_shape_pointset + deltas\n    assign_matrix = torch.softmax(assign_matrix, dim=2)\n    coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n    point_indices = point_indices[0].cpu().numpy()\n    _, point_indices = np.unique(point_indices, return_index=True)\n    nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n    extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n    points = points[0, point_indices, :].cpu().numpy()\n    scale, orientation_m, position, _ = crnet.estimateSimilarityTransform(\n        nocs_coords, points\n    )\n    orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n    reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n    # Recenter for mug category\n    if category_str == \"mug\":  # undo mug translation\n        x_offset = (\n            (\n                self._mean_shape_pointsets[5].max(axis=0)[0]\n                + self._mean_shape_pointsets[5].min(axis=0)[0]\n            )\n            / 2\n            * scale\n        )\n        reconstructed_points[:, 0] -= x_offset\n        position += quaternion_utils.quaternion_apply(\n            orientation_q, torch.FloatTensor([x_offset, 0, 0])\n        ).numpy()\n\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n    extents, _ = reconstructed_points.abs().max(dim=0)\n    extents *= 2.0\n\n    return {\n        \"position\": torch.Tensor(position),\n        \"orientation\": orientation_q,\n        \"extents\": torch.Tensor(extents),\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/","title":"dpdn.py","text":""},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn","title":"cpas_toolbox.cpas_methods.dpdn","text":"<p>This module defines DPDN interface.</p> <p>Method is described in Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks, Lin, 2022.</p> <p>Implementation based on https://github.com/JiehongLin/Self-DPDN.</p>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN","title":"DPDN","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for DPDN.</p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>class DPDN(CPASMethod):\n    \"\"\"Wrapper class for DPDN.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for DPDN.\n\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            device: Device string for the model.\n        \"\"\"\n\n        model: str\n        num_categories: int\n        num_shape_points: int\n        num_input_points: int\n        image_size: int\n        model: str\n        model_url: str\n        mean_shape: str\n        mean_shape_url: str\n        resnet_dir: str\n        device: str\n\n    default_config: Config = {\n        \"model\": None,\n        \"num_categories\": None,\n        \"num_shape_points\": None,\n        \"num_input_points\": None,\n        \"image_size\": None,\n        \"model\": None,\n        \"model_url\": None,\n        \"mean_shape\": None,\n        \"mean_shape_url\": None,\n        \"resnet_dir\": None,\n        \"device\": \"cuda\",\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load DPDN model.\n\n        Args:\n            config: DPDN configuration. See DPDN.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=DPDN.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_file_path = utils.resolve_path(config[\"model\"])\n        self._model_url = config[\"model_url\"]\n        self._mean_shape_file_path = utils.resolve_path(config[\"mean_shape\"])\n        self._mean_shape_url = config[\"mean_shape_url\"]\n        self._check_paths()\n        self._resnet_dir_path = utils.resolve_path(config[\"resnet_dir\"])\n\n        self._dpdn = dpdn.Net(\n            config[\"num_categories\"], config[\"num_shape_points\"], self._resnet_dir_path\n        )\n        self._dpdn = self._dpdn.to(self._device)\n        checkpoint = torch.load(self._model_file_path, map_location=self._device)\n        if \"model\" in checkpoint:\n            state_dict = checkpoint[\"model\"]\n        elif \"state_dict\":\n            state_dict = checkpoint[\"state_dict\"]\n        else:\n            state_dict = checkpoint\n        self._dpdn.load_state_dict(state_dict)\n        self._dpdn.eval()\n        self._mean_shape_pointsets = np.load(self._mean_shape_file_path)\n        self._num_input_points = config[\"num_input_points\"]\n        self._image_size = config[\"image_size\"]\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_file_path) or not os.path.exists(\n            self._mean_shape_file_path\n        ):\n            print(\"DPDN model weights not found, do you want to download to \")\n            print(\"  \", self._model_file_path)\n            print(\"  \", self._mean_shape_file_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"DPDN model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_file_path):\n            dl_dir_path = tempfile.mkdtemp()\n            print(dl_dir_path)\n            zip_file_path = os.path.join(dl_dir_path, \"temp\")\n            os.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\n            utils.download(\n                self._model_url,\n                zip_file_path,\n            )\n            z = zipfile.ZipFile(zip_file_path)\n            z.extract(\"log/supervised/epoch_30.pth\", dl_dir_path)\n            z.close()\n            os.remove(zip_file_path)\n            shutil.move(\n                os.path.join(dl_dir_path, \"log\", \"supervised\", \"epoch_30.pth\"),\n                self._model_file_path,\n            )\n            shutil.rmtree(dl_dir_path)\n        if not os.path.exists(self._mean_shape_file_path):\n            os.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\n            utils.download(\n                self._mean_shape_url,\n                self._mean_shape_file_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n        Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py\n        \"\"\"\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n        mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n        input_dict = {}\n\n        # Get bounding box\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        rmin, rmax, cmin, cmax = dpdn.get_bbox([y1, x1, y2, x2])\n\n        # Prepare image crop\n        color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()\n        color_input = cv2.resize(\n            color_input,\n            (self._image_size, self._image_size),\n            interpolation=cv2.INTER_LINEAR,\n        )\n        color_input = TF.normalize(\n            TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n        input_dict[\"rgb\"] = color_input.unsqueeze(0).to(self._device)\n\n        # Prepare point indices\n        mask = (depth_image != 0) * instance_mask\n        cropped_mask = mask[rmin:rmax, cmin:cmax]\n        cropped_mask_indices = cropped_mask.numpy().flatten().nonzero()[0]\n        if len(cropped_mask_indices) &lt;= self._num_input_points:\n            indices = np.random.choice(\n                len(cropped_mask_indices), self._num_input_points\n            )\n        else:\n            indices = np.random.choice(\n                len(cropped_mask_indices), self._num_input_points, replace=False\n            )\n        chosen_cropped_indices = cropped_mask_indices[indices]\n\n        # adjust indices for resizing of color image\n        crop_w = rmax - rmin\n        ratio = self._image_size / crop_w\n        col_idx = chosen_cropped_indices % crop_w\n        row_idx = chosen_cropped_indices // crop_w\n        final_cropped_indices = (\n            np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n        ).astype(np.int64)\n        input_dict[\"choose\"] = (\n            torch.LongTensor(final_cropped_indices).unsqueeze(0).to(self._device)\n        )\n\n        # Prepare input points\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n        width = self._camera.width\n        height = self._camera.height\n        depth_image_np = depth_image.numpy()\n        depth_image_np = dpdn.fill_missing(depth_image_np * 1000.0, 1000.0, 1) / 1000.0\n\n        xmap = np.array([[i for i in range(width)] for _ in range(height)])\n        ymap = np.array([[j for _ in range(width)] for j in range(height)])\n        pts2 = depth_image_np.copy()\n        pts0 = (xmap - cx) * pts2 / fx\n        pts1 = (ymap - cy) * pts2 / fy\n        pts_map = np.stack([pts0, pts1, pts2])\n        pts_map = np.transpose(pts_map, (1, 2, 0)).astype(np.float32)\n        cropped_pts_map = pts_map[rmin:rmax, cmin:cmax, :]\n        input_points = cropped_pts_map.reshape((-1, 3))[chosen_cropped_indices, :]\n        input_dict[\"pts\"] = (\n            torch.FloatTensor(input_points).unsqueeze(0).to(self._device)\n        )\n\n        # Prepare prior\n        input_dict[\"prior\"] = (\n            torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n        )\n\n        # Prepare category id\n        input_dict[\"category_label\"] = torch.cuda.LongTensor([category_id]).to(\n            self._device\n        )\n\n        # Call DPDN\n        outputs = self._dpdn(input_dict)\n\n        # Convert outputs to expected format\n        position = outputs[\"pred_translation\"][0].detach().cpu()\n\n        orientation_mat = outputs[\"pred_rotation\"][0].detach().cpu()\n        orientation = Rotation.from_matrix(orientation_mat.detach().numpy())\n        orientation_q = torch.FloatTensor(orientation.as_quat())\n        extents = outputs[\"pred_size\"][0].detach().cpu()\n        reconstructed_points = outputs[\"pred_qv\"][0].detach().cpu()\n        scale = torch.linalg.norm(extents)\n        reconstructed_points *= scale\n\n        # Recenter for mug category\n        # TODO not really sure if this is correct, but seems to give best results\n        if category_str == \"mug\":  # undo mug translation\n            x_offset = (\n                (\n                    self._mean_shape_pointsets[5].max(axis=0)[0]\n                    + self._mean_shape_pointsets[5].min(axis=0)[0]\n                )\n                / 2\n                * scale\n            )\n            reconstructed_points[:, 0] -= x_offset\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents = torch.FloatTensor([extents[2], extents[1], extents[0]])\n\n        return {\n            \"position\": position,\n            \"orientation\": orientation_q,\n            \"extents\": extents,\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for DPDN.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for DPDN.\n\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        device: Device string for the model.\n    \"\"\"\n\n    model: str\n    num_categories: int\n    num_shape_points: int\n    num_input_points: int\n    image_size: int\n    model: str\n    model_url: str\n    mean_shape: str\n    mean_shape_url: str\n    resnet_dir: str\n    device: str\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load DPDN model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>DPDN configuration. See DPDN.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load DPDN model.\n\n    Args:\n        config: DPDN configuration. See DPDN.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=DPDN.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/dpdn/#cpas_toolbox.cpas_methods.dpdn.DPDN.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py</p> Source code in <code>cpas_toolbox/cpas_methods/dpdn.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n    Based on https://github.com/JiehongLin/Self-DPDN/blob/main/test.py\n    \"\"\"\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n    mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n    input_dict = {}\n\n    # Get bounding box\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    rmin, rmax, cmin, cmax = dpdn.get_bbox([y1, x1, y2, x2])\n\n    # Prepare image crop\n    color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()\n    color_input = cv2.resize(\n        color_input,\n        (self._image_size, self._image_size),\n        interpolation=cv2.INTER_LINEAR,\n    )\n    color_input = TF.normalize(\n        TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n    input_dict[\"rgb\"] = color_input.unsqueeze(0).to(self._device)\n\n    # Prepare point indices\n    mask = (depth_image != 0) * instance_mask\n    cropped_mask = mask[rmin:rmax, cmin:cmax]\n    cropped_mask_indices = cropped_mask.numpy().flatten().nonzero()[0]\n    if len(cropped_mask_indices) &lt;= self._num_input_points:\n        indices = np.random.choice(\n            len(cropped_mask_indices), self._num_input_points\n        )\n    else:\n        indices = np.random.choice(\n            len(cropped_mask_indices), self._num_input_points, replace=False\n        )\n    chosen_cropped_indices = cropped_mask_indices[indices]\n\n    # adjust indices for resizing of color image\n    crop_w = rmax - rmin\n    ratio = self._image_size / crop_w\n    col_idx = chosen_cropped_indices % crop_w\n    row_idx = chosen_cropped_indices // crop_w\n    final_cropped_indices = (\n        np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n    ).astype(np.int64)\n    input_dict[\"choose\"] = (\n        torch.LongTensor(final_cropped_indices).unsqueeze(0).to(self._device)\n    )\n\n    # Prepare input points\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n    width = self._camera.width\n    height = self._camera.height\n    depth_image_np = depth_image.numpy()\n    depth_image_np = dpdn.fill_missing(depth_image_np * 1000.0, 1000.0, 1) / 1000.0\n\n    xmap = np.array([[i for i in range(width)] for _ in range(height)])\n    ymap = np.array([[j for _ in range(width)] for j in range(height)])\n    pts2 = depth_image_np.copy()\n    pts0 = (xmap - cx) * pts2 / fx\n    pts1 = (ymap - cy) * pts2 / fy\n    pts_map = np.stack([pts0, pts1, pts2])\n    pts_map = np.transpose(pts_map, (1, 2, 0)).astype(np.float32)\n    cropped_pts_map = pts_map[rmin:rmax, cmin:cmax, :]\n    input_points = cropped_pts_map.reshape((-1, 3))[chosen_cropped_indices, :]\n    input_dict[\"pts\"] = (\n        torch.FloatTensor(input_points).unsqueeze(0).to(self._device)\n    )\n\n    # Prepare prior\n    input_dict[\"prior\"] = (\n        torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n    )\n\n    # Prepare category id\n    input_dict[\"category_label\"] = torch.cuda.LongTensor([category_id]).to(\n        self._device\n    )\n\n    # Call DPDN\n    outputs = self._dpdn(input_dict)\n\n    # Convert outputs to expected format\n    position = outputs[\"pred_translation\"][0].detach().cpu()\n\n    orientation_mat = outputs[\"pred_rotation\"][0].detach().cpu()\n    orientation = Rotation.from_matrix(orientation_mat.detach().numpy())\n    orientation_q = torch.FloatTensor(orientation.as_quat())\n    extents = outputs[\"pred_size\"][0].detach().cpu()\n    reconstructed_points = outputs[\"pred_qv\"][0].detach().cpu()\n    scale = torch.linalg.norm(extents)\n    reconstructed_points *= scale\n\n    # Recenter for mug category\n    # TODO not really sure if this is correct, but seems to give best results\n    if category_str == \"mug\":  # undo mug translation\n        x_offset = (\n            (\n                self._mean_shape_pointsets[5].max(axis=0)[0]\n                + self._mean_shape_pointsets[5].min(axis=0)[0]\n            )\n            / 2\n            * scale\n        )\n        reconstructed_points[:, 0] -= x_offset\n\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n    extents = torch.FloatTensor([extents[2], extents[1], extents[0]])\n\n    return {\n        \"position\": position,\n        \"orientation\": orientation_q,\n        \"extents\": extents,\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/","title":"icaps.py","text":""},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps","title":"cpas_toolbox.cpas_methods.icaps","text":"<p>This module defines iCaps interface.</p> <p>Method is described in iCaps Iterative Category-Level Object Pose and Shape Estimation, Deng, 2022.</p> <p>Implementation based on https://github.com/aerogjy/iCaps</p>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps","title":"ICaps","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for iCaps.</p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>class ICaps(CPASMethod):\n    \"\"\"Wrapper class for iCaps.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for iCaps.\n\n        Attributes:\n            pf_config_dir:\n                Particle filter configuration directory for iCaps. Must contain one yml\n                file for each supported category_str.\n            deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints.\n            latentnet_checkpoint_dir: Directory containing LatentNet checkpoints.\n            aae_checkpoint_dir: Directory containing auto-encoder checkpoints.\n            checkpoints_url:\n                URL to download checkpoints from if checkpoint directories are empty or\n                do not exist yet (assumed to be tar file).\n            categories:\n                List of category strings. Each category requires corresponding\n                directories in each checkpoint dir.\n            device:\n                The device to use.\n        \"\"\"\n\n        pf_config_dir: str\n        deepsdf_checkpoint_dir: str\n        latentnet_checkpoint_dir: str\n        aae_checkpoint_dir: str\n        checkpoints_url: str\n        categories: List[str]\n        device: str\n\n    default_config: Config = {\n        \"pf_config_dir\": None,\n        \"deepsdf_checkpoint_dir\": None,\n        \"latentnet_checkpoint_dir\": None,\n        \"aae_checkpoint_dir\": None,\n        \"checkpoints_url\": None,\n        \"categories\": [\"bottle\", \"bowl\", \"camera\", \"can\", \"laptop\", \"mug\"],\n        \"device\": \"cuda\",\n    }\n\n    # This is to replace reliance on ground-truth mesh in iCaps\n    # Numbers computed from mean shapes used in SDF\n    # see: https://github.com/aerogjy/iCaps/issues/1\n\n    category_str_to_ratio = {\n        \"bottle\": 2.149,\n        \"bowl\": 2.7,\n        \"camera\": 2.328,\n        \"can\": 2.327,\n        \"laptop\": 2.076,\n        \"mug\": 2.199,\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load ICaps models.\n\n        Args:\n            config: iCaps configuration. See ICaps.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=ICaps.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._num_points = config[\"num_points\"]\n        self._category_strs = config[\"categories\"]\n\n        self._checkpoints_url = config[\"checkpoints_url\"]\n        self._pose_rbpfs = {}\n        pf_cfg_dir_path = utils.resolve_path(\n            config[\"pf_config_dir\"],\n            search_paths=[\n                \".\",\n                \"~/.cpas_toolbox\",\n                os.path.join(os.path.dirname(__file__), \"config\"),\n                os.path.dirname(__file__),\n            ],\n        )\n        self._deepsdf_ckp_dir_path = utils.resolve_path(\n            config[\"deepsdf_checkpoint_dir\"]\n        )\n        self._latentnet_ckp_dir_path = utils.resolve_path(\n            config[\"latentnet_checkpoint_dir\"]\n        )\n        self._aae_ckp_dir_path = utils.resolve_path(config[\"aae_checkpoint_dir\"])\n        self._check_paths()\n\n        for category_str in self._category_strs:\n            full_ckpt_dir_path = os.path.join(self._aae_ckp_dir_path, category_str)\n            train_cfg_file = os.path.join(full_ckpt_dir_path, \"config.yml\")\n            icaps.icaps_config.cfg_from_file(train_cfg_file)\n            test_cfg_file = os.path.join(pf_cfg_dir_path, category_str + \".yml\")\n            icaps.icaps_config.cfg_from_file(test_cfg_file)\n            obj_list = icaps.icaps_config.cfg.TEST.OBJECTS\n            cfg_list = []\n            cfg_list.append(copy.deepcopy(icaps.icaps_config.cfg))\n\n            self._pose_rbpfs[category_str] = icaps.PoseRBPF(\n                obj_list,\n                cfg_list,\n                full_ckpt_dir_path,\n                self._deepsdf_ckp_dir_path,\n                self._latentnet_ckp_dir_path,\n                device=self._device,\n            )\n            self._pose_rbpfs[category_str].set_target_obj(\n                icaps.icaps_config.cfg.TEST.OBJECTS[0]\n            )\n            self._pose_rbpfs[category_str].set_ratio(\n                self.category_str_to_ratio[category_str]\n            )\n\n    def _check_paths(self) -&gt; None:\n        path_exists = (\n            os.path.exists(p)\n            for p in [\n                self._aae_ckp_dir_path,\n                self._deepsdf_ckp_dir_path,\n                self._latentnet_ckp_dir_path,\n            ]\n        )\n        if not all(path_exists):\n            print(\"iCaps model weights not found, do you want to download to \")\n            print(\"  \", self._aae_ckp_dir_path)\n            print(\"  \", self._deepsdf_ckp_dir_path)\n            print(\"  \", self._latentnet_ckp_dir_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"iCaps model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        dl_dir_path = tempfile.mkdtemp()\n        tar_file_path = os.path.join(dl_dir_path, \"temp\")\n        print(self._checkpoints_url, tar_file_path)\n        utils.download(\n            self._checkpoints_url,\n            tar_file_path,\n        )\n        tar_file = tarfile.open(tar_file_path)\n        print(\"Extracting weights... (this might take a while)\")\n        tar_file.extractall(dl_dir_path)\n        if not os.path.exists(self._latentnet_ckp_dir_path):\n            src_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"latentnet_ckpts\")\n            shutil.move(src_dir_path, self._latentnet_ckp_dir_path)\n        if not os.path.exists(self._deepsdf_ckp_dir_path):\n            src_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"deepsdf_ckpts\")\n            shutil.move(src_dir_path, self._deepsdf_ckp_dir_path)\n        if not os.path.exists(self._aae_ckp_dir_path):\n            src_dir_path = os.path.join(dl_dir_path, \"checkpoints\", \"aae_ckpts\")\n            shutil.move(src_dir_path, self._aae_ckp_dir_path)\n            # normalize names\n            for category_str in self._category_strs:\n                for aae_category_dir in os.listdir(self._aae_ckp_dir_path):\n                    if category_str in aae_category_dir:\n                        os.rename(\n                            aae_category_dir,\n                            os.path.join(self._aae_ckp_dir_path, category_str),\n                        )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See MethodWrapper.inference.\n\n        Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n        \"\"\"\n        # prepare data as expected by iCaps functions (same as nocs_real_dataset)\n        color_image = color_image * 255  # see icaps.datasets.nocs_real_dataset l71\n        depth_image = depth_image.unsqueeze(2)  # (...)nocs_real_dataset l79\n        instance_mask = instance_mask.float()  # (...)nocs_real_dataset l100\n        intrinsics = torch.eye(3)\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n        intrinsics[0, 0] = fx\n        intrinsics[1, 1] = fy\n        intrinsics[0, 2] = cx\n        intrinsics[1, 2] = cy\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        bbox = [y1, y2, x1, x2]\n\n        # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n        pose_rbpf = self._pose_rbpfs[category_str]\n        pose_rbpf.reset()  # like init but without loading models\n        self._pose_rbpfs[category_str].set_target_obj(category_str)\n\n        pose_rbpf.data_intrinsics = intrinsics.numpy()\n        pose_rbpf.intrinsics = intrinsics.numpy()\n        pose_rbpf.target_obj_cfg.PF.FU = pose_rbpf.intrinsics[0, 0]\n        pose_rbpf.target_obj_cfg.PF.FV = pose_rbpf.intrinsics[1, 1]\n        pose_rbpf.target_obj_cfg.PF.U0 = pose_rbpf.intrinsics[0, 2]\n        pose_rbpf.target_obj_cfg.PF.V0 = pose_rbpf.intrinsics[1, 2]\n\n        pose_rbpf.data_with_est_center = False\n        pose_rbpf.data_with_gt = False  # should this be False now?\n\n        pose_rbpf.mask_raw = instance_mask[:, :].cpu().numpy()\n        pose_rbpf.mask = scipy.ndimage.binary_erosion(\n            pose_rbpf.mask_raw, iterations=2\n        ).astype(pose_rbpf.mask_raw.dtype)\n\n        pose_rbpf.prior_uv[0] = (bbox[2] + bbox[3]) / 2\n        pose_rbpf.prior_uv[1] = (bbox[0] + bbox[1]) / 2\n\n        # what is this ??\n        if pose_rbpf.aae_full.angle_diff.shape[0] != 0:\n            pose_rbpf.aae_full.angle_diff = np.array([])\n\n        if pose_rbpf.target_obj_cfg.PF.USE_DEPTH:\n            depth_data = depth_image\n        else:\n            depth_data = None\n\n        try:\n            pose_rbpf.initialize_poserbpf(\n                color_image,\n                pose_rbpf.data_intrinsics,\n                pose_rbpf.prior_uv[:2],\n                pose_rbpf.target_obj_cfg.PF.N_INIT,\n                scale_prior=pose_rbpf.target_obj_cfg.PF.SCALE_PRIOR,\n                depth=depth_data,\n            )\n\n            pose_rbpf.process_poserbpf(\n                color_image,\n                intrinsics.unsqueeze(0),\n                depth=depth_data,\n                run_deep_sdf=False,\n            )\n            # 3 * 50 iters by default\n            pose_rbpf.refine_pose_and_shape(depth_data, intrinsics.unsqueeze(0))\n\n            position_cv = torch.tensor(pose_rbpf.rbpf.trans_bar)\n            orientation_q = torch.Tensor(\n                Rotation.from_matrix(pose_rbpf.rbpf.rot_bar).as_quat()\n            )\n\n            # NOCS Object -&gt; ShapeNet Object convention\n            obj_fix = torch.tensor(\n                [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n            )  # CASS object to ShapeNet object\n            orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n\n            orientation_cv = orientation_q\n            extents = torch.tensor([0.5, 0.5, 0.5])\n\n            mesh_dir_path = tempfile.mkdtemp()\n            mesh_file_path = os.path.join(mesh_dir_path, \"mesh.ply\")\n            point_set = pose_rbpf.evaluator.latent_vec_to_points(\n                pose_rbpf.latent_vec_refine,\n                N=64,\n                num_points=self._num_points,\n                silent=True,\n                fname=mesh_file_path,\n            )\n\n            if point_set is None:\n                point_set = torch.tensor([[0.0, 0.0, 0.0]])  # failed / no isosurface\n                reconstructed_mesh = None\n            else:\n                scale = pose_rbpf.size_est / pose_rbpf.ratio\n                point_set *= scale\n                reconstructed_mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n                reconstructed_mesh.scale(scale.item(), np.array([0, 0, 0]))\n\n            reconstructed_points = torch.tensor(point_set)\n\n            extents, _ = reconstructed_points.abs().max(dim=0)\n            extents *= 2.0\n            return {\n                \"position\": position_cv.detach().cpu(),\n                \"orientation\": orientation_cv.detach().cpu(),\n                \"extents\": extents.detach().cpu(),\n                \"reconstructed_pointcloud\": reconstructed_points,\n                \"reconstructed_mesh\": reconstructed_mesh,\n            }\n        except:\n            print(\"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\")\n            return {\n                \"position\": torch.tensor([0.0, 0.0, 0.0]),\n                \"orientation\": torch.tensor([0.0, 0.0, 0.0, 1.0]),\n                \"extents\": torch.tensor([0.5, 0.5, 0.5]),\n                \"reconstructed_pointcloud\": torch.tensor([[0.0, 0.0, 0.0]]),\n                \"reconstructed_mesh\": None,  # TODO if time\n            }\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for iCaps.</p> ATTRIBUTE DESCRIPTION <code>pf_config_dir</code> <p>Particle filter configuration directory for iCaps. Must contain one yml file for each supported category_str.</p> <p> TYPE: <code>str</code> </p> <code>deepsdf_checkpoint_dir</code> <p>Directory containing DeepSDF checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>latentnet_checkpoint_dir</code> <p>Directory containing LatentNet checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>aae_checkpoint_dir</code> <p>Directory containing auto-encoder checkpoints.</p> <p> TYPE: <code>str</code> </p> <code>checkpoints_url</code> <p>URL to download checkpoints from if checkpoint directories are empty or do not exist yet (assumed to be tar file).</p> <p> TYPE: <code>str</code> </p> <code>categories</code> <p>List of category strings. Each category requires corresponding directories in each checkpoint dir.</p> <p> TYPE: <code>List[str]</code> </p> <code>device</code> <p>The device to use.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for iCaps.\n\n    Attributes:\n        pf_config_dir:\n            Particle filter configuration directory for iCaps. Must contain one yml\n            file for each supported category_str.\n        deepsdf_checkpoint_dir: Directory containing DeepSDF checkpoints.\n        latentnet_checkpoint_dir: Directory containing LatentNet checkpoints.\n        aae_checkpoint_dir: Directory containing auto-encoder checkpoints.\n        checkpoints_url:\n            URL to download checkpoints from if checkpoint directories are empty or\n            do not exist yet (assumed to be tar file).\n        categories:\n            List of category strings. Each category requires corresponding\n            directories in each checkpoint dir.\n        device:\n            The device to use.\n    \"\"\"\n\n    pf_config_dir: str\n    deepsdf_checkpoint_dir: str\n    latentnet_checkpoint_dir: str\n    aae_checkpoint_dir: str\n    checkpoints_url: str\n    categories: List[str]\n    device: str\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load ICaps models.</p> PARAMETER  DESCRIPTION <code>config</code> <p>iCaps configuration. See ICaps.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load ICaps models.\n\n    Args:\n        config: iCaps configuration. See ICaps.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=ICaps.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/icaps/#cpas_toolbox.cpas_methods.icaps.ICaps.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See MethodWrapper.inference.</p> <p>Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset</p> Source code in <code>cpas_toolbox/cpas_methods/icaps.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See MethodWrapper.inference.\n\n    Based on icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n    \"\"\"\n    # prepare data as expected by iCaps functions (same as nocs_real_dataset)\n    color_image = color_image * 255  # see icaps.datasets.nocs_real_dataset l71\n    depth_image = depth_image.unsqueeze(2)  # (...)nocs_real_dataset l79\n    instance_mask = instance_mask.float()  # (...)nocs_real_dataset l100\n    intrinsics = torch.eye(3)\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n    intrinsics[0, 0] = fx\n    intrinsics[1, 1] = fy\n    intrinsics[0, 2] = cx\n    intrinsics[1, 2] = cy\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    bbox = [y1, y2, x1, x2]\n\n    # from here follow icaps.pose_rbpf.pose_rbps.PoseRBPF.run_nocs_dataset\n    pose_rbpf = self._pose_rbpfs[category_str]\n    pose_rbpf.reset()  # like init but without loading models\n    self._pose_rbpfs[category_str].set_target_obj(category_str)\n\n    pose_rbpf.data_intrinsics = intrinsics.numpy()\n    pose_rbpf.intrinsics = intrinsics.numpy()\n    pose_rbpf.target_obj_cfg.PF.FU = pose_rbpf.intrinsics[0, 0]\n    pose_rbpf.target_obj_cfg.PF.FV = pose_rbpf.intrinsics[1, 1]\n    pose_rbpf.target_obj_cfg.PF.U0 = pose_rbpf.intrinsics[0, 2]\n    pose_rbpf.target_obj_cfg.PF.V0 = pose_rbpf.intrinsics[1, 2]\n\n    pose_rbpf.data_with_est_center = False\n    pose_rbpf.data_with_gt = False  # should this be False now?\n\n    pose_rbpf.mask_raw = instance_mask[:, :].cpu().numpy()\n    pose_rbpf.mask = scipy.ndimage.binary_erosion(\n        pose_rbpf.mask_raw, iterations=2\n    ).astype(pose_rbpf.mask_raw.dtype)\n\n    pose_rbpf.prior_uv[0] = (bbox[2] + bbox[3]) / 2\n    pose_rbpf.prior_uv[1] = (bbox[0] + bbox[1]) / 2\n\n    # what is this ??\n    if pose_rbpf.aae_full.angle_diff.shape[0] != 0:\n        pose_rbpf.aae_full.angle_diff = np.array([])\n\n    if pose_rbpf.target_obj_cfg.PF.USE_DEPTH:\n        depth_data = depth_image\n    else:\n        depth_data = None\n\n    try:\n        pose_rbpf.initialize_poserbpf(\n            color_image,\n            pose_rbpf.data_intrinsics,\n            pose_rbpf.prior_uv[:2],\n            pose_rbpf.target_obj_cfg.PF.N_INIT,\n            scale_prior=pose_rbpf.target_obj_cfg.PF.SCALE_PRIOR,\n            depth=depth_data,\n        )\n\n        pose_rbpf.process_poserbpf(\n            color_image,\n            intrinsics.unsqueeze(0),\n            depth=depth_data,\n            run_deep_sdf=False,\n        )\n        # 3 * 50 iters by default\n        pose_rbpf.refine_pose_and_shape(depth_data, intrinsics.unsqueeze(0))\n\n        position_cv = torch.tensor(pose_rbpf.rbpf.trans_bar)\n        orientation_q = torch.Tensor(\n            Rotation.from_matrix(pose_rbpf.rbpf.rot_bar).as_quat()\n        )\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor(\n            [0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)]\n        )  # CASS object to ShapeNet object\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n\n        orientation_cv = orientation_q\n        extents = torch.tensor([0.5, 0.5, 0.5])\n\n        mesh_dir_path = tempfile.mkdtemp()\n        mesh_file_path = os.path.join(mesh_dir_path, \"mesh.ply\")\n        point_set = pose_rbpf.evaluator.latent_vec_to_points(\n            pose_rbpf.latent_vec_refine,\n            N=64,\n            num_points=self._num_points,\n            silent=True,\n            fname=mesh_file_path,\n        )\n\n        if point_set is None:\n            point_set = torch.tensor([[0.0, 0.0, 0.0]])  # failed / no isosurface\n            reconstructed_mesh = None\n        else:\n            scale = pose_rbpf.size_est / pose_rbpf.ratio\n            point_set *= scale\n            reconstructed_mesh = o3d.io.read_triangle_mesh(mesh_file_path)\n            reconstructed_mesh.scale(scale.item(), np.array([0, 0, 0]))\n\n        reconstructed_points = torch.tensor(point_set)\n\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n        return {\n            \"position\": position_cv.detach().cpu(),\n            \"orientation\": orientation_cv.detach().cpu(),\n            \"extents\": extents.detach().cpu(),\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": reconstructed_mesh,\n        }\n    except:\n        print(\"===PROBLEM DETECTED WITH ICAPS, RETURNING NO PREDICTION INSTEAD===\")\n        return {\n            \"position\": torch.tensor([0.0, 0.0, 0.0]),\n            \"orientation\": torch.tensor([0.0, 0.0, 0.0, 1.0]),\n            \"extents\": torch.tensor([0.5, 0.5, 0.5]),\n            \"reconstructed_pointcloud\": torch.tensor([[0.0, 0.0, 0.0]]),\n            \"reconstructed_mesh\": None,  # TODO if time\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/","title":"rbppose.py","text":""},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose","title":"cpas_toolbox.cpas_methods.rbppose","text":"<p>This module defines RBPPose interface.</p> <p>Method is described in RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation, Zhang, 2022</p> <p>Implementation based on https://github.com/lolrudy/RBP_Pose.</p>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose","title":"RBPPose","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for RBPPose.</p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>class RBPPose(CPASMethod):\n    \"\"\"Wrapper class for RBPPose.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for RBPPose.\n\n        Attributes:\n            model: File path for model weights.\n            model_url: URL to download model weights if file is not found.\n            mean_shape: File path for mean shape file.\n            mean_shape_url: URL to download mean shape file if it is not found.\n            device: Device string for the model.\n        \"\"\"\n\n        model: str\n        model_url: str\n        mean_shape: str\n        mean_shape_url: str\n        device: str\n\n    default_config: Config = {\n        \"model\": None,\n        \"model_url\": None,\n        \"mean_shape\": None,\n        \"mean_shape_url\": None,\n        \"device\": \"cuda\",\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load RBPPose model.\n\n        Args:\n            config: RBPPose configuration. See RBPPose.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=RBPPose.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_file_path = utils.resolve_path(config[\"model\"])\n        self._model_url = config[\"model_url\"]\n        self._mean_shape_file_path = utils.resolve_path(config[\"mean_shape\"])\n        self._mean_shape_url = config[\"mean_shape_url\"]\n\n        # Check if files are available and download if not\n        self._check_paths()\n\n        # Initialize model\n        self._net = rbppose.SSPN(...)\n        self._net = self._net.to(self._device)\n        state_dict = torch.load(self._model_file_path, map_location=self._device)\n        cleaned_state_dict = copy.copy(state_dict)\n        for key in state_dict.keys():\n            if \"face_recon\" in key:\n                cleaned_state_dict.pop(key)\n            elif \"pcl_encoder_prior\" in key:\n                cleaned_state_dict.pop(key)\n        current_model_dict = self._net.state_dict()\n        current_model_dict.update(cleaned_state_dict)\n        self._net.load_state_dict(current_model_dict)\n        self._net.eval()\n        self._mean_shape_pointsets = np.load(self._mean_shape_file_path)\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_file_path) or not os.path.exists(\n            self._mean_shape_file_path\n        ):\n            print(\"RBPPose model weights not found, do you want to download to \")\n            print(\"  \", self._model_file_path)\n            print(\"  \", self._mean_shape_file_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"RBPPose model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_file_path):\n            os.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\n            utils.download(\n                self._model_url,\n                self._model_file_path,\n            )\n        if not os.path.exists(self._mean_shape_file_path):\n            os.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\n            utils.download(\n                self._mean_shape_url,\n                self._mean_shape_file_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n        Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py\n        \"\"\"\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n\n        # Handle camera information\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(0)\n        width = self._camera.width\n        height = self._camera.height\n        camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n        camera_matrix = torch.FloatTensor(camera_matrix).unsqueeze(0).to(self._device)\n\n        # Prepare RGB crop (not used by default config)\n        rgb_cv = color_image.numpy()[:, :, ::-1]  # BGR\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        rmin, rmax, cmin, cmax = rbppose.get_bbox([y1, x1, y2, x2])\n        cx = 0.5 * (cmin + cmax)\n        cy = 0.5 * (rmin + rmax)\n        bbox_center = np.array([cx, cy])  # (w/2, h/2)\n        scale = min(max(cmax - cmin, rmax - rmin), max(height, width))\n        rgb_crop = rbppose.crop_resize_by_warp_affine(\n            rgb_cv,\n            bbox_center,\n            scale,\n            rbppose.FLAGS.img_size,\n            interpolation=cv2.INTER_NEAREST,\n        ).transpose(2, 0, 1)\n        rgb_crop = torch.FloatTensor(rgb_crop).unsqueeze(0).to(self._device)\n\n        # Prepare depth crop (expected in mm)\n        depth_cv = depth_image.numpy() * 1000\n        depth_crop = rbppose.crop_resize_by_warp_affine(\n            depth_cv,\n            bbox_center,\n            scale,\n            rbppose.FLAGS.img_size,\n            interpolation=cv2.INTER_NEAREST,\n        )\n        depth_crop = torch.FloatTensor(depth_crop)[None, None].to(self._device)\n\n        # Prepare category\n        category_input = torch.LongTensor([category_id]).to(self._device)\n\n        # Prepare ROI Mask\n        mask_np = instance_mask.float().numpy()\n        roi_mask = rbppose.crop_resize_by_warp_affine(\n            mask_np,\n            bbox_center,\n            scale,\n            rbppose.FLAGS.img_size,\n            interpolation=cv2.INTER_NEAREST,\n        )\n        roi_mask = torch.FloatTensor(roi_mask)[None, None].to(self._device)\n\n        # Prepare mean shape (size?)\n        mean_shape = rbppose.get_mean_shape(category_str) / 1000.0\n        mean_shape = torch.FloatTensor(mean_shape).unsqueeze(0).to(self._device)\n\n        # Prepare shape prior\n        mean_shape_pointset = self._mean_shape_pointsets[category_id]\n        shape_prior = (\n            torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n        )\n\n        # Prepare 2D coordinates\n        coord_2d = rbppose.get_2d_coord_np(width, height).transpose(1, 2, 0)\n        roi_coord_2d = rbppose.crop_resize_by_warp_affine(\n            coord_2d,\n            bbox_center,\n            scale,\n            rbppose.FLAGS.img_size,\n            interpolation=cv2.INTER_NEAREST,\n        ).transpose(2, 0, 1)\n        roi_coord_2d = torch.FloatTensor(roi_coord_2d).unsqueeze(0).to(self._device)\n\n        output_dict = self._net(\n            rgb=rgb_crop,\n            depth=depth_crop,\n            obj_id=category_input,\n            camK=camera_matrix,\n            def_mask=roi_mask,\n            mean_shape=mean_shape,\n            shape_prior=shape_prior,\n            gt_2D=roi_coord_2d,\n        )\n\n        p_green_R_vec = output_dict[\"p_green_R\"].detach().cpu()\n        p_red_R_vec = output_dict[\"p_red_R\"].detach().cpu()\n        p_T = output_dict[\"Pred_T\"].detach().cpu()\n        f_green_R = output_dict[\"f_green_R\"].detach().cpu()\n        f_red_R = output_dict[\"f_red_R\"].detach().cpu()\n        sym = torch.FloatTensor(rbppose.get_sym_info(category_str)).unsqueeze(0)\n        pred_RT = rbppose.generate_RT(\n            [p_green_R_vec, p_red_R_vec],\n            [f_green_R, f_red_R],\n            p_T,\n            mode=\"vec\",\n            sym=sym,\n        )[0]\n        position = output_dict[\"Pred_T\"][0].detach().cpu()\n        orientation_mat = pred_RT[:3, :3].detach().cpu()\n        orientation = Rotation.from_matrix(orientation_mat.numpy())\n        orientation_q = torch.FloatTensor(orientation.as_quat())\n        extents = output_dict[\"Pred_s\"][0].detach().cpu()\n        scale = torch.linalg.norm(extents)\n        reconstructed_points = output_dict[\"recon_model\"][0].detach().cpu()\n        reconstructed_points *= scale\n\n        # Recenter for mug category\n        if category_str == \"mug\":  # undo mug translation\n            x_offset = (\n                (\n                    self._mean_shape_pointsets[5].max(axis=0)[0]\n                    + self._mean_shape_pointsets[5].min(axis=0)[0]\n                )\n                / 2\n                * scale\n            )\n            reconstructed_points[:, 0] -= x_offset\n            position += quaternion_utils.quaternion_apply(\n                orientation_q, torch.FloatTensor([x_offset, 0, 0])\n            ).numpy()\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents = torch.FloatTensor([extents[2], extents[1], extents[0]])\n\n        return {\n            \"position\": position,\n            \"orientation\": orientation_q,\n            \"extents\": extents,\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for RBPPose.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>File path for model weights.</p> <p> TYPE: <code>str</code> </p> <code>model_url</code> <p>URL to download model weights if file is not found.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape</code> <p>File path for mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_url</code> <p>URL to download mean shape file if it is not found.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for RBPPose.\n\n    Attributes:\n        model: File path for model weights.\n        model_url: URL to download model weights if file is not found.\n        mean_shape: File path for mean shape file.\n        mean_shape_url: URL to download mean shape file if it is not found.\n        device: Device string for the model.\n    \"\"\"\n\n    model: str\n    model_url: str\n    mean_shape: str\n    mean_shape_url: str\n    device: str\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load RBPPose model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>RBPPose configuration. See RBPPose.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load RBPPose model.\n\n    Args:\n        config: RBPPose configuration. See RBPPose.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=RBPPose.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/rbppose/#cpas_toolbox.cpas_methods.rbppose.RBPPose.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/rbppose.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n    Based on https://github.com/lolrudy/RBP_Pose/blob/master/evaluation/evaluate.py\n    \"\"\"\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n\n    # Handle camera information\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(0)\n    width = self._camera.width\n    height = self._camera.height\n    camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n    camera_matrix = torch.FloatTensor(camera_matrix).unsqueeze(0).to(self._device)\n\n    # Prepare RGB crop (not used by default config)\n    rgb_cv = color_image.numpy()[:, :, ::-1]  # BGR\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    rmin, rmax, cmin, cmax = rbppose.get_bbox([y1, x1, y2, x2])\n    cx = 0.5 * (cmin + cmax)\n    cy = 0.5 * (rmin + rmax)\n    bbox_center = np.array([cx, cy])  # (w/2, h/2)\n    scale = min(max(cmax - cmin, rmax - rmin), max(height, width))\n    rgb_crop = rbppose.crop_resize_by_warp_affine(\n        rgb_cv,\n        bbox_center,\n        scale,\n        rbppose.FLAGS.img_size,\n        interpolation=cv2.INTER_NEAREST,\n    ).transpose(2, 0, 1)\n    rgb_crop = torch.FloatTensor(rgb_crop).unsqueeze(0).to(self._device)\n\n    # Prepare depth crop (expected in mm)\n    depth_cv = depth_image.numpy() * 1000\n    depth_crop = rbppose.crop_resize_by_warp_affine(\n        depth_cv,\n        bbox_center,\n        scale,\n        rbppose.FLAGS.img_size,\n        interpolation=cv2.INTER_NEAREST,\n    )\n    depth_crop = torch.FloatTensor(depth_crop)[None, None].to(self._device)\n\n    # Prepare category\n    category_input = torch.LongTensor([category_id]).to(self._device)\n\n    # Prepare ROI Mask\n    mask_np = instance_mask.float().numpy()\n    roi_mask = rbppose.crop_resize_by_warp_affine(\n        mask_np,\n        bbox_center,\n        scale,\n        rbppose.FLAGS.img_size,\n        interpolation=cv2.INTER_NEAREST,\n    )\n    roi_mask = torch.FloatTensor(roi_mask)[None, None].to(self._device)\n\n    # Prepare mean shape (size?)\n    mean_shape = rbppose.get_mean_shape(category_str) / 1000.0\n    mean_shape = torch.FloatTensor(mean_shape).unsqueeze(0).to(self._device)\n\n    # Prepare shape prior\n    mean_shape_pointset = self._mean_shape_pointsets[category_id]\n    shape_prior = (\n        torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n    )\n\n    # Prepare 2D coordinates\n    coord_2d = rbppose.get_2d_coord_np(width, height).transpose(1, 2, 0)\n    roi_coord_2d = rbppose.crop_resize_by_warp_affine(\n        coord_2d,\n        bbox_center,\n        scale,\n        rbppose.FLAGS.img_size,\n        interpolation=cv2.INTER_NEAREST,\n    ).transpose(2, 0, 1)\n    roi_coord_2d = torch.FloatTensor(roi_coord_2d).unsqueeze(0).to(self._device)\n\n    output_dict = self._net(\n        rgb=rgb_crop,\n        depth=depth_crop,\n        obj_id=category_input,\n        camK=camera_matrix,\n        def_mask=roi_mask,\n        mean_shape=mean_shape,\n        shape_prior=shape_prior,\n        gt_2D=roi_coord_2d,\n    )\n\n    p_green_R_vec = output_dict[\"p_green_R\"].detach().cpu()\n    p_red_R_vec = output_dict[\"p_red_R\"].detach().cpu()\n    p_T = output_dict[\"Pred_T\"].detach().cpu()\n    f_green_R = output_dict[\"f_green_R\"].detach().cpu()\n    f_red_R = output_dict[\"f_red_R\"].detach().cpu()\n    sym = torch.FloatTensor(rbppose.get_sym_info(category_str)).unsqueeze(0)\n    pred_RT = rbppose.generate_RT(\n        [p_green_R_vec, p_red_R_vec],\n        [f_green_R, f_red_R],\n        p_T,\n        mode=\"vec\",\n        sym=sym,\n    )[0]\n    position = output_dict[\"Pred_T\"][0].detach().cpu()\n    orientation_mat = pred_RT[:3, :3].detach().cpu()\n    orientation = Rotation.from_matrix(orientation_mat.numpy())\n    orientation_q = torch.FloatTensor(orientation.as_quat())\n    extents = output_dict[\"Pred_s\"][0].detach().cpu()\n    scale = torch.linalg.norm(extents)\n    reconstructed_points = output_dict[\"recon_model\"][0].detach().cpu()\n    reconstructed_points *= scale\n\n    # Recenter for mug category\n    if category_str == \"mug\":  # undo mug translation\n        x_offset = (\n            (\n                self._mean_shape_pointsets[5].max(axis=0)[0]\n                + self._mean_shape_pointsets[5].min(axis=0)[0]\n            )\n            / 2\n            * scale\n        )\n        reconstructed_points[:, 0] -= x_offset\n        position += quaternion_utils.quaternion_apply(\n            orientation_q, torch.FloatTensor([x_offset, 0, 0])\n        ).numpy()\n\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n    extents = torch.FloatTensor([extents[2], extents[1], extents[0]])\n\n    return {\n        \"position\": position,\n        \"orientation\": orientation_q,\n        \"extents\": extents,\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/","title":"sdfest.py","text":""},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest","title":"cpas_toolbox.cpas_methods.sdfest","text":"<p>This module defines SDFEst interface.</p> <p>Method is described in SDFEst: Categorical Pose and Shape Estimation of Objects From RGB-D Using Signed Distance Fields, Bruns, 2022.</p> <p>Implementation based on https://github.com/roym899/sdfest/</p>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst","title":"SDFEst","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for SDFEst.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>class SDFEst(CPASMethod):\n    \"\"\"Wrapper class for SDFEst.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for SDFEst.\n\n        All keys supported by SDFPipeline are supported and will overwrite config\n        contained in sdfest_... files. The keys specified here are used by this\n        script only.\n\n        The two keys sdfest_..._config_files  will be parsed with SDFEst install\n        directory as part of the search paths. This allows to use the default config\n        that comes with SDFEst installation.\n\n        Attributes:\n            sdfest_default_config_file: Default configuration file loaded first.\n            sdfest_category_config_files: Per-category configuration file loaded second.\n            device: Device used for computation.\n            num_points: Numbner of points extracted from mesh.\n            prior: Prior distribution to modify orientation distribution.\n            visualize_optimization:\n                Whether to show additional optimization visualization.\n        \"\"\"\n\n    default_config: Config = {\n        \"sdfest_default_config_file\": \"estimation/configs/default.yaml\",\n        \"sdfest_category_config_files\": {\n            \"bottle\": \"estimation/configs/models/bottle.yaml\",\n            \"bowl\": \"estimation/configs/models/bowl.yaml\",\n            \"laptop\": \"estimation/configs/models/laptop.yaml\",\n            \"can\": \"estimation/configs/models/can.yaml\",\n            \"camera\": \"estimation/configs/models/camera.yaml\",\n            \"mug\": \"estimation/configs/models/mug.yaml\",\n        },\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load SDFEst models.\n\n        Configuration loaded in following order\n            sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys\n        I.e., keys specified directly will take precedence over keys specified in\n        default file.\n        \"\"\"\n        default_config = yoco.load_config_from_file(\n            config[\"sdfest_default_config_file\"],\n            search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n        )\n\n        self._pipeline_dict = {}  # maps category to category-specific pipeline\n        self._device = config[\"device\"]\n        self._visualize_optimization = config[\"visualize_optimization\"]\n        self._num_points = config[\"num_points\"]\n        self._prior = config[\"prior\"] if \"prior\" in config else None\n\n        # create per-categry models\n        for category_str in config[\"sdfest_category_config_files\"].keys():\n            category_config = yoco.load_config_from_file(\n                config[\"sdfest_category_config_files\"][category_str],\n                current_dict=default_config,\n                search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n            )\n            category_config = yoco.load_config(config, category_config)\n            self._pipeline_dict[category_str] = SDFPipeline(category_config)\n            self._pipeline_dict[category_str].cam = camera\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See CPASMethod.inference.\"\"\"\n        # skip unsupported category\n        if category_str not in self._pipeline_dict:\n            return {\n                \"position\": torch.tensor([0, 0, 0]),\n                \"orientation\": torch.tensor([0, 0, 0, 1]),\n                \"extents\": torch.tensor([1, 1, 1]),\n                \"reconstructed_pointcloud\": torch.tensor([[0, 0, 0]]),\n                \"reconstructed_mesh\": None,\n            }\n\n        pipeline = self._pipeline_dict[category_str]\n\n        # move inputs to device\n        color_image = color_image.to(self._device)\n        depth_image = depth_image.to(self._device, copy=True)\n        instance_mask = instance_mask.to(self._device)\n\n        if self._prior is not None:\n            prior = torch.tensor(self._prior[category_str], device=self._device)\n            prior /= torch.sum(prior)\n        else:\n            prior = None\n\n        position, orientation, scale, shape = pipeline(\n            depth_image,\n            instance_mask,\n            color_image,\n            visualize=self._visualize_optimization,\n            prior_orientation_distribution=prior,\n        )\n\n        # outputs of SDFEst are OpenGL camera, ShapeNet object convention\n        position_cv = pointset_utils.change_position_camera_convention(\n            position[0], \"opengl\", \"opencv\"\n        )\n        orientation_cv = pointset_utils.change_orientation_camera_convention(\n            orientation[0], \"opengl\", \"opencv\"\n        )\n\n        # reconstruction + extent\n        mesh = pipeline.generate_mesh(shape, scale, True).get_transformed_o3d_geometry()\n        reconstructed_points = torch.from_numpy(\n            np.asarray(mesh.sample_points_uniformly(self._num_points).points)\n        )\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n        return {\n            \"position\": position_cv.detach().cpu(),\n            \"orientation\": orientation_cv.detach().cpu(),\n            \"extents\": extents,\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": mesh,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SDFEst.</p> <p>All keys supported by SDFPipeline are supported and will overwrite config contained in sdfest_... files. The keys specified here are used by this script only.</p> <p>The two keys sdfest_..._config_files  will be parsed with SDFEst install directory as part of the search paths. This allows to use the default config that comes with SDFEst installation.</p> ATTRIBUTE DESCRIPTION <code>sdfest_default_config_file</code> <p>Default configuration file loaded first.</p> <p> </p> <code>sdfest_category_config_files</code> <p>Per-category configuration file loaded second.</p> <p> </p> <code>device</code> <p>Device used for computation.</p> <p> </p> <code>num_points</code> <p>Numbner of points extracted from mesh.</p> <p> </p> <code>prior</code> <p>Prior distribution to modify orientation distribution.</p> <p> </p> <code>visualize_optimization</code> <p>Whether to show additional optimization visualization.</p> <p> </p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for SDFEst.\n\n    All keys supported by SDFPipeline are supported and will overwrite config\n    contained in sdfest_... files. The keys specified here are used by this\n    script only.\n\n    The two keys sdfest_..._config_files  will be parsed with SDFEst install\n    directory as part of the search paths. This allows to use the default config\n    that comes with SDFEst installation.\n\n    Attributes:\n        sdfest_default_config_file: Default configuration file loaded first.\n        sdfest_category_config_files: Per-category configuration file loaded second.\n        device: Device used for computation.\n        num_points: Numbner of points extracted from mesh.\n        prior: Prior distribution to modify orientation distribution.\n        visualize_optimization:\n            Whether to show additional optimization visualization.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SDFEst models.</p> <p>Configuration loaded in following order     sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys I.e., keys specified directly will take precedence over keys specified in default file.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load SDFEst models.\n\n    Configuration loaded in following order\n        sdfest_default_config_file -&gt; sdfest_category_config_files -&gt; all other keys\n    I.e., keys specified directly will take precedence over keys specified in\n    default file.\n    \"\"\"\n    default_config = yoco.load_config_from_file(\n        config[\"sdfest_default_config_file\"],\n        search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n    )\n\n    self._pipeline_dict = {}  # maps category to category-specific pipeline\n    self._device = config[\"device\"]\n    self._visualize_optimization = config[\"visualize_optimization\"]\n    self._num_points = config[\"num_points\"]\n    self._prior = config[\"prior\"] if \"prior\" in config else None\n\n    # create per-categry models\n    for category_str in config[\"sdfest_category_config_files\"].keys():\n        category_config = yoco.load_config_from_file(\n            config[\"sdfest_category_config_files\"][category_str],\n            current_dict=default_config,\n            search_paths=[\".\", \"~/.sdfest/\", sdfest.__path__[0]],\n        )\n        category_config = yoco.load_config(config, category_config)\n        self._pipeline_dict[category_str] = SDFPipeline(category_config)\n        self._pipeline_dict[category_str].cam = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/sdfest/#cpas_toolbox.cpas_methods.sdfest.SDFEst.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See CPASMethod.inference.</p> Source code in <code>cpas_toolbox/cpas_methods/sdfest.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See CPASMethod.inference.\"\"\"\n    # skip unsupported category\n    if category_str not in self._pipeline_dict:\n        return {\n            \"position\": torch.tensor([0, 0, 0]),\n            \"orientation\": torch.tensor([0, 0, 0, 1]),\n            \"extents\": torch.tensor([1, 1, 1]),\n            \"reconstructed_pointcloud\": torch.tensor([[0, 0, 0]]),\n            \"reconstructed_mesh\": None,\n        }\n\n    pipeline = self._pipeline_dict[category_str]\n\n    # move inputs to device\n    color_image = color_image.to(self._device)\n    depth_image = depth_image.to(self._device, copy=True)\n    instance_mask = instance_mask.to(self._device)\n\n    if self._prior is not None:\n        prior = torch.tensor(self._prior[category_str], device=self._device)\n        prior /= torch.sum(prior)\n    else:\n        prior = None\n\n    position, orientation, scale, shape = pipeline(\n        depth_image,\n        instance_mask,\n        color_image,\n        visualize=self._visualize_optimization,\n        prior_orientation_distribution=prior,\n    )\n\n    # outputs of SDFEst are OpenGL camera, ShapeNet object convention\n    position_cv = pointset_utils.change_position_camera_convention(\n        position[0], \"opengl\", \"opencv\"\n    )\n    orientation_cv = pointset_utils.change_orientation_camera_convention(\n        orientation[0], \"opengl\", \"opencv\"\n    )\n\n    # reconstruction + extent\n    mesh = pipeline.generate_mesh(shape, scale, True).get_transformed_o3d_geometry()\n    reconstructed_points = torch.from_numpy(\n        np.asarray(mesh.sample_points_uniformly(self._num_points).points)\n    )\n    extents, _ = reconstructed_points.abs().max(dim=0)\n    extents *= 2.0\n\n    return {\n        \"position\": position_cv.detach().cpu(),\n        \"orientation\": orientation_cv.detach().cpu(),\n        \"extents\": extents,\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": mesh,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/","title":"sgpa.py","text":""},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa","title":"cpas_toolbox.cpas_methods.sgpa","text":"<p>This module defines CR-Net interface.</p> <p>Method is described in SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation, Chen, 2021.</p> <p>Implementation based on https://github.com/ck-kai/SGPA</p>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA","title":"SGPA","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for SGPA.</p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>class SGPA(CPASMethod):\n    \"\"\"Wrapper class for SGPA.\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for SGPA.\n\n        Attributes:\n            model: Path to model.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            num_input_points: Number of 3D input points.\n            num_structure_points: Number of keypoints used for pose estimation.\n            image_size: Image size consumed by network (crop will be resized to this).\n            model: File path for model weights.\n            model_url: URL to download model weights if file is not found.\n            mean_shape: File path for mean shape file.\n            mean_shape_url: URL to download mean shape file if it is not found.\n            device: Device string for the model.\n        \"\"\"\n\n        model: str\n        num_categories: int\n        num_shape_points: int\n        num_input_points: int\n        num_structure_points: int\n        image_size: int\n        model: str\n        model_url: str\n        mean_shape: str\n        mean_shape_url: str\n        device: str\n\n    default_config: Config = {\n        \"model\": None,\n        \"num_categories\": None,\n        \"num_shape_points\": None,\n        \"num_input_points\": None,\n        \"num_structure_points\": None,\n        \"image_size\": None,\n        \"model\": None,\n        \"model_url\": None,\n        \"mean_shape\": None,\n        \"mean_shape_url\": None,\n        \"device\": \"cuda\",\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load SGPA model.\n\n        Args:\n            config: SGPA configuration. See SGPA.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=SGPA.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_path = utils.resolve_path(config[\"model\"])\n        self._model_url = config[\"model_url\"]\n        self._mean_shape_path = utils.resolve_path(config[\"mean_shape\"])\n        self._mean_shape_url = config[\"mean_shape_url\"]\n        self._check_paths()\n        self._sgpa = sgpa.SGPANet(\n            config[\"num_categories\"],\n            config[\"num_shape_points\"],\n            num_structure_points=config[\"num_structure_points\"],\n        )\n        self._sgpa.cuda()\n        self._sgpa = torch.nn.DataParallel(self._sgpa, device_ids=[self._device])\n        self._sgpa.load_state_dict(\n            torch.load(self._model_path, map_location=self._device)\n        )\n        self._sgpa.eval()\n        self._mean_shape_pointsets = np.load(self._mean_shape_path)\n        self._num_input_points = config[\"num_input_points\"]\n        self._image_size = config[\"image_size\"]\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_path) or not os.path.exists(\n            self._mean_shape_path\n        ):\n            print(\"SGPA model weights not found, do you want to download to \")\n            print(\"  \", self._model_path)\n            print(\"  \", self._mean_shape_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"SGPA model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_path):\n            os.makedirs(os.path.dirname(self._model_path), exist_ok=True)\n            utils.download(\n                self._model_url,\n                self._model_path,\n            )\n        if not os.path.exists(self._mean_shape_path):\n            os.makedirs(os.path.dirname(self._mean_shape_path), exist_ok=True)\n            utils.download(\n                self._mean_shape_url,\n                self._mean_shape_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n        Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n        \"\"\"\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n        mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n        # Get bounding box\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        rmin, rmax, cmin, cmax = sgpa.get_bbox([y1, x1, y2, x2])\n\n        valid_mask = (depth_image != 0) * instance_mask\n\n        # Prepare image crop\n        color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n        color_input = cv2.resize(\n            color_input,\n            (self._image_size, self._image_size),\n            interpolation=cv2.INTER_LINEAR,\n        )\n        color_input = TF.normalize(\n            TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n        # Prepare input points\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n        width = self._camera.width\n        height = self._camera.height\n        point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n        xmap = np.array([[i for i in range(width)] for _ in range(height)])\n        ymap = np.array([[j for _ in range(width)] for j in range(height)])\n        # if len(choose) &lt; 32:\n        #     f_sRT[i] = np.identity(4, dtype=float)\n        #     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n        #     continue\n        # else:\n        #     valid_inst.append(i)\n        if len(point_indices) &gt; self._num_input_points:\n            # take subset of points if two many depth points\n            point_indices_mask = np.zeros(len(point_indices), dtype=int)\n            point_indices_mask[: self._num_input_points] = 1\n            np.random.shuffle(point_indices_mask)\n            point_indices = point_indices[point_indices_mask.nonzero()]\n        else:\n            point_indices = np.pad(\n                point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n            )  # repeat points if not enough depth observation\n        depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n            :, None\n        ]\n        xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        pt2 = depth_masked.numpy()\n        pt0 = (xmap_masked - cx) * pt2 / fx\n        pt1 = (ymap_masked - cy) * pt2 / fy\n        points = np.concatenate((pt0, pt1, pt2), axis=1)\n        # adjust indices for resizing of color image\n        crop_w = rmax - rmin\n        ratio = self._image_size / crop_w\n        col_idx = point_indices % crop_w\n        row_idx = point_indices // crop_w\n        point_indices = (\n            np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n        ).astype(np.int64)\n\n        # Move inputs to device and convert to right shape\n        color_input = color_input.unsqueeze(0).to(self._device)\n        points = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\n        point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n        category_id = torch.cuda.LongTensor([category_id]).to(self._device)\n        mean_shape_pointset = (\n            torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n        )\n\n        # Call SGPA\n        _, assign_matrix, deltas = self._sgpa(\n            points,\n            color_input,\n            point_indices,\n            category_id,\n            mean_shape_pointset,\n        )\n\n        # Postprocess output\n        inst_shape = mean_shape_pointset + deltas\n        assign_matrix = torch.softmax(assign_matrix, dim=2)\n        coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n        point_indices = point_indices[0].cpu().numpy()\n        _, point_indices = np.unique(point_indices, return_index=True)\n        nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n        extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n        points = points[0, point_indices, :].cpu().numpy()\n        scale, orientation_m, position, _ = sgpa.estimateSimilarityTransform(\n            nocs_coords, points\n        )\n        orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n        reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n        # Recenter for mug category\n        if category_str == \"mug\":  # undo mug translation\n            x_offset = (\n                (\n                    self._mean_shape_pointsets[5].max(axis=0)[0]\n                    + self._mean_shape_pointsets[5].min(axis=0)[0]\n                )\n                / 2\n                * scale\n            )\n            reconstructed_points[:, 0] -= x_offset\n            position += quaternion_utils.quaternion_apply(\n                orientation_q, torch.FloatTensor([x_offset, 0, 0])\n            ).numpy()\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n        return {\n            \"position\": torch.Tensor(position),\n            \"orientation\": orientation_q,\n            \"extents\": torch.Tensor(extents),\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SGPA.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>num_input_points</code> <p>Number of 3D input points.</p> <p> TYPE: <code>int</code> </p> <code>num_structure_points</code> <p>Number of keypoints used for pose estimation.</p> <p> TYPE: <code>int</code> </p> <code>image_size</code> <p>Image size consumed by network (crop will be resized to this).</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>File path for model weights.</p> <p> TYPE: <code>str</code> </p> <code>model_url</code> <p>URL to download model weights if file is not found.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape</code> <p>File path for mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_url</code> <p>URL to download mean shape file if it is not found.</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for SGPA.\n\n    Attributes:\n        model: Path to model.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        num_input_points: Number of 3D input points.\n        num_structure_points: Number of keypoints used for pose estimation.\n        image_size: Image size consumed by network (crop will be resized to this).\n        model: File path for model weights.\n        model_url: URL to download model weights if file is not found.\n        mean_shape: File path for mean shape file.\n        mean_shape_url: URL to download mean shape file if it is not found.\n        device: Device string for the model.\n    \"\"\"\n\n    model: str\n    num_categories: int\n    num_shape_points: int\n    num_input_points: int\n    num_structure_points: int\n    image_size: int\n    model: str\n    model_url: str\n    mean_shape: str\n    mean_shape_url: str\n    device: str\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SGPA model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>SGPA configuration. See SGPA.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load SGPA model.\n\n    Args:\n        config: SGPA configuration. See SGPA.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=SGPA.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/sgpa/#cpas_toolbox.cpas_methods.sgpa.SGPA.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See cpas_toolbox.cpas_method.CPASMethod.inference.</p> <p>Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py</p> Source code in <code>cpas_toolbox/cpas_methods/sgpa.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See cpas_toolbox.cpas_method.CPASMethod.inference.\n\n    Based on https://github.com/JeremyWANGJZ/Category-6D-Pose/blob/main/evaluate.py\n    \"\"\"\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n    mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n    # Get bounding box\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    rmin, rmax, cmin, cmax = sgpa.get_bbox([y1, x1, y2, x2])\n\n    valid_mask = (depth_image != 0) * instance_mask\n\n    # Prepare image crop\n    color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n    color_input = cv2.resize(\n        color_input,\n        (self._image_size, self._image_size),\n        interpolation=cv2.INTER_LINEAR,\n    )\n    color_input = TF.normalize(\n        TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n\n    # Prepare input points\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n    width = self._camera.width\n    height = self._camera.height\n    point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n    xmap = np.array([[i for i in range(width)] for _ in range(height)])\n    ymap = np.array([[j for _ in range(width)] for j in range(height)])\n    # if len(choose) &lt; 32:\n    #     f_sRT[i] = np.identity(4, dtype=float)\n    #     f_size[i] = 2 * np.amax(np.abs(mean_shape_pointset), axis=0)\n    #     continue\n    # else:\n    #     valid_inst.append(i)\n    if len(point_indices) &gt; self._num_input_points:\n        # take subset of points if two many depth points\n        point_indices_mask = np.zeros(len(point_indices), dtype=int)\n        point_indices_mask[: self._num_input_points] = 1\n        np.random.shuffle(point_indices_mask)\n        point_indices = point_indices[point_indices_mask.nonzero()]\n    else:\n        point_indices = np.pad(\n            point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n        )  # repeat points if not enough depth observation\n    depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n        :, None\n    ]\n    xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    pt2 = depth_masked.numpy()\n    pt0 = (xmap_masked - cx) * pt2 / fx\n    pt1 = (ymap_masked - cy) * pt2 / fy\n    points = np.concatenate((pt0, pt1, pt2), axis=1)\n    # adjust indices for resizing of color image\n    crop_w = rmax - rmin\n    ratio = self._image_size / crop_w\n    col_idx = point_indices % crop_w\n    row_idx = point_indices // crop_w\n    point_indices = (\n        np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n    ).astype(np.int64)\n\n    # Move inputs to device and convert to right shape\n    color_input = color_input.unsqueeze(0).to(self._device)\n    points = torch.cuda.FloatTensor(points).unsqueeze(0).to(self._device)\n    point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n    category_id = torch.cuda.LongTensor([category_id]).to(self._device)\n    mean_shape_pointset = (\n        torch.FloatTensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n    )\n\n    # Call SGPA\n    _, assign_matrix, deltas = self._sgpa(\n        points,\n        color_input,\n        point_indices,\n        category_id,\n        mean_shape_pointset,\n    )\n\n    # Postprocess output\n    inst_shape = mean_shape_pointset + deltas\n    assign_matrix = torch.softmax(assign_matrix, dim=2)\n    coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n    point_indices = point_indices[0].cpu().numpy()\n    _, point_indices = np.unique(point_indices, return_index=True)\n    nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n    extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n    points = points[0, point_indices, :].cpu().numpy()\n    scale, orientation_m, position, _ = sgpa.estimateSimilarityTransform(\n        nocs_coords, points\n    )\n    orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n    reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n    # Recenter for mug category\n    if category_str == \"mug\":  # undo mug translation\n        x_offset = (\n            (\n                self._mean_shape_pointsets[5].max(axis=0)[0]\n                + self._mean_shape_pointsets[5].min(axis=0)[0]\n            )\n            / 2\n            * scale\n        )\n        reconstructed_points[:, 0] -= x_offset\n        position += quaternion_utils.quaternion_apply(\n            orientation_q, torch.FloatTensor([x_offset, 0, 0])\n        ).numpy()\n\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n    extents, _ = reconstructed_points.abs().max(dim=0)\n    extents *= 2.0\n\n    return {\n        \"position\": torch.Tensor(position),\n        \"orientation\": orientation_q,\n        \"extents\": torch.Tensor(extents),\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/","title":"spd.py","text":""},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd","title":"cpas_toolbox.cpas_methods.spd","text":"<p>This module defines SPD interface.</p> <p>Method is described in Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation, Tian, 2020.</p> <p>Implementation based on https://github.com/mentian/object-deformnet</p>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD","title":"SPD","text":"<p>             Bases: <code>CPASMethod</code></p> <p>Wrapper class for Shape Prior Deformation (SPD).</p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>class SPD(CPASMethod):\n    \"\"\"Wrapper class for Shape Prior Deformation (SPD).\"\"\"\n\n    class Config(TypedDict):\n        \"\"\"Configuration dictionary for SPD.\n\n        Attributes:\n            model_file: Path to model.\n            mean_shape_file: Path to mean shape file.\n            num_categories: Number of categories used by model.\n            num_shape_points: Number of points in shape prior.\n            num_input_points: Number of input points.\n            image_size: Size of image crop.\n            device: Device string for the model.\n        \"\"\"\n\n        model_file: str\n        mean_shape_file: str\n        num_categories: int\n        num_shape_points: int\n        num_input_points: int\n        image_size: int\n        device: str\n\n    default_config: Config = {\n        \"model_file\": None,\n        \"mean_shape_file\": None,\n        \"num_categories\": None,\n        \"num_shape_points\": None,\n        \"num_input_points\": None,\n        \"image_size\": None,\n        \"device\": \"cuda\",\n    }\n\n    def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n        \"\"\"Initialize and load SPD model.\n\n        Args:\n            config: SPD configuration. See SPD.Config for more information.\n            camera: Camera used for the input image.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=SPD.default_config)\n        self._parse_config(config)\n        self._camera = camera\n\n    def _parse_config(self, config: Config) -&gt; None:\n        self._device = config[\"device\"]\n        self._model_file_path = utils.resolve_path(config[\"model_file\"])\n        self._mean_shape_file_path = utils.resolve_path(config[\"mean_shape_file\"])\n        self._check_paths()\n        self._spd_net = spd.DeformNet(\n            config[\"num_categories\"], config[\"num_shape_points\"]\n        )\n        self._spd_net.to(self._device)\n        self._spd_net.load_state_dict(\n            torch.load(self._model_file_path, map_location=self._device)\n        )\n        self._spd_net.eval()\n        self._mean_shape_pointsets = np.load(self._mean_shape_file_path)\n        self._num_input_points = config[\"num_input_points\"]\n        self._image_size = config[\"image_size\"]\n\n    def _check_paths(self) -&gt; None:\n        if not os.path.exists(self._model_file_path) or not os.path.exists(\n            self._mean_shape_file_path\n        ):\n            print(\"SPD model weights not found, do you want to download to \")\n            print(\"  \", self._model_file_path)\n            print(\"  \", self._mean_shape_file_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_weights()\n                    break\n                elif decision == \"n\":\n                    print(\"SPD model weights not found. Aborting.\")\n                    exit(0)\n\n    def _download_weights(self) -&gt; None:\n        if not os.path.exists(self._model_file_path):\n            os.makedirs(os.path.dirname(self._model_file_path), exist_ok=True)\n            download_dir_path = os.path.dirname(self._model_file_path)\n            zip_path = os.path.join(download_dir_path, \"temp.zip\")\n            utils.download(\n                \"https://drive.google.com/u/0/uc?id=1p72NdY4Bie_sra9U8zoUNI4fTrQZdbnc&amp;\"\n                \"export=download\",\n                zip_path,\n            )\n            z = zipfile.ZipFile(zip_path)\n            z.extract(\"deformnet_eval/real/model_50.pth\", download_dir_path)\n            z.close()\n            os.remove(zip_path)\n            shutil.move(\n                os.path.join(\n                    download_dir_path, \"deformnet_eval\", \"real\", \"model_50.pth\"\n                ),\n                download_dir_path,\n            )\n            shutil.rmtree(os.path.join(download_dir_path, \"deformnet_eval\"))\n        if not os.path.exists(self._mean_shape_file_path):\n            os.makedirs(os.path.dirname(self._mean_shape_file_path), exist_ok=True)\n            utils.download(\n                \"https://github.com/mentian/object-deformnet/raw/master/assets/\"\n                \"mean_points_emb.npy\",\n                self._mean_shape_file_path,\n            )\n\n    def inference(\n        self,\n        color_image: torch.Tensor,\n        depth_image: torch.Tensor,\n        instance_mask: torch.Tensor,\n        category_str: str,\n    ) -&gt; PredictionDict:\n        \"\"\"See MethodWrapper.inference.\n\n        Based on spd.evaluate.\n        \"\"\"\n        category_str_to_id = {\n            \"bottle\": 0,\n            \"bowl\": 1,\n            \"camera\": 2,\n            \"can\": 3,\n            \"laptop\": 4,\n            \"mug\": 5,\n        }\n        category_id = category_str_to_id[category_str]\n        mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n        # get bounding box\n        x1 = min(instance_mask.nonzero()[:, 1]).item()\n        y1 = min(instance_mask.nonzero()[:, 0]).item()\n        x2 = max(instance_mask.nonzero()[:, 1]).item()\n        y2 = max(instance_mask.nonzero()[:, 0]).item()\n        rmin, rmax, cmin, cmax = spd.get_bbox([y1, x1, y2, x2])\n        bb_mask = torch.zeros_like(depth_image)\n        bb_mask[rmin:rmax, cmin:cmax] = 1.0\n\n        valid_mask = (depth_image != 0) * instance_mask\n\n        # prepare image crop\n        color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n        color_input = cv2.resize(\n            color_input,\n            (self._image_size, self._image_size),\n            interpolation=cv2.INTER_LINEAR,\n        )\n        color_input = TF.normalize(\n            TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n        color_input = color_input.unsqueeze(0)  # add batch dim\n\n        # convert depth to pointcloud\n        fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n        width = self._camera.width\n        height = self._camera.height\n        point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n        xmap = np.array([[i for i in range(width)] for _ in range(height)])\n        ymap = np.array([[j for _ in range(width)] for j in range(height)])\n        if len(point_indices) &gt; self._num_input_points:\n            # take subset of points if two many depth points\n            point_indices_mask = np.zeros(len(point_indices), dtype=int)\n            point_indices_mask[: self._num_input_points] = 1\n            np.random.shuffle(point_indices_mask)\n            point_indices = point_indices[point_indices_mask.nonzero()]\n        else:\n            point_indices = np.pad(\n                point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n            )  # repeat points if not enough depth observation\n        depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n            :, None\n        ]\n        xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n        pt2 = depth_masked.numpy()\n        pt0 = (xmap_masked - cx) * pt2 / fx\n        pt1 = (ymap_masked - cy) * pt2 / fy\n        points = np.concatenate((pt0, pt1, pt2), axis=1)\n        # adjust indices for resizing of color image\n        crop_w = rmax - rmin\n        ratio = self._image_size / crop_w\n        col_idx = point_indices % crop_w\n        row_idx = point_indices // crop_w\n        point_indices = (\n            np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n        ).astype(np.int64)\n\n        # move inputs to device\n        color_input = color_input.to(self._device)\n        points = torch.Tensor(points).unsqueeze(0).to(self._device)\n        point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n        category_id = torch.LongTensor([category_id]).to(self._device)\n        mean_shape_pointset = (\n            torch.Tensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n        )\n\n        # Call SPD network\n        assign_matrix, deltas = self._spd_net(\n            points, color_input, point_indices, category_id, mean_shape_pointset\n        )\n\n        # Postprocess outputs\n        inst_shape = mean_shape_pointset + deltas\n        assign_matrix = torch.softmax(assign_matrix, dim=2)\n        coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n        point_indices = point_indices[0].cpu().numpy()\n        _, point_indices = np.unique(point_indices, return_index=True)\n        nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n        extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n        points = points[0, point_indices, :].cpu().numpy()\n        scale, orientation_m, position, _ = spd.align.estimateSimilarityTransform(\n            nocs_coords, points\n        )\n        orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n        reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n        # Recenter for mug category\n        if category_str == \"mug\":  # undo mug translation\n            x_offset = (\n                (\n                    self._mean_shape_pointsets[5].max(axis=0)[0]\n                    + self._mean_shape_pointsets[5].min(axis=0)[0]\n                )\n                / 2\n                * scale\n            )\n            reconstructed_points[:, 0] -= x_offset\n            position += quaternion_utils.quaternion_apply(\n                orientation_q, torch.FloatTensor([x_offset, 0, 0])\n            ).numpy()\n\n        # NOCS Object -&gt; ShapeNet Object convention\n        obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n        orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n        reconstructed_points = quaternion_utils.quaternion_apply(\n            quaternion_utils.quaternion_invert(obj_fix),\n            reconstructed_points,\n        )\n        extents, _ = reconstructed_points.abs().max(dim=0)\n        extents *= 2.0\n\n        return {\n            \"position\": torch.Tensor(position),\n            \"orientation\": orientation_q,\n            \"extents\": torch.Tensor(extents),\n            \"reconstructed_pointcloud\": reconstructed_points,\n            \"reconstructed_mesh\": None,\n        }\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for SPD.</p> ATTRIBUTE DESCRIPTION <code>model_file</code> <p>Path to model.</p> <p> TYPE: <code>str</code> </p> <code>mean_shape_file</code> <p>Path to mean shape file.</p> <p> TYPE: <code>str</code> </p> <code>num_categories</code> <p>Number of categories used by model.</p> <p> TYPE: <code>int</code> </p> <code>num_shape_points</code> <p>Number of points in shape prior.</p> <p> TYPE: <code>int</code> </p> <code>num_input_points</code> <p>Number of input points.</p> <p> TYPE: <code>int</code> </p> <code>image_size</code> <p>Size of image crop.</p> <p> TYPE: <code>int</code> </p> <code>device</code> <p>Device string for the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>class Config(TypedDict):\n    \"\"\"Configuration dictionary for SPD.\n\n    Attributes:\n        model_file: Path to model.\n        mean_shape_file: Path to mean shape file.\n        num_categories: Number of categories used by model.\n        num_shape_points: Number of points in shape prior.\n        num_input_points: Number of input points.\n        image_size: Size of image crop.\n        device: Device string for the model.\n    \"\"\"\n\n    model_file: str\n    mean_shape_file: str\n    num_categories: int\n    num_shape_points: int\n    num_input_points: int\n    image_size: int\n    device: str\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.__init__","title":"__init__","text":"<pre><code>__init__(config: Config, camera: camera_utils.Camera) -&gt; None\n</code></pre> <p>Initialize and load SPD model.</p> PARAMETER  DESCRIPTION <code>config</code> <p>SPD configuration. See SPD.Config for more information.</p> <p> TYPE: <code>Config</code> </p> <code>camera</code> <p>Camera used for the input image.</p> <p> TYPE: <code>Camera</code> </p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>def __init__(self, config: Config, camera: camera_utils.Camera) -&gt; None:\n    \"\"\"Initialize and load SPD model.\n\n    Args:\n        config: SPD configuration. See SPD.Config for more information.\n        camera: Camera used for the input image.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=SPD.default_config)\n    self._parse_config(config)\n    self._camera = camera\n</code></pre>"},{"location":"api_reference/cpas_methods/spd/#cpas_toolbox.cpas_methods.spd.SPD.inference","title":"inference","text":"<pre><code>inference(\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict\n</code></pre> <p>See MethodWrapper.inference.</p> <p>Based on spd.evaluate.</p> Source code in <code>cpas_toolbox/cpas_methods/spd.py</code> <pre><code>def inference(\n    self,\n    color_image: torch.Tensor,\n    depth_image: torch.Tensor,\n    instance_mask: torch.Tensor,\n    category_str: str,\n) -&gt; PredictionDict:\n    \"\"\"See MethodWrapper.inference.\n\n    Based on spd.evaluate.\n    \"\"\"\n    category_str_to_id = {\n        \"bottle\": 0,\n        \"bowl\": 1,\n        \"camera\": 2,\n        \"can\": 3,\n        \"laptop\": 4,\n        \"mug\": 5,\n    }\n    category_id = category_str_to_id[category_str]\n    mean_shape_pointset = self._mean_shape_pointsets[category_id]\n\n    # get bounding box\n    x1 = min(instance_mask.nonzero()[:, 1]).item()\n    y1 = min(instance_mask.nonzero()[:, 0]).item()\n    x2 = max(instance_mask.nonzero()[:, 1]).item()\n    y2 = max(instance_mask.nonzero()[:, 0]).item()\n    rmin, rmax, cmin, cmax = spd.get_bbox([y1, x1, y2, x2])\n    bb_mask = torch.zeros_like(depth_image)\n    bb_mask[rmin:rmax, cmin:cmax] = 1.0\n\n    valid_mask = (depth_image != 0) * instance_mask\n\n    # prepare image crop\n    color_input = color_image[rmin:rmax, cmin:cmax, :].numpy()  # bb crop\n    color_input = cv2.resize(\n        color_input,\n        (self._image_size, self._image_size),\n        interpolation=cv2.INTER_LINEAR,\n    )\n    color_input = TF.normalize(\n        TF.to_tensor(color_input),  # (H, W, C) -&gt; (C, H, W), RGB\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n    color_input = color_input.unsqueeze(0)  # add batch dim\n\n    # convert depth to pointcloud\n    fx, fy, cx, cy, _ = self._camera.get_pinhole_camera_parameters(pixel_center=0.0)\n    width = self._camera.width\n    height = self._camera.height\n    point_indices = valid_mask[rmin:rmax, cmin:cmax].numpy().flatten().nonzero()[0]\n    xmap = np.array([[i for i in range(width)] for _ in range(height)])\n    ymap = np.array([[j for _ in range(width)] for j in range(height)])\n    if len(point_indices) &gt; self._num_input_points:\n        # take subset of points if two many depth points\n        point_indices_mask = np.zeros(len(point_indices), dtype=int)\n        point_indices_mask[: self._num_input_points] = 1\n        np.random.shuffle(point_indices_mask)\n        point_indices = point_indices[point_indices_mask.nonzero()]\n    else:\n        point_indices = np.pad(\n            point_indices, (0, self._num_input_points - len(point_indices)), \"wrap\"\n        )  # repeat points if not enough depth observation\n    depth_masked = depth_image[rmin:rmax, cmin:cmax].flatten()[point_indices][\n        :, None\n    ]\n    xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[point_indices][:, None]\n    pt2 = depth_masked.numpy()\n    pt0 = (xmap_masked - cx) * pt2 / fx\n    pt1 = (ymap_masked - cy) * pt2 / fy\n    points = np.concatenate((pt0, pt1, pt2), axis=1)\n    # adjust indices for resizing of color image\n    crop_w = rmax - rmin\n    ratio = self._image_size / crop_w\n    col_idx = point_indices % crop_w\n    row_idx = point_indices // crop_w\n    point_indices = (\n        np.floor(row_idx * ratio) * self._image_size + np.floor(col_idx * ratio)\n    ).astype(np.int64)\n\n    # move inputs to device\n    color_input = color_input.to(self._device)\n    points = torch.Tensor(points).unsqueeze(0).to(self._device)\n    point_indices = torch.LongTensor(point_indices).unsqueeze(0).to(self._device)\n    category_id = torch.LongTensor([category_id]).to(self._device)\n    mean_shape_pointset = (\n        torch.Tensor(mean_shape_pointset).unsqueeze(0).to(self._device)\n    )\n\n    # Call SPD network\n    assign_matrix, deltas = self._spd_net(\n        points, color_input, point_indices, category_id, mean_shape_pointset\n    )\n\n    # Postprocess outputs\n    inst_shape = mean_shape_pointset + deltas\n    assign_matrix = torch.softmax(assign_matrix, dim=2)\n    coords = torch.bmm(assign_matrix, inst_shape)  # (1, n_pts, 3)\n\n    point_indices = point_indices[0].cpu().numpy()\n    _, point_indices = np.unique(point_indices, return_index=True)\n    nocs_coords = coords[0, point_indices, :].detach().cpu().numpy()\n    extents = 2 * np.amax(np.abs(inst_shape[0].detach().cpu().numpy()), axis=0)\n    points = points[0, point_indices, :].cpu().numpy()\n    scale, orientation_m, position, _ = spd.align.estimateSimilarityTransform(\n        nocs_coords, points\n    )\n    orientation_q = torch.Tensor(Rotation.from_matrix(orientation_m).as_quat())\n\n    reconstructed_points = inst_shape[0].detach().cpu() * scale\n\n    # Recenter for mug category\n    if category_str == \"mug\":  # undo mug translation\n        x_offset = (\n            (\n                self._mean_shape_pointsets[5].max(axis=0)[0]\n                + self._mean_shape_pointsets[5].min(axis=0)[0]\n            )\n            / 2\n            * scale\n        )\n        reconstructed_points[:, 0] -= x_offset\n        position += quaternion_utils.quaternion_apply(\n            orientation_q, torch.FloatTensor([x_offset, 0, 0])\n        ).numpy()\n\n    # NOCS Object -&gt; ShapeNet Object convention\n    obj_fix = torch.tensor([0.0, -1 / np.sqrt(2.0), 0.0, 1 / np.sqrt(2.0)])\n    orientation_q = quaternion_utils.quaternion_multiply(orientation_q, obj_fix)\n    reconstructed_points = quaternion_utils.quaternion_apply(\n        quaternion_utils.quaternion_invert(obj_fix),\n        reconstructed_points,\n    )\n    extents, _ = reconstructed_points.abs().max(dim=0)\n    extents *= 2.0\n\n    return {\n        \"position\": torch.Tensor(position),\n        \"orientation\": orientation_q,\n        \"extents\": torch.Tensor(extents),\n        \"reconstructed_pointcloud\": reconstructed_points,\n        \"reconstructed_mesh\": None,\n    }\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/","title":"nocs_dataset.py","text":""},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset","title":"cpas_toolbox.datasets.nocs_dataset","text":"<p>Module providing dataset class for NOCS datasets (CAMERA / REAL).</p>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset","title":"NOCSDataset","text":"<p>             Bases: <code>Dataset</code></p> <p>Dataset class for NOCS dataset.</p> <p>CAMERA and REAL are training sets. CAMERA25 and REAL275 are test data. Some papers use CAMERA25 as validation when benchmarking REAL275.</p> <p>Datasets can be found here: https://github.com/hughw19/NOCS_CVPR2019/tree/master</p> Expected directory format <p>{root_dir}/real_train/... {root_dir}/real_test/... {root_dir}/gts/... {root_dir}/obj_models/... {root_dir}/camera_composed_depth/... {root_dir}/val/... {root_dir}/train/... {root_dir}/fixed_real_test_obj_models/...</p> <p>Which is easily obtained by downloading all the provided files and extracting them into the same directory.</p> <p>Necessary preprocessing of this data is performed during first initialization per and is saved to     {root_dir}/cpas_toolbox/...</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class NOCSDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset class for NOCS dataset.\n\n    CAMERA* and REAL* are training sets.\n    CAMERA25 and REAL275 are test data.\n    Some papers use CAMERA25 as validation when benchmarking REAL275.\n\n    Datasets can be found here:\n    https://github.com/hughw19/NOCS_CVPR2019/tree/master\n\n    Expected directory format:\n        {root_dir}/real_train/...\n        {root_dir}/real_test/...\n        {root_dir}/gts/...\n        {root_dir}/obj_models/...\n        {root_dir}/camera_composed_depth/...\n        {root_dir}/val/...\n        {root_dir}/train/...\n        {root_dir}/fixed_real_test_obj_models/...\n    Which is easily obtained by downloading all the provided files and extracting them\n    into the same directory.\n\n    Necessary preprocessing of this data is performed during first initialization per\n    and is saved to\n        {root_dir}/cpas_toolbox/...\n    \"\"\"\n\n    num_categories = 7\n    category_id_to_str = {\n        0: \"unknown\",\n        1: \"bottle\",\n        2: \"bowl\",\n        3: \"camera\",\n        4: \"can\",\n        5: \"laptop\",\n        6: \"mug\",\n    }\n    category_str_to_id = {v: k for k, v in category_id_to_str.items()}\n\n    class Config(TypedDict, total=False):\n        \"\"\"Configuration dictionary for NOCSDataset.\n\n        Attributes:\n            root_dir: See NOCSDataset docstring.\n            split:\n                The dataset split. The following strings are supported:\n                    \"camera_train\": 275000 images, synthetic objects + real background\n                    \"camera_val\": 25000 images, synthetic objects + real background\n                    \"real_train\": 4300 images in 7 scenes, real\n                    \"real_test\": 2750 images in 6 scenes, real\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. Currently only \"quaternion\"\n                supported.\n            remap_y_axis:\n                If not None, the NOCS y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See NOCSDataset.category_id_to_str for admissible category strings.\n        \"\"\"\n\n        root_dir: str\n        split: str\n        mask_pointcloud: bool\n        normalize_pointcloud: bool\n        scale_convention: str\n        camera_convention: str\n        orientation_repr: str\n        remap_y_axis: Optional[str]\n        remap_x_axis: Optional[str]\n        category_str: Optional[str]\n\n    default_config: Config = {\n        \"root_dir\": None,\n        \"split\": None,\n        \"mask_pointcloud\": False,\n        \"normalize_pointcloud\": False,\n        \"camera_convention\": \"opengl\",\n        \"scale_convention\": \"half_max\",\n        \"orientation_repr\": \"quaternion\",\n        \"category_str\": None,\n        \"remap_y_axis\": None,\n        \"remap_x_axis\": None,\n    }\n\n    def __init__(\n        self,\n        config: Config,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See NOCSDataset.Config for supported keys.\n        \"\"\"\n        config = yoco.load_config(config, current_dict=NOCSDataset.default_config)\n        self._root_dir_path = utils.resolve_path(config[\"root_dir\"])\n        self._split = config[\"split\"]\n        self._check_dirs()\n        self._camera_convention = config[\"camera_convention\"]\n        self._camera = self._get_split_camera()\n        self._preprocess_path = os.path.join(\n            self._root_dir_path, \"cpas_toolbox\", self._split\n        )\n        if not os.path.isdir(self._preprocess_path):\n            self._preprocess_dataset()\n        self._mask_pointcloud = config[\"mask_pointcloud\"]\n        self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n        self._scale_convention = config[\"scale_convention\"]\n        self._sample_files = self._get_sample_files(config[\"category_str\"])\n        self._remap_y_axis = config[\"remap_y_axis\"]\n        self._remap_x_axis = config[\"remap_x_axis\"]\n        self._orientation_repr = config[\"orientation_repr\"]\n\n    def _check_dirs(self) -&gt; None:\n        directories = self._get_dirs()\n\n        # required directories\n        if all(os.path.exists(directory) for directory in directories):\n            pass\n        else:\n            print(\n                f\"NOCS dataset ({self._split} split) not found, do you want to download\"\n                \" it into the following directory:\"\n            )\n            print(\"  \", self._root_dir_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_dataset()\n                    break\n                elif decision == \"n\":\n                    print(\"Dataset not found. Aborting.\")\n                    exit(0)\n\n    def _get_dirs(self) -&gt; List[str]:\n        \"\"\"Return required list of directories for current split.\"\"\"\n        dirs = []\n\n        # gts only available for real test and camera val\n        if self._split in [\"real_test\", \"camera_val\"]:\n            dirs.append(os.path.join(self._root_dir_path, \"gts\"))\n\n        dirs.append(os.path.join(self._root_dir_path, \"obj_models\"))\n\n        # Fixed object model, need to be downloaded separately\n        if self._split == \"real_test\":\n            dirs.append(os.path.join(self._root_dir_path, \"fixed_real_test_obj_models\"))\n\n        # full depths for CAMERA\n        if self._split in [\"camera_val\", \"camera_train\"]:\n            dirs.append(os.path.join(self._root_dir_path, \"camera_full_depths\"))\n\n        if self._split == \"camera_train\":\n            dirs.append(os.path.join(self._root_dir_path, \"train\"))\n        elif self._split == \"camera_val\":\n            dirs.append(os.path.join(self._root_dir_path, \"val\"))\n        elif self._split in [\"real_train\", \"real_test\"]:\n            dirs.append(os.path.join(self._root_dir_path, self._split))\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n        return dirs\n\n    def _download_dataset(self) -&gt; None:\n        missing_dirs = [d for d in self._get_dirs() if not os.path.exists(d)]\n        for missing_dir in missing_dirs:\n            download_dir, identifier = os.path.split(missing_dir)\n            os.makedirs(download_dir, exist_ok=True)\n            if identifier == \"gts\":\n                zip_path = os.path.join(download_dir, \"gts.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/gts.zip\", zip_path\n                )\n            elif identifier == \"obj_models\":\n                zip_path = os.path.join(download_dir, \"obj_models.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/obj_models.zip\",\n                    zip_path,\n                )\n            elif identifier == \"camera_full_depths\":\n                zip_path = os.path.join(download_dir, \"camera_full_depths.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/camera_composed_depth.zip\",\n                    zip_path,\n                )\n                z = zipfile.ZipFile(zip_path)\n            elif identifier == \"train\":\n                zip_path = os.path.join(download_dir, \"train.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/camera_train.zip\",\n                    zip_path,\n                )\n            elif identifier == \"val\":\n                zip_path = os.path.join(download_dir, \"val.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/camera_val25K.zip\",\n                    zip_path,\n                )\n            elif identifier == \"real_train\":\n                zip_path = os.path.join(download_dir, \"real_train.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/real_train.zip\",\n                    zip_path,\n                )\n            elif identifier == \"real_test\":\n                zip_path = os.path.join(download_dir, \"real_test.zip\")\n                utils.download(\n                    \"http://download.cs.stanford.edu/orion/nocs/real_test.zip\", zip_path\n                )\n            elif identifier == \"fixed_real_test_obj_models\":\n                zip_path = os.path.join(download_dir, \"fixed_real_test_obj_models.zip\")\n                utils.download(\n                    \"https://github.com/roym899/pose_and_shape_evaluation/releases/download/v1.0.0/fixed_real_test_obj_models.zip\",\n                    zip_path,\n                )\n            else:\n                raise ValueError(f\"Downloading dir {missing_dir} unsupported.\")\n            z = zipfile.ZipFile(zip_path)\n            z.extractall(download_dir)\n            z.close()\n            os.remove(zip_path)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of sample in dataset.\"\"\"\n        return len(self._sample_files)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"Return a sample of the dataset.\n\n        Args:\n            idx: Index of the instance.\n\n        Returns:\n            Sample containing the following items:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n        \"\"\"\n        sample_file = self._sample_files[idx]\n        sample_data = pickle.load(open(sample_file, \"rb\"))\n        sample = self._sample_from_sample_data(sample_data)\n        return sample\n\n    def _preprocess_dataset(self) -&gt; None:\n        \"\"\"Create preprocessing files for the current split.\n\n        One file per sample, which currently means per valid object mask will be\n        created.\n\n        Preprocessing will be stored on disk to {root_dir}/cpas_toolbox/...\n        This function will not store the preprocessing, so it still has to be loaded\n        afterwards.\n        \"\"\"\n        os.makedirs(self._preprocess_path)\n\n        self._fix_obj_models()\n\n        self._start_time = time.time()\n        self._color_paths = self._get_color_files()\n\n        Parallel(n_jobs=-1)(\n            (\n                delayed(self._preprocess_color_path)(i, color_path)\n                for i, color_path in enumerate(self._color_paths)\n            )\n        )\n\n        # store dictionary to map category to files\n        sample_files = self._get_sample_files()\n        category_str_to_files = {\n            category_str: [] for category_str in NOCSDataset.category_id_to_str.values()\n        }\n        for sample_file in tqdm(sample_files):\n            sample_data = pickle.load(open(sample_file, \"rb\"))\n            category_id = sample_data[\"category_id\"]\n            category_str = NOCSDataset.category_id_to_str[category_id]\n            _, file_name = os.path.split(sample_file)\n            category_str_to_files[category_str].append(file_name)\n\n        category_json_path = os.path.join(self._preprocess_path, \"categories.json\")\n        with open(category_json_path, \"w\") as f:\n            json.dump(dict(category_str_to_files), f)\n\n        print(f\"Finished preprocessing for {self._split}.\", end=\"\\033[K\\n\")\n\n    def _fix_obj_models(self) -&gt; None:\n        \"\"\"Fix issues with fileextensions.\n\n        Some png files have jpg extension. This function fixes these models.\n        \"\"\"\n        glob_pattern = os.path.join(self._root_dir_path, \"**\", \"*.jpg\")\n        files = glob(glob_pattern, recursive=True)\n        for filepath in files:\n            what = imghdr.what(filepath)\n            if what == \"png\":\n                print(\"Fixing: \", filepath)\n                obj_dir_path, problematic_filename = os.path.split(filepath)\n                name, _ = problematic_filename.split(\".\")\n                fixed_filename = f\"fixed_{name}.png\"\n                fixed_filepath = os.path.join(obj_dir_path, fixed_filename)\n\n                mtl_filepath = os.path.join(obj_dir_path, \"model.mtl\")\n                bu_mtl_filepath = os.path.join(obj_dir_path, \"model.mtl.old\")\n                copyfile(mtl_filepath, bu_mtl_filepath)\n\n                copyfile(filepath, fixed_filepath)\n\n    def _update_preprocess_progress(self, image_id: int) -&gt; None:\n        current_time = time.time()\n        duration = current_time - self._start_time\n        imgs_per_sec = image_id / duration\n        if image_id &gt; 10:\n            remaining_imgs = len(self._color_paths) - image_id\n            remaining_secs = remaining_imgs / imgs_per_sec\n            remaining_time_str = str(datetime.timedelta(seconds=round(remaining_secs)))\n        else:\n            remaining_time_str = \"N/A\"\n        print(\n            f\"Preprocessing image: {image_id:&gt;10} / {len(self._color_paths)}\"\n            f\" {image_id / len(self._color_paths) * 100:&gt;6.2f}%\"  # progress percentage\n            f\" Remaining time: {remaining_time_str}\"  # remaining time\n            \"\\033[K\",  # clear until end of line\n            end=\"\\r\",  # overwrite previous\n        )\n\n    def _preprocess_color_path(self, image_id: int, color_path: str) -&gt; None:\n        counter = 0\n        self._update_preprocess_progress(image_id)\n\n        depth_path = self._depth_path_from_color_path(color_path)\n        if not os.path.isfile(depth_path):\n            print(f\"Missing depth file {depth_path}. Skipping.\", end=\"\\033[K\\n\")\n            return\n\n        mask_path = self._mask_path_from_color_path(color_path)\n        meta_path = self._meta_path_from_color_path(color_path)\n        meta_data = pd.read_csv(\n            meta_path, sep=\" \", header=None, converters={2: lambda x: str(x)}\n        )\n        instances_mask = self._load_mask(mask_path)\n        mask_ids = np.unique(instances_mask).tolist()\n        gt_id = 0  # GT only contains valid objects of interests and is 0-indexed\n        for mask_id in mask_ids:\n\n            if mask_id == 255:  # 255 is background\n                continue\n            match = meta_data[meta_data.iloc[:, 0] == mask_id]\n            if match.empty:\n                print(\n                    f\"Warning: mask {mask_id} not found in {meta_path}\", end=\"\\033[K\\n\"\n                )\n            elif match.shape[0] != 1:\n                print(\n                    f\"Warning: mask {mask_id} not unique in {meta_path}\", end=\"\\033[K\\n\"\n                )\n\n            meta_row = match.iloc[0]\n            category_id = meta_row.iloc[1]\n            if category_id == 0:  # unknown / distractor object\n                continue\n\n            try:\n                (\n                    position,\n                    orientation_q,\n                    extents,\n                    nocs_transform,\n                ) = self._get_pose_and_scale(color_path, mask_id, gt_id, meta_row)\n            except nocs_utils.PoseEstimationError:\n                print(\n                    \"Insufficient data for pose estimation. \"\n                    f\"Skipping {color_path}:{mask_id}.\",\n                    end=\"\\033[K\\n\",\n                )\n                continue\n            except ObjectError:\n                print(\n                    \"Insufficient object mesh for pose estimation. \"\n                    f\"Skipping {color_path}:{mask_id}.\",\n                    end=\"\\033[K\\n\",\n                )\n                continue\n\n            obj_path = self._get_obj_path(meta_row)\n            sample_info = {\n                \"color_path\": color_path,\n                \"depth_path\": depth_path,\n                \"mask_path\": mask_path,\n                \"mask_id\": mask_id,\n                \"category_id\": category_id,\n                \"obj_path\": obj_path,\n                \"nocs_transform\": nocs_transform,\n                \"position\": position,\n                \"orientation_q\": orientation_q,\n                \"extents\": extents,\n                \"nocs_scale\": torch.linalg.norm(extents),\n                \"max_extent\": torch.max(extents),\n            }\n            out_file = os.path.join(\n                self._preprocess_path, f\"{image_id:08}_{counter}.pkl\"\n            )\n            pickle.dump(sample_info, open(out_file, \"wb\"))\n            counter += 1\n            gt_id += 1\n\n    def _get_color_files(self) -&gt; list:\n        \"\"\"Return list of paths of color images of the selected split.\"\"\"\n        if self._split == \"camera_train\":\n            glob_pattern = os.path.join(\n                self._root_dir_path, \"train\", \"**\", \"*_color.png\"\n            )\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"camera_val\":\n            glob_pattern = os.path.join(self._root_dir_path, \"val\", \"**\", \"*_color.png\")\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"real_train\":\n            glob_pattern = os.path.join(\n                self._root_dir_path, \"real_train\", \"**\", \"*_color.png\"\n            )\n            return sorted(glob(glob_pattern, recursive=True))\n        elif self._split == \"real_test\":\n            glob_pattern = os.path.join(\n                self._root_dir_path, \"real_test\", \"**\", \"*_color.png\"\n            )\n            return sorted(glob(glob_pattern, recursive=True))\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n    def _get_sample_files(self, category_str: Optional[str] = None) -&gt; list:\n        \"\"\"Return sorted list of sample file paths.\n\n        Sample files are generated by NOCSDataset._preprocess_dataset.\n\n        Args:\n            category_str:\n                If not None, only instances of the provided category will be returned.\n\n        Returns:\n            List of sample_data files.\n        \"\"\"\n        glob_pattern = os.path.join(self._preprocess_path, \"*.pkl\")\n        sample_files = glob(glob_pattern)\n        sample_files.sort()\n        if category_str is None:\n            return sample_files\n        if category_str not in NOCSDataset.category_str_to_id:\n            raise ValueError(f\"Unsupported category_str {category_str}.\")\n\n        category_json_path = os.path.join(self._preprocess_path, \"categories.json\")\n        with open(category_json_path, \"r\") as f:\n            category_str_to_filenames = json.load(f)\n        filtered_sample_files = [\n            os.path.join(self._preprocess_path, fn)\n            for fn in category_str_to_filenames[category_str]\n        ]\n        return filtered_sample_files\n\n    def _get_split_camera(self) -&gt; None:\n        \"\"\"Return camera information for selected split.\"\"\"\n        # from: https://github.com/hughw19/NOCS_CVPR2019/blob/master/detect_eval.py\n        if self._split in [\"real_train\", \"real_test\"]:\n            return camera_utils.Camera(\n                width=640,\n                height=480,\n                fx=591.0125,\n                fy=590.16775,\n                cx=322.525,\n                cy=244.11084,\n                pixel_center=0.0,\n            )\n        elif self._split in [\"camera_train\", \"camera_val\"]:\n            return camera_utils.Camera(\n                width=640,\n                height=480,\n                fx=577.5,\n                fy=577.5,\n                cx=319.5,\n                cy=239.5,\n                pixel_center=0.0,\n            )\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n    def _sample_from_sample_data(self, sample_data: dict) -&gt; dict:\n        \"\"\"Create dictionary containing a single sample.\"\"\"\n        color = torch.from_numpy(\n            np.asarray(Image.open(sample_data[\"color_path\"]), dtype=np.float32) / 255\n        )\n        depth = self._load_depth(sample_data[\"depth_path\"])\n        instances_mask = self._load_mask(sample_data[\"mask_path\"])\n        instance_mask = instances_mask == sample_data[\"mask_id\"]\n\n        pointcloud_mask = instance_mask if self._mask_pointcloud else None\n        pointcloud = pointset_utils.depth_to_pointcloud(\n            depth,\n            self._camera,\n            mask=pointcloud_mask,\n            convention=self._camera_convention,\n        )\n\n        # adjust camera convention for position, orientation and scale\n        position = pointset_utils.change_position_camera_convention(\n            sample_data[\"position\"], \"opencv\", self._camera_convention\n        )\n\n        # orientation / scale\n        orientation_q, extents = self._change_axis_convention(\n            sample_data[\"orientation_q\"], sample_data[\"extents\"]\n        )\n        orientation_q = pointset_utils.change_orientation_camera_convention(\n            orientation_q, \"opencv\", self._camera_convention\n        )\n        orientation = self._quat_to_orientation_repr(orientation_q)\n        scale = self._get_scale(sample_data, extents)\n\n        # normalize pointcloud &amp; position\n        if self._normalize_pointcloud:\n            pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n            position = position - centroid\n\n        sample = {\n            \"color\": color,\n            \"depth\": depth,\n            \"pointset\": pointcloud,\n            \"mask\": instance_mask,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"quaternion\": orientation_q,\n            \"scale\": scale,\n            \"color_path\": sample_data[\"color_path\"],\n            \"obj_path\": sample_data[\"obj_path\"],\n            \"category_id\": sample_data[\"category_id\"],\n            \"category_str\": NOCSDataset.category_id_to_str[sample_data[\"category_id\"]],\n        }\n        return sample\n\n    def _depth_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to depth file from color filepath.\"\"\"\n        if self._split in [\"real_train\", \"real_test\"]:\n            depth_path = color_path.replace(\"color\", \"depth\")\n        elif self._split in [\"camera_train\"]:\n            depth_path = color_path.replace(\"color\", \"composed\")\n            depth_path = depth_path.replace(\"/train/\", \"/camera_full_depths/train/\")\n        elif self._split in [\"camera_val\"]:\n            depth_path = color_path.replace(\"color\", \"composed\")\n            depth_path = depth_path.replace(\"/val/\", \"/camera_full_depths/val/\")\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n        return depth_path\n\n    def _mask_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to mask file from color filepath.\"\"\"\n        mask_path = color_path.replace(\"color\", \"mask\")\n        return mask_path\n\n    def _meta_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return path to meta file from color filepath.\"\"\"\n        meta_path = color_path.replace(\"color.png\", \"meta.txt\")\n        return meta_path\n\n    def _nocs_map_path_from_color_path(self, color_path: str) -&gt; str:\n        \"\"\"Return NOCS map filepath from color filepath.\"\"\"\n        nocs_map_path = color_path.replace(\"color.png\", \"coord.png\")\n        return nocs_map_path\n\n    def _get_pose_and_scale(\n        self, color_path: str, mask_id: int, gt_id: int, meta_row: pd.Series\n    ) -&gt; tuple:\n        \"\"\"Return position, orientation, scale and NOCS transform.\n\n        All of those follow OpenCV (x right, y down, z forward) convention.\n\n        Args:\n            color_path: Path to the color file.\n            mask_id: Instance id in the instances mask.\n            gt_id:\n                Ground truth id. This is 0-indexed id of valid instances in meta file.\n            meta_row:\n                Matching row of meta file. Contains necessary information about mesh.\n\n        Returns:\n            position (torch.Tensor):\n                Position of object center in camera frame. Shape (3,).\n            quaternion (torch.Tensor):\n                Orientation of object in camera frame.\n                Scalar-last quaternion, shape (4,).\n            extents (torch.Tensor):\n                Bounding box side lengths.\n            nocs_transformation (torch.Tensor):\n                Transformation from centered [-0.5,0.5]^3 NOCS coordinates to camera.\n        \"\"\"\n        gts_path = self._get_gts_path(color_path)\n        obj_path = self._get_obj_path(meta_row)\n        if self._split == \"real_test\":\n            # only use gt for real test data, since there are errors in camera val\n            gts_data = pickle.load(open(gts_path, \"rb\"))\n            nocs_transform = gts_data[\"gt_RTs\"][gt_id]\n            position = nocs_transform[0:3, 3]\n            rot_scale = nocs_transform[0:3, 0:3]\n            nocs_scales = np.sqrt(np.sum(rot_scale**2, axis=0))\n            rotation_matrix = rot_scale / nocs_scales[:, None]\n            nocs_scale = nocs_scales[0]\n        else:  # camera_train, camera_val, real_train\n            # use ground truth NOCS mask to perform alignment\n            (\n                position,\n                rotation_matrix,\n                nocs_scale,\n                nocs_transform,\n            ) = self._estimate_object(color_path, mask_id)\n\n        orientation_q = Rotation.from_matrix(rotation_matrix).as_quat()\n        mesh_extents = self._get_mesh_extents_from_obj(obj_path)\n\n        if \"camera\" in self._split:\n            # CAMERA / ShapeNet meshes are normalized s.t. diagonal == 1\n            # get metric extents by scaling with the diagonal\n            extents = nocs_scale * mesh_extents\n        elif \"real\" in self._split:\n            # REAL object meshes are not normalized\n            extents = mesh_extents\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n\n        position = torch.Tensor(position)\n        orientation_q = torch.Tensor(orientation_q)\n        extents = torch.Tensor(extents)\n        nocs_transform = torch.Tensor(nocs_transform)\n        return position, orientation_q, extents, nocs_transform\n\n    def _get_gts_path(self, color_path: str) -&gt; Optional[str]:\n        \"\"\"Return path to gts file from color filepath.\n\n        Return None if split does not have ground truth information.\n        \"\"\"\n        if self._split == \"real_test\":\n            gts_dir_path = os.path.join(self._root_dir_path, \"gts\", \"real_test\")\n        elif self._split == \"camera_val\":\n            gts_dir_path = os.path.join(self._root_dir_path, \"gts\", \"val\")\n        else:\n            return None\n\n        path = os.path.normpath(color_path)\n        split_path = path.split(os.sep)\n        number = path[-14:-10]\n        gts_filename = f\"results_{split_path[-3]}_{split_path[-2]}_{number}.pkl\"\n        gts_path = os.path.join(gts_dir_path, gts_filename)\n        return gts_path\n\n    def _get_obj_path(self, meta_row: pd.Series) -&gt; str:\n        \"\"\"Return path to object file from meta data row.\"\"\"\n        if \"camera\" in self._split:  # ShapeNet mesh\n            synset_id = meta_row.iloc[2]\n            object_id = meta_row.iloc[3]\n            obj_path = os.path.join(\n                self._root_dir_path,\n                \"obj_models\",\n                self._split.replace(\"camera_\", \"\"),\n                synset_id,\n                object_id,\n                \"model.obj\",\n            )\n        elif \"real_test\" in self._split:  # Fixed REAL test meshes\n            object_id = meta_row.iloc[2]\n            obj_path = os.path.join(\n                self._root_dir_path,\n                \"fixed_real_test_obj_models\",\n                object_id + \".obj\",\n            )\n        elif \"real_train\" in self._split:  # REAL train mesh (not complete)\n            object_id = meta_row.iloc[2]\n            obj_path = os.path.join(\n                self._root_dir_path, \"obj_models\", self._split, object_id + \".obj\"\n            )\n        else:\n            raise ValueError(f\"Specified split {self._split} is not supported.\")\n        return obj_path\n\n    def _get_mesh_extents_from_obj(self, obj_path: str) -&gt; torch.Tensor:\n        \"\"\"Return maximum extent of bounding box from obj filepath.\n\n        Note that this is normalized extent (with diagonal == 1) in the case of CAMERA\n        dataset, and unnormalized (i.e., metric) extent in the case of REAL dataset.\n        \"\"\"\n        mesh = o3d.io.read_triangle_mesh(obj_path)\n        vertices = np.asarray(mesh.vertices)\n        if len(vertices) == 0:\n            raise ObjectError()\n        extents = np.max(vertices, axis=0) - np.min(vertices, axis=0)\n        return torch.Tensor(extents)\n\n    def _load_mask(self, mask_path: str) -&gt; torch.Tensor:\n        \"\"\"Load mask from mask filepath.\"\"\"\n        mask_img = np.asarray(Image.open(mask_path), dtype=np.uint8)\n        if mask_img.ndim == 3 and mask_img.shape[2] == 4:  # CAMERA masks are RGBA\n            instances_mask = mask_img[:, :, 0]  # use first channel only\n        else:  # REAL masks are grayscale\n            instances_mask = mask_img\n        return torch.from_numpy(instances_mask)\n\n    def _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n        \"\"\"Load depth from depth filepath.\"\"\"\n        depth = torch.from_numpy(\n            np.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n        )\n        return depth\n\n    def _load_nocs_map(self, nocs_map_path: str) -&gt; torch.Tensor:\n        \"\"\"Load NOCS map from NOCS map filepath.\n\n        Returns:\n            NOCS map where each channel corresponds to one dimension in NOCS.\n            Coordinates are normalized to [0,1], shape (H,W,3).\n        \"\"\"\n        nocs_map = torch.from_numpy(\n            np.asarray(Image.open(nocs_map_path), dtype=np.float32) / 255\n        )\n        # z-coordinate has to be flipped\n        # see https://github.com/hughw19/NOCS_CVPR2019/blob/14dbce775c3c7c45bb7b19269bd53d68efb8f73f/dataset.py#L327 # noqa: E501\n        nocs_map[:, :, 2] = 1 - nocs_map[:, :, 2]\n        return nocs_map[:, :, :3]\n\n    def _estimate_object(self, color_path: str, mask_id: int) -&gt; tuple:\n        \"\"\"Estimate pose and scale through ground truth NOCS map.\"\"\"\n        position = rotation_matrix = scale = out_transform = None\n        depth_path = self._depth_path_from_color_path(color_path)\n        depth = self._load_depth(depth_path)\n        mask_path = self._mask_path_from_color_path(color_path)\n        instances_mask = self._load_mask(mask_path)\n        instance_mask = instances_mask == mask_id\n        nocs_map_path = self._nocs_map_path_from_color_path(color_path)\n        nocs_map = self._load_nocs_map(nocs_map_path)\n        valid_instance_mask = instance_mask * depth != 0\n        nocs_map[~valid_instance_mask] = 0\n        centered_nocs_points = nocs_map[valid_instance_mask] - 0.5\n\n        measured_points = pointset_utils.depth_to_pointcloud(\n            depth, self._camera, mask=valid_instance_mask, convention=\"opencv\"\n        )\n\n        # require at least 30 point correspondences to prevent outliers\n        if len(measured_points) &lt; 30:\n            raise nocs_utils.PoseEstimationError()\n\n        # skip object if it cointains errorneous depth\n        if torch.max(depth[valid_instance_mask]) &gt; 32.0:\n            print(\"Erroneous depth detected.\", end=\"\\033[K\\n\")\n            raise nocs_utils.PoseEstimationError()\n\n        (\n            position,\n            rotation_matrix,\n            scale,\n            out_transform,\n        ) = nocs_utils.estimate_similarity_transform(\n            centered_nocs_points, measured_points, verbose=False\n        )\n\n        if position is None:\n            raise nocs_utils.PoseEstimationError()\n\n        return position, rotation_matrix, scale, out_transform\n\n    def _get_scale(self, sample_data: dict, extents: torch.Tensor) -&gt; float:\n        \"\"\"Return scale from stored sample data and extents.\"\"\"\n        if self._scale_convention == \"diagonal\":\n            return sample_data[\"nocs_scale\"]\n        elif self._scale_convention == \"max\":\n            return sample_data[\"max_extent\"]\n        elif self._scale_convention == \"half_max\":\n            return 0.5 * sample_data[\"max_extent\"]\n        elif self._scale_convention == \"full\":\n            return extents\n        else:\n            raise ValueError(\n                f\"Specified scale convention {self._scale_convnetion} not supported.\"\n            )\n\n    def _change_axis_convention(\n        self, orientation_q: torch.Tensor, extents: torch.Tensor\n    ) -&gt; tuple:\n        \"\"\"Adjust up-axis for orientation and extents.\n\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return orientation_q, extents\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        remapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n\n        # quaternion so far: original -&gt; camera\n        # we want a quaternion: new -&gt; camera\n        rotation_n2o = rotation_o2n.T\n\n        quaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\n\n        remapped_orientation_q = quaternion_utils.quaternion_multiply(\n            orientation_q, quaternion_n2o\n        )  # new -&gt; original -&gt; camera\n\n        return remapped_orientation_q, remapped_extents\n\n    def _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n        \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\n        rotation_o2n = np.zeros((3, 3))  # original to new object convention\n        if self._remap_y_axis == \"x\":\n            rotation_o2n[0, 1] = 1\n        elif self._remap_y_axis == \"-x\":\n            rotation_o2n[0, 1] = -1\n        elif self._remap_y_axis == \"y\":\n            rotation_o2n[1, 1] = 1\n        elif self._remap_y_axis == \"-y\":\n            rotation_o2n[1, 1] = -1\n        elif self._remap_y_axis == \"z\":\n            rotation_o2n[2, 1] = 1\n        elif self._remap_y_axis == \"-z\":\n            rotation_o2n[2, 1] = -1\n        else:\n            raise ValueError(\"Unsupported remap_y_axis {self._remap_y_axis}\")\n\n        if self._remap_x_axis == \"x\":\n            rotation_o2n[0, 0] = 1\n        elif self._remap_x_axis == \"-x\":\n            rotation_o2n[0, 0] = -1\n        elif self._remap_x_axis == \"y\":\n            rotation_o2n[1, 0] = 1\n        elif self._remap_x_axis == \"-y\":\n            rotation_o2n[1, 0] = -1\n        elif self._remap_x_axis == \"z\":\n            rotation_o2n[2, 0] = 1\n        elif self._remap_x_axis == \"-z\":\n            rotation_o2n[2, 0] = -1\n        else:\n            raise ValueError(\"Unsupported remap_x_axis {self._remap_x_axis}\")\n\n        # infer last column\n        rotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\n        rotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\n        if np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\n            raise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\n        return rotation_o2n\n\n    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert quaternion to selected orientation representation.\n\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\n        if self._orientation_repr == \"quaternion\":\n            return quaternion\n        else:\n            raise NotImplementedError(\n                f\"Orientation representation {self._orientation_repr} is not supported.\"\n            )\n\n    def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n        \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n        mesh = o3d.io.read_triangle_mesh(object_path)\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return mesh\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        mesh.rotate(\n            rotation_o2n,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        return mesh\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for NOCSDataset.</p> ATTRIBUTE DESCRIPTION <code>root_dir</code> <p>See NOCSDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>The dataset split. The following strings are supported:     \"camera_train\": 275000 images, synthetic objects + real background     \"camera_val\": 25000 images, synthetic objects + real background     \"real_train\": 4300 images in 7 scenes, real     \"real_test\": 2750 images in 6 scenes, real</p> <p> TYPE: <code>str</code> </p> <code>mask_pointcloud</code> <p>Whether the returned pointcloud will be masked.</p> <p> TYPE: <code>bool</code> </p> <code>normalize_pointcloud</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <p> TYPE: <code>bool</code> </p> <code>scale_convention</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <p> TYPE: <code>str</code> </p> <code>camera_convention</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <p> TYPE: <code>str</code> </p> <code>orientation_repr</code> <p>Which orientation representation is used. Currently only \"quaternion\" supported.</p> <p> TYPE: <code>str</code> </p> <code>remap_y_axis</code> <p>If not None, the NOCS y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>remap_x_axis</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>category_str</code> <p>If not None, only samples from the matching category will be returned. See NOCSDataset.category_id_to_str for admissible category strings.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n    \"\"\"Configuration dictionary for NOCSDataset.\n\n    Attributes:\n        root_dir: See NOCSDataset docstring.\n        split:\n            The dataset split. The following strings are supported:\n                \"camera_train\": 275000 images, synthetic objects + real background\n                \"camera_val\": 25000 images, synthetic objects + real background\n                \"real_train\": 4300 images in 7 scenes, real\n                \"real_test\": 2750 images in 6 scenes, real\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. Currently only \"quaternion\"\n            supported.\n        remap_y_axis:\n            If not None, the NOCS y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See NOCSDataset.category_id_to_str for admissible category strings.\n    \"\"\"\n\n    root_dir: str\n    split: str\n    mask_pointcloud: bool\n    normalize_pointcloud: bool\n    scale_convention: str\n    camera_convention: str\n    orientation_repr: str\n    remap_y_axis: Optional[str]\n    remap_x_axis: Optional[str]\n    category_str: Optional[str]\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initialize the dataset.</p> PARAMETER  DESCRIPTION <code>config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See NOCSDataset.Config for supported keys.</p> <p> TYPE: <code>Config</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See NOCSDataset.Config for supported keys.\n    \"\"\"\n    config = yoco.load_config(config, current_dict=NOCSDataset.default_config)\n    self._root_dir_path = utils.resolve_path(config[\"root_dir\"])\n    self._split = config[\"split\"]\n    self._check_dirs()\n    self._camera_convention = config[\"camera_convention\"]\n    self._camera = self._get_split_camera()\n    self._preprocess_path = os.path.join(\n        self._root_dir_path, \"cpas_toolbox\", self._split\n    )\n    if not os.path.isdir(self._preprocess_path):\n        self._preprocess_dataset()\n    self._mask_pointcloud = config[\"mask_pointcloud\"]\n    self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n    self._scale_convention = config[\"scale_convention\"]\n    self._sample_files = self._get_sample_files(config[\"category_str\"])\n    self._remap_y_axis = config[\"remap_y_axis\"]\n    self._remap_x_axis = config[\"remap_x_axis\"]\n    self._orientation_repr = config[\"orientation_repr\"]\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of sample in dataset.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of sample in dataset.\"\"\"\n    return len(self._sample_files)\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict\n</code></pre> <p>Return a sample of the dataset.</p> PARAMETER  DESCRIPTION <code>idx</code> <p>Index of the instance.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Sample containing the following items: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\"</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"Return a sample of the dataset.\n\n    Args:\n        idx: Index of the instance.\n\n    Returns:\n        Sample containing the following items:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n    \"\"\"\n    sample_file = self._sample_files[idx]\n    sample_data = pickle.load(open(sample_file, \"rb\"))\n    sample = self._sample_from_sample_data(sample_data)\n    return sample\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.NOCSDataset.load_mesh","title":"load_mesh","text":"<pre><code>load_mesh(object_path: str) -&gt; o3d.geometry.TriangleMesh\n</code></pre> <p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n    \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n    mesh = o3d.io.read_triangle_mesh(object_path)\n    if self._remap_y_axis is None and self._remap_x_axis is None:\n        return mesh\n    elif self._remap_y_axis is None or self._remap_x_axis is None:\n        raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n    rotation_o2n = self._get_o2n_object_rotation_matrix()\n    mesh.rotate(\n        rotation_o2n,\n        center=np.array([0.0, 0.0, 0.0])[:, None],\n    )\n    return mesh\n</code></pre>"},{"location":"api_reference/datasets/nocs_dataset/#cpas_toolbox.datasets.nocs_dataset.ObjectError","title":"ObjectError","text":"<p>             Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>cpas_toolbox/datasets/nocs_dataset.py</code> <pre><code>class ObjectError(Exception):\n    \"\"\"Error if something with the mesh is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/datasets/nocs_utils/","title":"nocs_utils.py","text":""},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils","title":"cpas_toolbox.datasets.nocs_utils","text":"<p>Module for utility function related to NOCS dataset.</p> <p>This module contains functions to find similarity transform from NOCS maps and evaluation function for typical metrics on the NOCS datasets.</p> Aligning code by Srinath Sridhar <p>https://raw.githubusercontent.com/hughw19/NOCS_CVPR2019/master/aligning.py</p>"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.PoseEstimationError","title":"PoseEstimationError","text":"<p>             Bases: <code>Exception</code></p> <p>Error if pose estimation encountered an error.</p> Source code in <code>cpas_toolbox/datasets/nocs_utils.py</code> <pre><code>class PoseEstimationError(Exception):\n    \"\"\"Error if pose estimation encountered an error.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/datasets/nocs_utils/#cpas_toolbox.datasets.nocs_utils.estimate_similarity_transform","title":"estimate_similarity_transform","text":"<pre><code>estimate_similarity_transform(\n    source: np.ndarray, target: np.ndarray, verbose: bool = False\n) -&gt; tuple\n</code></pre> <p>Estimate similarity transform from source to target from point correspondences.</p> <p>Source and target are pairwise correponding pointsets, i.e., they include same number of points and the first point of source corresponds to the first point of target. RANSAC is used for outlier-robust estimation.</p> <p>A similarity transform is estimated (i.e., isotropic scale, rotation and translation) that transforms source points onto the target points.</p> <p>Note that the returned values fulfill the following equations     transform @ source_points = scale * rotation_matrix @ source_points + position when ignoring homogeneous coordinate for left-hand side.</p> PARAMETER  DESCRIPTION <code>source</code> <p>Source points that will be transformed, shape (N,3).</p> <p> TYPE: <code>ndarray</code> </p> <code>target</code> <p>Target points to which source will be aligned to, shape (N,3).</p> <p> TYPE: <code>ndarray</code> </p> <code>verbose</code> <p>If true additional information will be printed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>position</code> <p>Translation to translate source to target, shape (3,).</p> <p> TYPE: <code>ndarray</code> </p> <code>rotation_matrix</code> <p>Rotation to rotate source to target, shape (3,3).</p> <p> TYPE: <code>ndarray</code> </p> <code>scale</code> <p>Scaling factor along each axis, to scale source to target.</p> <p> TYPE: <code>float</code> </p> <code>transform</code> <p>Homogeneous transformation matrix, shape (4,4).</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>cpas_toolbox/datasets/nocs_utils.py</code> <pre><code>def estimate_similarity_transform(\n    source: np.ndarray, target: np.ndarray, verbose: bool = False\n) -&gt; tuple:\n    \"\"\"Estimate similarity transform from source to target from point correspondences.\n\n    Source and target are pairwise correponding pointsets, i.e., they include same\n    number of points and the first point of source corresponds to the first point of\n    target. RANSAC is used for outlier-robust estimation.\n\n    A similarity transform is estimated (i.e., isotropic scale, rotation and\n    translation) that transforms source points onto the target points.\n\n    Note that the returned values fulfill the following equations\n        transform @ source_points = scale * rotation_matrix @ source_points + position\n    when ignoring homogeneous coordinate for left-hand side.\n\n    Args:\n        source: Source points that will be transformed, shape (N,3).\n        target: Target points to which source will be aligned to, shape (N,3).\n        verbose: If true additional information will be printed.\n\n    Returns:\n        position (np.ndarray): Translation to translate source to target, shape (3,).\n        rotation_matrix (np.ndarray): Rotation to rotate source to target, shape (3,3).\n        scale (float):\n            Scaling factor along each axis, to scale source to target.\n        transform (np.ndarray): Homogeneous transformation matrix, shape (4,4).\n    \"\"\"\n    if len(source) &lt; 5 or len(target) &lt; 5:\n        print(\"Pose estimation failed. Not enough point correspondences: \", len(source))\n        return None, None, None, None\n\n    # make points homogeneous\n    source_hom = np.transpose(np.hstack([source, np.ones([source.shape[0], 1])]))  # 4,N\n    target_hom = np.transpose(np.hstack([target, np.ones([source.shape[0], 1])]))  # 4,N\n\n    # Auto-parameter selection based on source-target heuristics\n    target_norm = np.mean(np.linalg.norm(target, axis=1))  # mean distance from origin\n    source_norm = np.mean(np.linalg.norm(source, axis=1))\n    ratio_ts = target_norm / source_norm\n    ratio_st = source_norm / target_norm\n    pass_t = ratio_st if (ratio_st &gt; ratio_ts) else ratio_ts\n    pass_t *= 0.01  # tighter bound\n    stop_t = pass_t / 100\n    n_iter = 100\n    if verbose:\n        print(\"Pass threshold: \", pass_t)\n        print(\"Stop threshold: \", stop_t)\n        print(\"Number of iterations: \", n_iter)\n\n    source_inliers_hom, target_inliers_hom, best_inlier_ratio = _get_ransac_inliers(\n        source_hom,\n        target_hom,\n        max_iterations=n_iter,\n        pass_threshold=pass_t,\n        stop_threshold=stop_t,\n    )\n\n    if best_inlier_ratio &lt; 0.1:\n        print(\"Pose estimation failed. Small inlier ratio: \", best_inlier_ratio)\n        return None, None, None, None\n\n    scales, rotation_matrix, position, out_transform = _estimate_similarity_umeyama(\n        source_inliers_hom, target_inliers_hom\n    )\n    scale = scales[0]\n\n    if verbose:\n        print(\"BestInlierRatio:\", best_inlier_ratio)\n        print(\"Rotation:\\n\", rotation_matrix)\n        print(\"Position:\\n\", position)\n        print(\"Scales:\", scales)\n\n    return position, rotation_matrix, scale, out_transform\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/","title":"redwood_dataset.py","text":""},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset","title":"cpas_toolbox.datasets.redwood_dataset","text":"<p>Module providing dataset class for annotated Redwood dataset.</p>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset","title":"AnnotatedRedwoodDataset","text":"<p>             Bases: <code>Dataset</code></p> <p>Dataset class for annotated Redwood dataset.</p> <p>Data can be found here: http://redwood-data.org/3dscan/index.html</p> <p>Annotations are of repo.</p> Expected directory format <p>{root_dir}/{category_str}/rgbd/{sequence_id}/... {ann_dir}/{sequence_id}.obj {ann_dir}/annotations.json</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class AnnotatedRedwoodDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset class for annotated Redwood dataset.\n\n    Data can be found here:\n    http://redwood-data.org/3dscan/index.html\n\n    Annotations are of repo.\n\n    Expected directory format:\n        {root_dir}/{category_str}/rgbd/{sequence_id}/...\n        {ann_dir}/{sequence_id}.obj\n        {ann_dir}/annotations.json\n    \"\"\"\n\n    num_categories = 3\n    category_id_to_str = {\n        0: \"bottle\",\n        1: \"bowl\",\n        2: \"mug\",\n    }\n    category_str_to_id = {v: k for k, v in category_id_to_str.items()}\n\n    class Config(TypedDict, total=False):\n        \"\"\"Configuration dictionary for annoated Redwood dataset.\n\n        Attributes:\n            root_dir: See AnnotatedRedwoodDataset docstring.\n            ann_dir: See AnnotatedRedwoodDataset docstring.\n            mask_pointcloud: Whether the returned pointcloud will be masked.\n            normalize_pointcloud:\n                Whether the returned pointcloud and position will be normalized, such\n                that pointcloud centroid is at the origin.\n            scale_convention:\n                Which scale is returned. The following strings are supported:\n                    \"diagonal\":\n                        Length of bounding box' diagonal. This is what NOCS uses.\n                    \"max\": Maximum side length of bounding box.\n                    \"half_max\": Half maximum side length of bounding box.\n                    \"full\": Bounding box side lengths. Shape (3,).\n            camera_convention:\n                Which camera convention is used for position and orientation. One of:\n                    \"opengl\": x right, y up, z back\n                    \"opencv\": x right, y down, z forward\n                Note that this does not influence how the dataset is processed, only the\n                returned position and quaternion.\n            orientation_repr:\n                Which orientation representation is used. Currently only \"quaternion\"\n                supported.\n            remap_y_axis:\n                If not None, the Redwood y-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                This is typically the up-axis.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: -y\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            remap_x_axis:\n                If not None, the original x-axis will be mapped to the provided axis.\n                Resulting coordinate system will always be right-handed.\n                Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n                To get ShapeNetV2 alignment: z\n                One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n            category_str:\n                If not None, only samples from the matching category will be returned.\n                See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n                strings.\n        \"\"\"\n\n        root_dir: str\n        ann_dir: str\n        split: str\n        mask_pointcloud: bool\n        normalize_pointcloud: bool\n        scale_convention: str\n        camera_convention: str\n        orientation_repr: str\n        orientation_grid_resolution: int\n        remap_y_axis: Optional[str]\n        remap_x_axis: Optional[str]\n        category_str: Optional[str]\n\n    default_config: Config = {\n        \"root_dir\": None,\n        \"ann_dir\": None,\n        \"mask_pointcloud\": False,\n        \"normalize_pointcloud\": False,\n        \"camera_convention\": \"opengl\",\n        \"scale_convention\": \"half_max\",\n        \"orientation_repr\": \"quaternion\",\n        \"orientation_grid_resolution\": None,\n        \"category_str\": None,\n        \"remap_y_axis\": None,\n        \"remap_x_axis\": None,\n    }\n\n    def __init__(\n        self,\n        config: Config,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            config:\n                Configuration dictionary of dataset. Provided dictionary will be merged\n                with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n        \"\"\"\n        config = yoco.load_config(\n            config, current_dict=AnnotatedRedwoodDataset.default_config\n        )\n        self._root_dir_path = utils.resolve_path(config[\"root_dir\"])\n        self._ann_dir_path = utils.resolve_path(config[\"ann_dir\"])\n        self._check_dirs()\n        self._camera_convention = config[\"camera_convention\"]\n        self._mask_pointcloud = config[\"mask_pointcloud\"]\n        self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n        self._scale_convention = config[\"scale_convention\"]\n        self._remap_y_axis = config[\"remap_y_axis\"]\n        self._remap_x_axis = config[\"remap_x_axis\"]\n        self._orientation_repr = config[\"orientation_repr\"]\n        self._category_str = config[\"category_str\"]\n        self._load_annotations()\n        self._camera = camera_utils.Camera(\n            width=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5\n        )\n\n    def _check_dirs(self) -&gt; None:\n        if os.path.exists(self._root_dir_path) and os.path.exists(self._ann_dir_path):\n            pass\n        else:\n            print(\n                \"REDWOOD75 dataset not found, do you want to download it into the \"\n                \"following directories:\"\n            )\n            print(\"  \", self._root_dir_path)\n            print(\"  \", self._ann_dir_path)\n            while True:\n                decision = input(\"(Y/n) \").lower()\n                if decision == \"\" or decision == \"y\":\n                    self._download_dataset()\n                    break\n                elif decision == \"n\":\n                    print(\"Dataset not found. Aborting.\")\n                    exit(0)\n\n    def _download_dataset(self) -&gt; None:\n        # Download anns\n        if not os.path.exists(self._ann_dir_path):\n            zip_path = os.path.join(self._ann_dir_path, \"redwood75.zip\")\n            os.makedirs(self._ann_dir_path, exist_ok=True)\n            url = (\n                \"https://drive.google.com/u/0/uc?id=1PMvIblsXWDxEJykVwhUk_QEjy4_bmDU\"\n                \"-&amp;export=download\"\n            )\n            utils.download(url, zip_path)\n            z = zipfile.ZipFile(zip_path)\n            z.extractall(os.path.join(self._ann_dir_path, \"..\"))\n            z.close()\n            os.remove(zip_path)\n\n        ann_json = os.path.join(self._ann_dir_path, \"annotations.json\")\n        with open(ann_json, \"r\") as f:\n            anns_dict = json.load(f)\n\n        baseurl = \"https://redwood-3dscan.b-cdn.net/rgbd/\"\n        for seq_id in anns_dict.keys():\n            download_dir_path = os.path.join(\n                self._root_dir_path, anns_dict[seq_id][\"category\"]\n            )\n            os.makedirs(download_dir_path, exist_ok=True)\n            zip_path = os.path.join(download_dir_path, f\"{seq_id}.zip\")\n            os.makedirs(os.path.dirname(zip_path), exist_ok=True)\n            url = baseurl + f\"{seq_id}.zip\"\n            utils.download(baseurl + f\"{seq_id}.zip\", zip_path)\n            z = zipfile.ZipFile(zip_path)\n            seq_dir_path = os.path.join(download_dir_path, \"rgbd\", seq_id)\n            os.makedirs(seq_dir_path, exist_ok=True)\n            z.extractall(seq_dir_path)\n            z.close()\n            os.remove(zip_path)\n\n    def _load_annotations(self) -&gt; None:\n        \"\"\"Load annotations into memory.\"\"\"\n        ann_json = os.path.join(self._ann_dir_path, \"annotations.json\")\n        with open(ann_json, \"r\") as f:\n            anns_dict = json.load(f)\n        self._raw_samples = []\n        for seq_id, seq_anns in anns_dict.items():\n            if (\n                self._category_str is not None\n                and self._category_str != seq_anns[\"category\"]\n            ):\n                continue\n            for pose_ann in seq_anns[\"pose_anns\"]:\n                self._raw_samples.append(\n                    self._create_raw_sample(seq_id, seq_anns, pose_ann)\n                )\n\n    def _create_raw_sample(\n        self, seq_id: str, sequence_dict: dict, annotation_dict: dict\n    ) -&gt; dict:\n        \"\"\"Create raw sample from information in annotations file.\"\"\"\n        position = torch.tensor(annotation_dict[\"position\"])\n        orientation_q = torch.tensor(annotation_dict[\"orientation\"])\n        rgb_filename = annotation_dict[\"rgb_file\"]\n        depth_filename = annotation_dict[\"depth_file\"]\n        mesh_filename = sequence_dict[\"mesh\"]\n        mesh_path = os.path.join(self._ann_dir_path, mesh_filename)\n        category_str = sequence_dict[\"category\"]\n        color_path = os.path.join(\n            self._root_dir_path, category_str, \"rgbd\", seq_id, \"rgb\", rgb_filename\n        )\n        depth_path = os.path.join(\n            self._root_dir_path, category_str, \"rgbd\", seq_id, \"depth\", depth_filename\n        )\n        extents = torch.tensor(sequence_dict[\"scale\"]) * 2\n        return {\n            \"position\": position,\n            \"orientation_q\": orientation_q,\n            \"extents\": extents,\n            \"color_path\": color_path,\n            \"depth_path\": depth_path,\n            \"mesh_path\": mesh_path,\n            \"category_str\": category_str,\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of sample in dataset.\"\"\"\n        return len(self._raw_samples)\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        \"\"\"Return a sample of the dataset.\n\n        Args:\n            idx: Index of the instance.\n\n        Returns:\n            Sample containing the following keys:\n                \"color\"\n                \"depth\"\n                \"mask\"\n                \"pointset\"\n                \"position\"\n                \"orientation\"\n                \"quaternion\"\n                \"scale\"\n                \"color_path\"\n                \"obj_path\"\n                \"category_id\"\n                \"category_str\"\n        \"\"\"\n        raw_sample = self._raw_samples[idx]\n        color = torch.from_numpy(\n            np.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n        )\n        depth = self._load_depth(raw_sample[\"depth_path\"])\n        instance_mask = self._compute_mask(depth, raw_sample)\n\n        pointcloud_mask = instance_mask if self._mask_pointcloud else None\n        pointcloud = pointset_utils.depth_to_pointcloud(\n            depth,\n            self._camera,\n            mask=pointcloud_mask,\n            convention=self._camera_convention,\n        )\n\n        # adjust camera convention for position, orientation and scale\n        position = pointset_utils.change_position_camera_convention(\n            raw_sample[\"position\"], \"opencv\", self._camera_convention\n        )\n\n        # orientation / scale\n        orientation_q, extents = self._change_axis_convention(\n            raw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n        )\n        orientation_q = pointset_utils.change_orientation_camera_convention(\n            orientation_q, \"opencv\", self._camera_convention\n        )\n        orientation = self._quat_to_orientation_repr(orientation_q)\n        scale = self._get_scale(extents)\n\n        # normalize pointcloud &amp; position\n        if self._normalize_pointcloud:\n            pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n            position = position - centroid\n\n        category_str = raw_sample[\"category_str\"]\n        sample = {\n            \"color\": color,\n            \"depth\": depth,\n            \"pointset\": pointcloud,\n            \"mask\": instance_mask,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"quaternion\": orientation_q,\n            \"scale\": scale,\n            \"color_path\": raw_sample[\"color_path\"],\n            \"obj_path\": raw_sample[\"mesh_path\"],\n            \"category_id\": self.category_str_to_id[category_str],\n            \"category_str\": category_str,\n        }\n        return sample\n\n    def _compute_mask(self, depth: torch.Tensor, raw_sample: dict) -&gt; torch.Tensor:\n        posed_mesh = o3d.io.read_triangle_mesh(raw_sample[\"mesh_path\"])\n        R = Rotation.from_quat(raw_sample[\"orientation_q\"]).as_matrix()\n        posed_mesh.rotate(R, center=np.array([0, 0, 0]))\n        posed_mesh.translate(raw_sample[\"position\"])\n        posed_mesh.compute_vertex_normals()\n        gt_depth = torch.from_numpy(_draw_depth_geometry(posed_mesh, self._camera))\n        mask = gt_depth != 0\n        # exclude occluded parts from mask\n        mask[(depth != 0) * (depth &lt; gt_depth - 0.01)] = 0\n        return mask\n\n    def _load_depth(self, depth_path: str) -&gt; torch.Tensor:\n        \"\"\"Load depth from depth filepath.\"\"\"\n        depth = torch.from_numpy(\n            np.asarray(Image.open(depth_path), dtype=np.float32) * 0.001\n        )\n        return depth\n\n    def _get_scale(self, extents: torch.Tensor) -&gt; float:\n        \"\"\"Return scale from stored sample data and extents.\"\"\"\n        if self._scale_convention == \"diagonal\":\n            return torch.linalg.norm(extents)\n        elif self._scale_convention == \"max\":\n            return extents.max()\n        elif self._scale_convention == \"half_max\":\n            return 0.5 * extents.max()\n        elif self._scale_convention == \"full\":\n            return extents\n        else:\n            raise ValueError(\n                f\"Specified scale convention {self._scale_convention} not supported.\"\n            )\n\n    def _change_axis_convention(\n        self, orientation_q: torch.Tensor, extents: torch.Tensor\n    ) -&gt; tuple:\n        \"\"\"Adjust up-axis for orientation and extents.\n\n        Returns:\n            Tuple of position, orienation_q and extents, with specified up-axis.\n        \"\"\"\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return orientation_q, extents\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        remapped_extents = torch.abs(torch.Tensor(rotation_o2n) @ extents)\n\n        # quaternion so far: original -&gt; camera\n        # we want a quaternion: new -&gt; camera\n        rotation_n2o = rotation_o2n.T\n\n        quaternion_n2o = torch.from_numpy(Rotation.from_matrix(rotation_n2o).as_quat())\n\n        remapped_orientation_q = quaternion_utils.quaternion_multiply(\n            orientation_q, quaternion_n2o\n        )  # new -&gt; original -&gt; camera\n\n        return remapped_orientation_q, remapped_extents\n\n    def _get_o2n_object_rotation_matrix(self) -&gt; np.ndarray:\n        \"\"\"Compute rotation matrix which rotates original to new object coordinates.\"\"\"\n        rotation_o2n = np.zeros((3, 3))  # original to new object convention\n        if self._remap_y_axis == \"x\":\n            rotation_o2n[0, 1] = 1\n        elif self._remap_y_axis == \"-x\":\n            rotation_o2n[0, 1] = -1\n        elif self._remap_y_axis == \"y\":\n            rotation_o2n[1, 1] = 1\n        elif self._remap_y_axis == \"-y\":\n            rotation_o2n[1, 1] = -1\n        elif self._remap_y_axis == \"z\":\n            rotation_o2n[2, 1] = 1\n        elif self._remap_y_axis == \"-z\":\n            rotation_o2n[2, 1] = -1\n        else:\n            raise ValueError(\"Unsupported remap_y_axis {self._remap_y_axis}\")\n\n        if self._remap_x_axis == \"x\":\n            rotation_o2n[0, 0] = 1\n        elif self._remap_x_axis == \"-x\":\n            rotation_o2n[0, 0] = -1\n        elif self._remap_x_axis == \"y\":\n            rotation_o2n[1, 0] = 1\n        elif self._remap_x_axis == \"-y\":\n            rotation_o2n[1, 0] = -1\n        elif self._remap_x_axis == \"z\":\n            rotation_o2n[2, 0] = 1\n        elif self._remap_x_axis == \"-z\":\n            rotation_o2n[2, 0] = -1\n        else:\n            raise ValueError(\"Unsupported remap_x_axis {self._remap_x_axis}\")\n\n        # infer last column\n        rotation_o2n[:, 2] = 1 - np.abs(np.sum(rotation_o2n, 1))  # rows must sum to +-1\n        rotation_o2n[:, 2] *= np.linalg.det(rotation_o2n)  # make special orthogonal\n        if np.linalg.det(rotation_o2n) != 1.0:  # check if special orthogonal\n            raise ValueError(\"Unsupported combination of remap_{y,x}_axis. det != 1\")\n        return rotation_o2n\n\n    def _quat_to_orientation_repr(self, quaternion: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert quaternion to selected orientation representation.\n\n        Args:\n            quaternion:\n                The quaternion to convert, scalar-last, shape (4,).\n\n        Returns:\n            The same orientation as represented by the quaternion in the chosen\n            orientation representation.\n        \"\"\"\n        if self._orientation_repr == \"quaternion\":\n            return quaternion\n        elif self._orientation_repr == \"discretized\":\n            index = self._orientation_grid.quat_to_index(quaternion.numpy())\n            return torch.tensor(\n                index,\n                dtype=torch.long,\n            )\n        else:\n            raise NotImplementedError(\n                f\"Orientation representation {self._orientation_repr} is not supported.\"\n            )\n\n    def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n        \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n        mesh = o3d.io.read_triangle_mesh(object_path)\n        if self._remap_y_axis is None and self._remap_x_axis is None:\n            return mesh\n        elif self._remap_y_axis is None or self._remap_x_axis is None:\n            raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n        rotation_o2n = self._get_o2n_object_rotation_matrix()\n        mesh.rotate(\n            rotation_o2n,\n            center=np.array([0.0, 0.0, 0.0])[:, None],\n        )\n        return mesh\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.Config","title":"Config","text":"<p>             Bases: <code>TypedDict</code></p> <p>Configuration dictionary for annoated Redwood dataset.</p> ATTRIBUTE DESCRIPTION <code>root_dir</code> <p>See AnnotatedRedwoodDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>ann_dir</code> <p>See AnnotatedRedwoodDataset docstring.</p> <p> TYPE: <code>str</code> </p> <code>mask_pointcloud</code> <p>Whether the returned pointcloud will be masked.</p> <p> TYPE: <code>bool</code> </p> <code>normalize_pointcloud</code> <p>Whether the returned pointcloud and position will be normalized, such that pointcloud centroid is at the origin.</p> <p> TYPE: <code>bool</code> </p> <code>scale_convention</code> <p>Which scale is returned. The following strings are supported:     \"diagonal\":         Length of bounding box' diagonal. This is what NOCS uses.     \"max\": Maximum side length of bounding box.     \"half_max\": Half maximum side length of bounding box.     \"full\": Bounding box side lengths. Shape (3,).</p> <p> TYPE: <code>str</code> </p> <code>camera_convention</code> <p>Which camera convention is used for position and orientation. One of:     \"opengl\": x right, y up, z back     \"opencv\": x right, y down, z forward Note that this does not influence how the dataset is processed, only the returned position and quaternion.</p> <p> TYPE: <code>str</code> </p> <code>orientation_repr</code> <p>Which orientation representation is used. Currently only \"quaternion\" supported.</p> <p> TYPE: <code>str</code> </p> <code>remap_y_axis</code> <p>If not None, the Redwood y-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. This is typically the up-axis. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: -y One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>remap_x_axis</code> <p>If not None, the original x-axis will be mapped to the provided axis. Resulting coordinate system will always be right-handed. Note that NOCS object models are NOT aligned the same as ShapeNetV2. To get ShapeNetV2 alignment: z One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"</p> <p> TYPE: <code>Optional[str]</code> </p> <code>category_str</code> <p>If not None, only samples from the matching category will be returned. See AnnotatedRedwoodDataset.category_id_to_str for admissible category strings.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class Config(TypedDict, total=False):\n    \"\"\"Configuration dictionary for annoated Redwood dataset.\n\n    Attributes:\n        root_dir: See AnnotatedRedwoodDataset docstring.\n        ann_dir: See AnnotatedRedwoodDataset docstring.\n        mask_pointcloud: Whether the returned pointcloud will be masked.\n        normalize_pointcloud:\n            Whether the returned pointcloud and position will be normalized, such\n            that pointcloud centroid is at the origin.\n        scale_convention:\n            Which scale is returned. The following strings are supported:\n                \"diagonal\":\n                    Length of bounding box' diagonal. This is what NOCS uses.\n                \"max\": Maximum side length of bounding box.\n                \"half_max\": Half maximum side length of bounding box.\n                \"full\": Bounding box side lengths. Shape (3,).\n        camera_convention:\n            Which camera convention is used for position and orientation. One of:\n                \"opengl\": x right, y up, z back\n                \"opencv\": x right, y down, z forward\n            Note that this does not influence how the dataset is processed, only the\n            returned position and quaternion.\n        orientation_repr:\n            Which orientation representation is used. Currently only \"quaternion\"\n            supported.\n        remap_y_axis:\n            If not None, the Redwood y-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            This is typically the up-axis.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: -y\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        remap_x_axis:\n            If not None, the original x-axis will be mapped to the provided axis.\n            Resulting coordinate system will always be right-handed.\n            Note that NOCS object models are NOT aligned the same as ShapeNetV2.\n            To get ShapeNetV2 alignment: z\n            One of: \"x\", \"y\", \"z\", \"-x\", \"-y\", \"-z\"\n        category_str:\n            If not None, only samples from the matching category will be returned.\n            See AnnotatedRedwoodDataset.category_id_to_str for admissible category\n            strings.\n    \"\"\"\n\n    root_dir: str\n    ann_dir: str\n    split: str\n    mask_pointcloud: bool\n    normalize_pointcloud: bool\n    scale_convention: str\n    camera_convention: str\n    orientation_repr: str\n    orientation_grid_resolution: int\n    remap_y_axis: Optional[str]\n    remap_x_axis: Optional[str]\n    category_str: Optional[str]\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initialize the dataset.</p> PARAMETER  DESCRIPTION <code>config</code> <p>Configuration dictionary of dataset. Provided dictionary will be merged with default_dict. See AnnotatedRedwoodDataset.Config for keys.</p> <p> TYPE: <code>Config</code> </p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        config:\n            Configuration dictionary of dataset. Provided dictionary will be merged\n            with default_dict. See AnnotatedRedwoodDataset.Config for keys.\n    \"\"\"\n    config = yoco.load_config(\n        config, current_dict=AnnotatedRedwoodDataset.default_config\n    )\n    self._root_dir_path = utils.resolve_path(config[\"root_dir\"])\n    self._ann_dir_path = utils.resolve_path(config[\"ann_dir\"])\n    self._check_dirs()\n    self._camera_convention = config[\"camera_convention\"]\n    self._mask_pointcloud = config[\"mask_pointcloud\"]\n    self._normalize_pointcloud = config[\"normalize_pointcloud\"]\n    self._scale_convention = config[\"scale_convention\"]\n    self._remap_y_axis = config[\"remap_y_axis\"]\n    self._remap_x_axis = config[\"remap_x_axis\"]\n    self._orientation_repr = config[\"orientation_repr\"]\n    self._category_str = config[\"category_str\"]\n    self._load_annotations()\n    self._camera = camera_utils.Camera(\n        width=640, height=480, fx=525, fy=525, cx=319.5, cy=239.5\n    )\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of sample in dataset.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of sample in dataset.\"\"\"\n    return len(self._raw_samples)\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict\n</code></pre> <p>Return a sample of the dataset.</p> PARAMETER  DESCRIPTION <code>idx</code> <p>Index of the instance.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Sample containing the following keys: \"color\" \"depth\" \"mask\" \"pointset\" \"position\" \"orientation\" \"quaternion\" \"scale\" \"color_path\" \"obj_path\" \"category_id\" \"category_str\"</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"Return a sample of the dataset.\n\n    Args:\n        idx: Index of the instance.\n\n    Returns:\n        Sample containing the following keys:\n            \"color\"\n            \"depth\"\n            \"mask\"\n            \"pointset\"\n            \"position\"\n            \"orientation\"\n            \"quaternion\"\n            \"scale\"\n            \"color_path\"\n            \"obj_path\"\n            \"category_id\"\n            \"category_str\"\n    \"\"\"\n    raw_sample = self._raw_samples[idx]\n    color = torch.from_numpy(\n        np.asarray(Image.open(raw_sample[\"color_path\"]), dtype=np.float32) / 255\n    )\n    depth = self._load_depth(raw_sample[\"depth_path\"])\n    instance_mask = self._compute_mask(depth, raw_sample)\n\n    pointcloud_mask = instance_mask if self._mask_pointcloud else None\n    pointcloud = pointset_utils.depth_to_pointcloud(\n        depth,\n        self._camera,\n        mask=pointcloud_mask,\n        convention=self._camera_convention,\n    )\n\n    # adjust camera convention for position, orientation and scale\n    position = pointset_utils.change_position_camera_convention(\n        raw_sample[\"position\"], \"opencv\", self._camera_convention\n    )\n\n    # orientation / scale\n    orientation_q, extents = self._change_axis_convention(\n        raw_sample[\"orientation_q\"], raw_sample[\"extents\"]\n    )\n    orientation_q = pointset_utils.change_orientation_camera_convention(\n        orientation_q, \"opencv\", self._camera_convention\n    )\n    orientation = self._quat_to_orientation_repr(orientation_q)\n    scale = self._get_scale(extents)\n\n    # normalize pointcloud &amp; position\n    if self._normalize_pointcloud:\n        pointcloud, centroid = pointset_utils.normalize_points(pointcloud)\n        position = position - centroid\n\n    category_str = raw_sample[\"category_str\"]\n    sample = {\n        \"color\": color,\n        \"depth\": depth,\n        \"pointset\": pointcloud,\n        \"mask\": instance_mask,\n        \"position\": position,\n        \"orientation\": orientation,\n        \"quaternion\": orientation_q,\n        \"scale\": scale,\n        \"color_path\": raw_sample[\"color_path\"],\n        \"obj_path\": raw_sample[\"mesh_path\"],\n        \"category_id\": self.category_str_to_id[category_str],\n        \"category_str\": category_str,\n    }\n    return sample\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.AnnotatedRedwoodDataset.load_mesh","title":"load_mesh","text":"<pre><code>load_mesh(object_path: str) -&gt; o3d.geometry.TriangleMesh\n</code></pre> <p>Load an object mesh and adjust its object frame convention.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>def load_mesh(self, object_path: str) -&gt; o3d.geometry.TriangleMesh:\n    \"\"\"Load an object mesh and adjust its object frame convention.\"\"\"\n    mesh = o3d.io.read_triangle_mesh(object_path)\n    if self._remap_y_axis is None and self._remap_x_axis is None:\n        return mesh\n    elif self._remap_y_axis is None or self._remap_x_axis is None:\n        raise ValueError(\"Either both or none of remap_{y,x}_axis have to be None.\")\n\n    rotation_o2n = self._get_o2n_object_rotation_matrix()\n    mesh.rotate(\n        rotation_o2n,\n        center=np.array([0.0, 0.0, 0.0])[:, None],\n    )\n    return mesh\n</code></pre>"},{"location":"api_reference/datasets/redwood_dataset/#cpas_toolbox.datasets.redwood_dataset.ObjectError","title":"ObjectError","text":"<p>             Bases: <code>Exception</code></p> <p>Error if something with the mesh is wrong.</p> Source code in <code>cpas_toolbox/datasets/redwood_dataset.py</code> <pre><code>class ObjectError(Exception):\n    \"\"\"Error if something with the mesh is wrong.\"\"\"\n\n    pass\n</code></pre>"}]}